{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from torch.utils.data import TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes a list of model weights (each model weight being a dictionary), averages them and returns the average weights\n",
    "def average_weights(w):\n",
    "    \"\"\"\n",
    "    Returns the average of the weights.\n",
    "    \"\"\"\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    for key in w_avg.keys():\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[key] += w[i][key]\n",
    "        w_avg[key] = torch.div(w_avg[key], len(w))\n",
    "    return w_avg\n",
    "\n",
    "\n",
    "\n",
    "#This function splits a dataset into a number of independent and identically distributed (IID) subsets. It is done in such a way that each user or client receives a unique set of samples.\n",
    "\n",
    "def split_iid(dataset, num_users):\n",
    "    num_items = int(len(dataset)/num_users) # the number of allocated samples for each client\n",
    "    dict_users, all_idxs = {}, [i for i in range (len(dataset))]\n",
    "    for i in range (num_users):\n",
    "        dict_users[i] = set(np.random.choice(all_idxs, num_items, replace=False))\n",
    "        all_idxs = list (set(all_idxs) - dict_users[i])\n",
    "    return dict_users \n",
    "\n",
    "\n",
    "# This function reads the training and test data from a csv file, \n",
    "#performs some preprocessing operations and returns a TensorDataset for both training and test data. \n",
    "#It also splits the training data into multiple subsets using the unlabeled_iid() function and returns these user groups\n",
    "def get_dataset(options):\n",
    "    \n",
    "    \n",
    "    #print(\"x_shape\", x.shape)\n",
    "    \n",
    "    file_labeled_train = pd.read_csv('IIoT_edge_base_classifier_pred_new.csv', skiprows=0, sep=',')\n",
    "    x_train=file_labeled_train.values[:, 0:30]\n",
    "    y_train=file_labeled_train.values[:, 30]\n",
    "    #y_train=pd.get_dummies(y_train)\n",
    "    #y_train = y_train.astype(np.float32)\n",
    "    #print(y_train.dtypes)\n",
    "    #y_train = y_train.to_numpy()\n",
    "    print(\"y_train_shape\", y_train.shape)\n",
    "    print(\"y_train\", y_train)\n",
    "    #y_class=len(set(y_train))\n",
    "    #print(\"y_class\", y_class)\n",
    "\n",
    "    \n",
    "    file_labeled_test = pd.read_csv('IIoT_edge_base_classifier_pred_test_new.csv', skiprows=0, sep=',')\n",
    "    x_test=file_labeled_test.values[:, 0:30]\n",
    "    y_test=file_labeled_test.values[:, 30]\n",
    "    #y_test=pd.get_dummies(y_test)\n",
    "    #print(y_test.dtypes)\n",
    "    #y_test = y_test.astype(np.float32)\n",
    "    #y_test = y_test.to_numpy()\n",
    "    print(\"y_test_shape\", y_test.shape)\n",
    "    print(\"y_test\", y_test)\n",
    "    #y_class_test=len(set(y_test))\n",
    "    #print(\"y_class_test\", y_class_test)\n",
    "    \n",
    "    #raise\n",
    "    \n",
    "    #train_data\n",
    "    #x_train=pd.DataFrame(x_test)\n",
    "    #y_train=pd.DataFrame(y)\n",
    "\n",
    "    #train_data=pd.concat([x_train, y_train],axis=1)\n",
    "    #print(\"train_data\", train_data.shape)\n",
    "    \n",
    "    #train_data.to_csv('IIoT_edge_base_classifier_pred_dummies.csv',index=False)\n",
    "\n",
    "    #test_data\n",
    "    #x_test=pd.DataFrame(x_test)\n",
    "    #y_test=pd.DataFrame(y_test)\n",
    "\n",
    "    #test_data=pd.concat([x_val, y_val], axis=1)\n",
    "    #print(\"test_data\", test_data.shape)\n",
    "    \n",
    "    #train_data.to_csv('IIoT_edge_base_classifier_pred_test_dummies.csv',index=False)\n",
    "\n",
    "    \n",
    "    #x_train_l,x_test,y_train_l,y_test=train_test_split(x_l, y ,random_state=0, test_size=0.5)\n",
    "    \n",
    "    #print(x_train_l.dtype) # This should not be 'object'\n",
    "    \n",
    "    #train_data = train_data.astype(float)\n",
    "    #test_data = test_data.astype(float)\n",
    "    #y_train_l = y_train_l.astype(float)\n",
    "    #y_test=y_test.astype(float)\n",
    "\n",
    "    #sc = MinMaxScaler()\n",
    "    #x_train_l = sc.fit_transform(x_train_l)\n",
    "    #x_test_l = sc.transform(x_test_l)\n",
    "    \n",
    "    #scaler=StandardScaler()\n",
    "    #x_train_l=scaler.fit_transform(x_train_l)\n",
    "    #x_test=scaler.fit_transform(x_test)\n",
    "    \n",
    "    #scaler=StandardScaler()\n",
    "    #x_train=scaler.fit_transform(x_train)\n",
    "    #x_test=scaler.fit_transform(x_test)\n",
    "\n",
    "\n",
    "    \n",
    "    train_dataset = TensorDataset(torch.from_numpy(x_train),torch.from_numpy(y_train))   \n",
    "    test_dataset = TensorDataset(torch.from_numpy(x_test),torch.from_numpy(y_test))   \n",
    "\n",
    "    \n",
    "    \n",
    "    #train_dataset = TensorDataset(torch.from_numpy(train_data))\n",
    "    #test_dataset = TensorDataset(torch.from_numpy(test_data))\n",
    "\n",
    "    \n",
    "    #sc = MinMaxScaler()\n",
    "    #unlabeled_dataset = sc.fit_transform(x)\n",
    "\n",
    "    \n",
    "    \n",
    "    #Feature scaling\n",
    "    #sc = StandardScaler()\n",
    "    #unlabeled_dataset = sc.fit_transform(x)\n",
    "\n",
    "    #converting to torch tensor\n",
    "    #train_dataset = torch.tensor(train_dataset, dtype=torch.float32)\n",
    "    #test_dataset = torch.tensor(test_dataset, dtype=torch.float32)\n",
    "\n",
    "    #print(\"train_dataset\")\n",
    "\n",
    "    \n",
    "    user_groups = split_iid(train_dataset, options.num_users)\n",
    "    print(\"user_group\", user_groups)\n",
    "    print(\"Done...\")\n",
    "    return train_dataset, test_dataset, user_groups\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#This class is a custom dataset that receives a base dataset and a list of indices. \n",
    "#It enables getting only a subset of the data in the base dataset, as specified by the indices list. \n",
    "#This is helpful in a federated learning scenario, where each client might only have access to a subset of the total data.\n",
    "class DatasetSplit(Dataset):\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = [int(i) for i in idxs]\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \n",
    "        data, target = self.dataset[self.idxs[item]]\n",
    "        return torch.tensor(data), torch.tensor(target)\n",
    "\n",
    "        #data = self.dataset[self.idxs[item]]\n",
    "        #return torch.tensor(data), torch.tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set program arguments\n",
    "options = lambda: None\n",
    "options.rounds= 10\n",
    "options.num_users= 15\n",
    "options.frac= 1\n",
    "options.local_ep= 10\n",
    "options.local_bs= 10\n",
    "options.lr= 0.01\n",
    "options.momentum= 0.5\n",
    "options.model= \"mlp\"\n",
    "options.kernel_num= 9\n",
    "options.kernel_sizes= \"3,4,5\"\n",
    "options.num_channels= 1\n",
    "options.norm= \"batch_norm\"\n",
    "options.num_filters= 32\n",
    "options.max_pool= \"True\"\n",
    "options.dataset= \"mnist\"\n",
    "options.num_classes= 10\n",
    "options.gpu= None\n",
    "options.optimizer= \"sgd\"\n",
    "options.iid= 1\n",
    "options.unequal= 0\n",
    "options.stopping_rounds= 10\n",
    "options.verbose= 1\n",
    "options.seed= 1\n",
    "options.batch_size = 100\n",
    "options.client_epochs = 50\n",
    "options.server_epochs=20\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = n_features=30\n",
    "hidden_size = [100, 100, 15]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.MLP= nn.Sequential(\n",
    "        nn.Linear(input_size, hidden_size[0]),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.01),\n",
    "        nn.Linear(hidden_size[0], hidden_size[1]),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.01),\n",
    "        nn.Linear(hidden_size[1], hidden_size[2]))\n",
    "        #nn.Softmax())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.MLP(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel(object):\n",
    "    def __init__(self, options, train_dataset, test_dataset, idxs, logger):\n",
    "        \n",
    "        self.options = options\n",
    "        self.logger = logger\n",
    "        self.train_loader = DataLoader(DatasetSplit(train_dataset, idxs),\n",
    "                             batch_size=options.batch_size, shuffle=True)\n",
    "        self.test_loader= DataLoader(test_dataset, batch_size= options.batch_size, shuffle=False)\n",
    "\n",
    "        #size=sys.getsizeof(self.train_loader)\n",
    "        #print(\"data_user\", size)\n",
    "        self.device = 'cuda' if options.gpu else 'cpu'\n",
    "        # Default criterion set to NLL loss function\n",
    "        #self.criterion = nn.NLLLoss()\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        #self.criterion= nn.BCELoss()\n",
    "        self.client_epochs=options.client_epochs\n",
    "        self.net=MLP().to(device)\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=0.01)\n",
    "\n",
    "        self.history = {}\n",
    "        self.history['train_loss'] = []\n",
    "        self.history['test_loss'] = []\n",
    "\n",
    "    def train(self, model):\n",
    "        mean_losses_superv = []\n",
    "        #self.net.train()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for epoch in range(self.options.client_epochs):\n",
    "            h = np.array([])\n",
    "            \n",
    "            for x, y in self.train_loader:\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                x=x.float()\n",
    "                \n",
    "                output = self.net(x)\n",
    "                \n",
    "                y=y.long()\n",
    "                \n",
    "                loss = self.criterion(output, y)\n",
    "                h = np.append(h, loss.item())\n",
    "                #raise\n",
    "                \n",
    "        # ===================backward====================\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                output = output.argmax(axis=1)\n",
    "                \n",
    "                total += y.size(0)\n",
    "                \n",
    "                y=y.float()\n",
    "                output=output.float()\n",
    "                \n",
    "                correct += (output == y).sum().item()\n",
    "                \n",
    "        #raise\n",
    "        # ===================log========================\n",
    "            mean_loss_superv = np.mean(h)\n",
    "            train_acc=correct/total\n",
    "            \n",
    "            mean_losses_superv.append(mean_loss_superv)\n",
    "            PATH = \"state_dict_model_Sup_IIoT_edge.pt\"\n",
    "            # Save\n",
    "            torch.save(self.net.state_dict(), PATH)\n",
    "            return sum(mean_losses_superv) / len(mean_losses_superv) , train_acc, self.net.state_dict() \n",
    "            #print('Done.....')\n",
    "\n",
    "   \n",
    "    def test_inference(self, model, test_dataset):\n",
    "        nb_classes = 15\n",
    "        confusion_matrix = np.zeros((nb_classes, nb_classes))\n",
    "        self.net.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        output_list=torch.zeros(0,dtype=torch.long)\n",
    "        target_list=torch.zeros(0,dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for data, target in self.test_loader:\n",
    "                \n",
    "                data, target = data.to(device), target.to(device)\n",
    "        \n",
    "                \n",
    "                output = self.net(data.float())\n",
    "                \n",
    "                batch_loss = self.criterion(output, target.long())\n",
    "                #print(\"done... test...\")\n",
    "                #raise\n",
    "                test_loss += batch_loss.item()\n",
    "                total += target.size(0)\n",
    "                \n",
    "                \n",
    "\n",
    "                target=target.float()\n",
    "                \n",
    "                output=output.argmax(axis=1)\n",
    "                output=output.float()\n",
    "                \n",
    "                \n",
    "                output_list=torch.cat([output_list, output.view(-1).long()])\n",
    "                target_list=torch.cat([target_list, target.view(-1).long()])\n",
    "                \n",
    "        \n",
    "                correct += (output == target).sum().item()\n",
    "        \n",
    "                    \n",
    "            test_loss/=total\n",
    "            acc=correct/total\n",
    "       \n",
    "    \n",
    "            F1_score= f1_score(target_list, output_list, average = \"macro\") #labels=np.unique(output_list))))\n",
    "            Precision=precision_score(target_list, output_list, average=\"macro\")                                      \n",
    "            Recall=recall_score(target_list, output_list, average=\"macro\")\n",
    "            class_report=classification_report(target_list,output_list, digits=4)\n",
    "\n",
    "            #print(' F1 Score : ' + str(f1_score(target_list, output_list, average = \"macro\"))) #labels=np.unique(output_list))))\n",
    "            #print(' Presicion : '+str(precision_score(target_list, output_list, average=\"macro\", labels=np.unique(output_list))))                                       \n",
    "            #print(' Recall : '+str(recall_score(target_list, output_list, average=\"macro\", labels=np.unique(output_list))))\n",
    "            #print(\"report\", classification_report(target_list,output_list, digits=4))\n",
    "\n",
    "            return acc, F1_score, Precision, Recall, class_report, test_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train_shape (24510,)\n",
      "y_train [ 8.  0.  4. ...  7.  2. 10.]\n",
      "y_test_shape (7004,)\n",
      "y_test [ 2. 10. 10. ...  3. 10.  3.]\n",
      "user_group {0: {16390, 6, 16398, 8206, 23, 8218, 16412, 31, 16418, 8233, 16426, 46, 16432, 53, 55, 61, 8255, 8257, 8259, 16464, 16465, 8286, 16478, 16481, 16491, 8302, 16500, 116, 8313, 123, 133, 138, 8333, 16530, 16532, 16543, 162, 172, 16563, 16571, 191, 16578, 195, 8389, 8392, 8393, 8394, 8398, 16593, 16598, 8407, 219, 8412, 8416, 8424, 16628, 247, 248, 16633, 8191, 8444, 16639, 8452, 261, 8453, 16647, 264, 288, 8489, 16684, 8496, 16688, 309, 314, 317, 16707, 16713, 16716, 339, 8536, 16729, 8557, 16767, 388, 395, 8592, 410, 8626, 16819, 8632, 8641, 16837, 456, 464, 16849, 470, 480, 8680, 16882, 16898, 521, 16908, 16914, 8727, 8735, 550, 558, 8759, 16952, 16955, 16958, 8770, 16963, 8774, 587, 16972, 8781, 600, 8803, 16997, 616, 8808, 17006, 624, 625, 8817, 627, 8819, 629, 8824, 8826, 8827, 638, 639, 17028, 649, 17034, 651, 8844, 654, 8857, 8858, 8863, 17059, 8870, 688, 702, 8896, 706, 711, 8904, 713, 17101, 8915, 17110, 17117, 17121, 17124, 741, 8939, 17135, 17137, 8949, 17148, 765, 771, 777, 780, 17166, 8974, 8982, 17181, 9002, 9006, 9007, 817, 824, 17222, 9030, 842, 846, 855, 860, 17247, 870, 9064, 9070, 9072, 9081, 913, 9115, 9127, 939, 944, 17329, 956, 9152, 9157, 9167, 9169, 9176, 17374, 9209, 1021, 9214, 17417, 1035, 1038, 1039, 1045, 9253, 1062, 17445, 9256, 17450, 1068, 1072, 17460, 17465, 17477, 9289, 9293, 1101, 9295, 9300, 9310, 9328, 17526, 17531, 1148, 1158, 1160, 1169, 1190, 1192, 1201, 17594, 9410, 1220, 9423, 1233, 9426, 17622, 17629, 1246, 9447, 1260, 17652, 1274, 1277, 9476, 1291, 9484, 17686, 17688, 17692, 17694, 1310, 9515, 17710, 9525, 1336, 17720, 17722, 9534, 9545, 1356, 9556, 17750, 9558, 17752, 17756, 1373, 1374, 9567, 17758, 9572, 9584, 9603, 1412, 9605, 1411, 9607, 17808, 17810, 1437, 9633, 1442, 1444, 1455, 9653, 17849, 1479, 9671, 17865, 1490, 9682, 17876, 17880, 17892, 9716, 1527, 1534, 17921, 17922, 17928, 17931, 1552, 1563, 17959, 1577, 17964, 17968, 9783, 1598, 17983, 9798, 17993, 9807, 1621, 9813, 9817, 1627, 1632, 9827, 9828, 9829, 9832, 18030, 9853, 18047, 9865, 18060, 1681, 9878, 18078, 1696, 18080, 18083, 9891, 18085, 1700, 9895, 1706, 9908, 18102, 18105, 9914, 18107, 9916, 18113, 1736, 9929, 18127, 9942, 1757, 18150, 9962, 9971, 1783, 9976, 1788, 1792, 18181, 9990, 18184, 10011, 18208, 10024, 10027, 10035, 18232, 1854, 18258, 1874, 1876, 10068, 1892, 10085, 10086, 18277, 18280, 18281, 1898, 1900, 10100, 1909, 1925, 1928, 18314, 1937, 1938, 1940, 1947, 18331, 18335, 1952, 10146, 1959, 18351, 1967, 1969, 18354, 18356, 1980, 18367, 10180, 10190, 1999, 2000, 2010, 10208, 10223, 2035, 10230, 18422, 2041, 18429, 10249, 18445, 10253, 2064, 18448, 10259, 18453, 2075, 18468, 18471, 10284, 2094, 10305, 2119, 18517, 2138, 18534, 18535, 10344, 10352, 18546, 2163, 18547, 2175, 10369, 2179, 2180, 18567, 2187, 2191, 2203, 18587, 10402, 18598, 18603, 10412, 18607, 2227, 10422, 18615, 10429, 18631, 2251, 18638, 10450, 10453, 18649, 2270, 18661, 2281, 10474, 10476, 18678, 18681, 18687, 2303, 2312, 18712, 18722, 10555, 2370, 2375, 18761, 18765, 2382, 10579, 18772, 18775, 18785, 2405, 10603, 2417, 10611, 10617, 18813, 2451, 10644, 18839, 2456, 10657, 18855, 2479, 18872, 18873, 18874, 2494, 10687, 18879, 10706, 2519, 2521, 10722, 18927, 18931, 18940, 18941, 10760, 10764, 18957, 10768, 10783, 10788, 2596, 18986, 10794, 10798, 19005, 10821, 10822, 2633, 2640, 19025, 19029, 2646, 19033, 10845, 19043, 2670, 19056, 10868, 19068, 10881, 2689, 2701, 19090, 10902, 19106, 10928, 10935, 10942, 2753, 2756, 19140, 10950, 19143, 19145, 2764, 10958, 2770, 10969, 19164, 19170, 2786, 2789, 2796, 2811, 2812, 11010, 11012, 19208, 11017, 2849, 19242, 19247, 19254, 19257, 2878, 2879, 11077, 2902, 11095, 19291, 11101, 2910, 2922, 2923, 11116, 2925, 19310, 2935, 11135, 2943, 2946, 2950, 19337, 19343, 11153, 2962, 2972, 11165, 11172, 11179, 2988, 11189, 19393, 3010, 3014, 3023, 11241, 11246, 11266, 3076, 19463, 19465, 3096, 19481, 3099, 3116, 11312, 11314, 19513, 11328, 3137, 19526, 11337, 11349, 19553, 19557, 3178, 11376, 11380, 3200, 19590, 11408, 3218, 3237, 11429, 3239, 19625, 19631, 11440, 3251, 3252, 3270, 3272, 19663, 19680, 11488, 11494, 3302, 11497, 19690, 11513, 3323, 3329, 19723, 19724, 11557, 3367, 19778, 3401, 11595, 3404, 19787, 11596, 3405, 11601, 11604, 11608, 11609, 3419, 3424, 11617, 11622, 11626, 19820, 11638, 3447, 19834, 3454, 3456, 11648, 11652, 11656, 11659, 11680, 3489, 19876, 19879, 3496, 3501, 11693, 3511, 11705, 19899, 11711, 3526, 11736, 3551, 3553, 19942, 3585, 19971, 3591, 19989, 11798, 11804, 19997, 11806, 11813, 3621, 3632, 3647, 20033, 3662, 20048, 3667, 11860, 3673, 20059, 3687, 3689, 11882, 11885, 3694, 20079, 11895, 3714, 11909, 3726, 3727, 20116, 11925, 11924, 3744, 3755, 20141, 11951, 3772, 20156, 20161, 11979, 20174, 20176, 3795, 20210, 20212, 3832, 20228, 3852, 20237, 20242, 20253, 12069, 3879, 12077, 3888, 3893, 3896, 12098, 3908, 12105, 3914, 3919, 20303, 20315, 3936, 3938, 3944, 3949, 20344, 12155, 12157, 20353, 12179, 12182, 12203, 12210, 12211, 20403, 4023, 4024, 12224, 4037, 12231, 12240, 20437, 4056, 12248, 4083, 20471, 4091, 20479, 4098, 12295, 20487, 4106, 12299, 4109, 12310, 20502, 12313, 20510, 4131, 20518, 12330, 12332, 12336, 20532, 12342, 12344, 12348, 12351, 12353, 4162, 20547, 12358, 12362, 12375, 12376, 12378, 12379, 20574, 12398, 4207, 4213, 20600, 20601, 12410, 4223, 4232, 20622, 4242, 12437, 4249, 4251, 20635, 20644, 20649, 12462, 4274, 20661, 4281, 4284, 4290, 4293, 12485, 4294, 12488, 12494, 12495, 12496, 4317, 12510, 20708, 20713, 12537, 20734, 4351, 12544, 4358, 4368, 20754, 12563, 4375, 4379, 20768, 4399, 20786, 20789, 12598, 4409, 20794, 12602, 12607, 12611, 20808, 12619, 4427, 20827, 20830, 20831, 4451, 4453, 20842, 20843, 20846, 12678, 12680, 20873, 4490, 20876, 12685, 20882, 12691, 12698, 20890, 20898, 12708, 4517, 4520, 4523, 12720, 4529, 20915, 4534, 12731, 4542, 20927, 4545, 20933, 12741, 12743, 12745, 4560, 4563, 20947, 4567, 20953, 4570, 20955, 12768, 12773, 20968, 12777, 4585, 4591, 4595, 20983, 4600, 4610, 12804, 12813, 12826, 21020, 21021, 21029, 4648, 12841, 4650, 21036, 12850, 4665, 4675, 4678, 12873, 4683, 21071, 4689, 4690, 4693, 4694, 21085, 4704, 21094, 12903, 4720, 21105, 21106, 21107, 12919, 4736, 21125, 12941, 4752, 4756, 21141, 21146, 4766, 4770, 12962, 21160, 21167, 21169, 4787, 12984, 4795, 4802, 12996, 12997, 21198, 13013, 13017, 13026, 13034, 21240, 21241, 13050, 13053, 13062, 13067, 13076, 13079, 4888, 13083, 4893, 4900, 21291, 13109, 21302, 21316, 4933, 13125, 13136, 21334, 13145, 21341, 4958, 21343, 13156, 21350, 21351, 21355, 13164, 21357, 21363, 21366, 13181, 13184, 13193, 21387, 13205, 13208, 13211, 21412, 21439, 21440, 5067, 13261, 5073, 13266, 21460, 21461, 21467, 21473, 5090, 21475, 13299, 21492, 13312, 21511, 21519, 21525, 21529, 5149, 5152, 13363, 13365, 5175, 13393, 5202, 13399, 21599, 13409, 21604, 13421, 13430, 5238, 13432, 13437, 21630, 21632, 13440, 21644, 13452, 5263, 21648, 21647, 5268, 21654, 13468, 21663, 13472, 5285, 5286, 13490, 5301, 13493, 21698, 13511, 21710, 5328, 21717, 5335, 21724, 5341, 5343, 13536, 21742, 5363, 5369, 13569, 13570, 13594, 13600, 13606, 5420, 5422, 21806, 13619, 5432, 21821, 5438, 21823, 21825, 5450, 13650, 21843, 13652, 5473, 21860, 5477, 21869, 5487, 21875, 5492, 5493, 21881, 13694, 5504, 13696, 5520, 13715, 5526, 13721, 21915, 13733, 5544, 13768, 5578, 21962, 13780, 13783, 13787, 13790, 5601, 13800, 22011, 13823, 22016, 5647, 22032, 22039, 5657, 5665, 13858, 13861, 13863, 5671, 13865, 5681, 5683, 22073, 22076, 13890, 13896, 13900, 13902, 22099, 22110, 5730, 13925, 13926, 5741, 22133, 5752, 5754, 13959, 5776, 5781, 5783, 22171, 5787, 13993, 22208, 5825, 14025, 5833, 14032, 22229, 5860, 5861, 5864, 14058, 5867, 5869, 22271, 14084, 5893, 5895, 22286, 22289, 14098, 22293, 5917, 5927, 14122, 14123, 14125, 22328, 22333, 14146, 22339, 5959, 22343, 5961, 14154, 22353, 14168, 22368, 14182, 14183, 22374, 22380, 22383, 14194, 22389, 22390, 14204, 6013, 14209, 14213, 22406, 6026, 22414, 22417, 6050, 22436, 6052, 22440, 6056, 14252, 6068, 6069, 6075, 14275, 14276, 22471, 14281, 6093, 6105, 22490, 6112, 22500, 6117, 22503, 22508, 14318, 14320, 6129, 22522, 14330, 14339, 22541, 22550, 6171, 14370, 6181, 14374, 22572, 6194, 22585, 6205, 6221, 22622, 22627, 22644, 22647, 22652, 6274, 6280, 6285, 14485, 14494, 14503, 6315, 14509, 22702, 22714, 22718, 6339, 22726, 14536, 6344, 22737, 22744, 22750, 6370, 6377, 22763, 14575, 22770, 6387, 22779, 22786, 14595, 14596, 22787, 6406, 6419, 22805, 14616, 14618, 6431, 14624, 6432, 6435, 6440, 14639, 22832, 14648, 14649, 6461, 6466, 22860, 22876, 22880, 14691, 6501, 22889, 22896, 22906, 22910, 6528, 6529, 14721, 22915, 14724, 6540, 6542, 14735, 6549, 22934, 6563, 14756, 14764, 14767, 22961, 14779, 6589, 14781, 22975, 22986, 6606, 14804, 23006, 6624, 14824, 23017, 6642, 14837, 23034, 6655, 6676, 14868, 23064, 23070, 23073, 6691, 6696, 14897, 23099, 23100, 23101, 14917, 6725, 14921, 14927, 14942, 23135, 6760, 14956, 23158, 14966, 6778, 14973, 14974, 6783, 23172, 6793, 14993, 6803, 6805, 23194, 23199, 23210, 15024, 23226, 6851, 15047, 6857, 15054, 6863, 23250, 6870, 15065, 23270, 15081, 23275, 15083, 15094, 15095, 15110, 6923, 23311, 15122, 6931, 23326, 23330, 6950, 6956, 23342, 15154, 23350, 6970, 6976, 15172, 6981, 6996, 23386, 23391, 23393, 7015, 23415, 7040, 15236, 23430, 15239, 7063, 15257, 23456, 7080, 15273, 23472, 7091, 7095, 23479, 7098, 23493, 7110, 23506, 7127, 23518, 23519, 15331, 23523, 23537, 7160, 15353, 23553, 23560, 7187, 7190, 23574, 23579, 15390, 23582, 7205, 15400, 23602, 7219, 23611, 23612, 15424, 15425, 15438, 7263, 7264, 7266, 7272, 7275, 7283, 23667, 15487, 15495, 15497, 15498, 23692, 23695, 15505, 23709, 7343, 7359, 23744, 15553, 23743, 7368, 15564, 15565, 23759, 7391, 7400, 23794, 15603, 7414, 7418, 15618, 23820, 15630, 23827, 15635, 7444, 7449, 23844, 7465, 7476, 15675, 15679, 15680, 7491, 23881, 23888, 15714, 23922, 23938, 23941, 23950, 15761, 15763, 15766, 23959, 15781, 15789, 15791, 15803, 7615, 7618, 24002, 24004, 24011, 24012, 7628, 15824, 15825, 7634, 15872, 7692, 7702, 15895, 24086, 15901, 7714, 15926, 15936, 15953, 7763, 24166, 7786, 7793, 24179, 7795, 24181, 7798, 7797, 24184, 24186, 15999, 7809, 7811, 16012, 16014, 24207, 24213, 16031, 7847, 24233, 7852, 24241, 16053, 16056, 24258, 24259, 7885, 24270, 24269, 7888, 7889, 7892, 16090, 24288, 7914, 7916, 7917, 24310, 16122, 24319, 16127, 24333, 16154, 7973, 16168, 16170, 7981, 24369, 24371, 16181, 7993, 7994, 8000, 16192, 8004, 16199, 8008, 16201, 24394, 16204, 24397, 8017, 8028, 24416, 24425, 8053, 24441, 16252, 24457, 16266, 16268, 16275, 16281, 8103, 16299, 24493, 16306, 24502, 16310, 8120, 16317, 16330, 16335, 16336, 8144, 8154, 8166, 16367, 8177, 16383}, 1: {0, 8193, 8201, 19, 21, 8217, 26, 33, 8229, 51, 66, 67, 78, 8289, 8301, 8303, 16502, 8312, 16505, 16514, 16516, 16517, 139, 141, 16529, 157, 8359, 167, 16555, 174, 16567, 8378, 189, 8386, 201, 8395, 16587, 16617, 8427, 16623, 242, 16635, 8449, 257, 266, 16652, 16657, 16660, 8472, 284, 8490, 318, 8512, 16709, 8518, 333, 344, 361, 16753, 371, 16759, 381, 16775, 8601, 16800, 16803, 16806, 16813, 430, 16821, 16823, 16824, 441, 445, 8639, 451, 460, 462, 16852, 8676, 8679, 489, 16884, 8696, 507, 16892, 520, 16911, 16924, 16925, 8739, 8741, 8749, 560, 566, 568, 572, 16965, 582, 16969, 16970, 586, 8793, 605, 619, 640, 8834, 652, 659, 8865, 674, 8869, 17063, 691, 8884, 17081, 8901, 17105, 726, 728, 8926, 17132, 17145, 8956, 17158, 8968, 788, 8981, 789, 17174, 17178, 9003, 9012, 17207, 9017, 9019, 17214, 17217, 17224, 841, 17226, 849, 851, 854, 856, 858, 9055, 17250, 9060, 17260, 880, 881, 888, 17288, 9101, 17294, 922, 17306, 17311, 930, 9128, 9131, 17323, 9133, 945, 9143, 962, 17347, 971, 17355, 17364, 9173, 9175, 984, 9177, 9180, 994, 996, 998, 9191, 9198, 9201, 1025, 17415, 9234, 1047, 9240, 9239, 17435, 17444, 17452, 17453, 1078, 1080, 17476, 9287, 9296, 1107, 9299, 17493, 9304, 9311, 17506, 17508, 1130, 1134, 9330, 1139, 17529, 17530, 9341, 9349, 1157, 9356, 9357, 1173, 9370, 17562, 1178, 9373, 9374, 1182, 1184, 17566, 9388, 1199, 9396, 9399, 1215, 1218, 9413, 1223, 9419, 1228, 1232, 9430, 17628, 17636, 1255, 17639, 1257, 1261, 1270, 9466, 1280, 1292, 17680, 1302, 1317, 9518, 1326, 9520, 17714, 1335, 9529, 1340, 9541, 9555, 17747, 9570, 9575, 9576, 17769, 17772, 1390, 17776, 9589, 1400, 17784, 9594, 17789, 1408, 9601, 17803, 9621, 1448, 9642, 1451, 17836, 17838, 1462, 17851, 17856, 17857, 9667, 17862, 17863, 1484, 17873, 17874, 1496, 17887, 9697, 17896, 17898, 17899, 17901, 17911, 1537, 17924, 17926, 17937, 1553, 9747, 9748, 17945, 9756, 9758, 9765, 17966, 9781, 9790, 9791, 1599, 9795, 9796, 9805, 1622, 1629, 9826, 18028, 18029, 18031, 1649, 18038, 1657, 18042, 18052, 1669, 18053, 1687, 18071, 9888, 1697, 9893, 9894, 9904, 1723, 18110, 1727, 1732, 1737, 1738, 1747, 1748, 9941, 9945, 1782, 18166, 18179, 9987, 9989, 1807, 1809, 18197, 10028, 10033, 18225, 1846, 10038, 18233, 1855, 1861, 18247, 18262, 1889, 10082, 10081, 18273, 1906, 10103, 18298, 1917, 10112, 10163, 18366, 10176, 1986, 10184, 18381, 18384, 18387, 18390, 18391, 2007, 2011, 10203, 2023, 10219, 18412, 18415, 18428, 10236, 10237, 10239, 2048, 2049, 2050, 18437, 2065, 10271, 2086, 10282, 2093, 2098, 18484, 18492, 10307, 2120, 10322, 2132, 10326, 2143, 2145, 2150, 18536, 18537, 10346, 2155, 18541, 2158, 2157, 10353, 18552, 18555, 2173, 10377, 2188, 10393, 18588, 10407, 18602, 10431, 2240, 10437, 10438, 2246, 2259, 2262, 18648, 18650, 2284, 10478, 2288, 18696, 10511, 2320, 2323, 2330, 10523, 18720, 18726, 2344, 10541, 18733, 18736, 2353, 18745, 2372, 10565, 18760, 2377, 18774, 10594, 2409, 2413, 18797, 2415, 10609, 18802, 10623, 2450, 10645, 18841, 2459, 18850, 10673, 18878, 2505, 2507, 2515, 10711, 10712, 2523, 18914, 18915, 10724, 2533, 10740, 18938, 2561, 10758, 2573, 2576, 10778, 18973, 10792, 2602, 10797, 10801, 2610, 10802, 19008, 10823, 19016, 19018, 2638, 10834, 2648, 2654, 19040, 10850, 10857, 2674, 19061, 19064, 19078, 19080, 19085, 19087, 10897, 10898, 19091, 2710, 2712, 19097, 2723, 2736, 19128, 2748, 2750, 2752, 10946, 19139, 2761, 19148, 19153, 19162, 10979, 10980, 19176, 19180, 19188, 2807, 11002, 2817, 19205, 19210, 11021, 2833, 11025, 2840, 2842, 11050, 2863, 2867, 19252, 2870, 19263, 11083, 19281, 2897, 11096, 19289, 2908, 11100, 19297, 2921, 19306, 19312, 19315, 11127, 19322, 11147, 19340, 11154, 19347, 2965, 11173, 11185, 19381, 19382, 19389, 19390, 19392, 19395, 3012, 19401, 3018, 19411, 11229, 3052, 11244, 19440, 11250, 11251, 19451, 11271, 3079, 11273, 19473, 11281, 11283, 11285, 11286, 19482, 3107, 19491, 19494, 3115, 19502, 19512, 3132, 19518, 19530, 3167, 19552, 3168, 11363, 19558, 3175, 11378, 11379, 3193, 19577, 3203, 3206, 19591, 19593, 11412, 11416, 19608, 19611, 11421, 3232, 11431, 19623, 19626, 3249, 11452, 3268, 11460, 19653, 11464, 11468, 19661, 3290, 19684, 19687, 3314, 19699, 3338, 11535, 3343, 19729, 19743, 19752, 19756, 3372, 3378, 3385, 19770, 3387, 11583, 11584, 19785, 19793, 3420, 11614, 11621, 3430, 3437, 19829, 19830, 19833, 19835, 11662, 3475, 19859, 11676, 11679, 11681, 19875, 3502, 19887, 19892, 3512, 19901, 19909, 3529, 11727, 19933, 11743, 3558, 3566, 11759, 3572, 11765, 3574, 3573, 19962, 19965, 3582, 11782, 3592, 11788, 19983, 11794, 3617, 11810, 20003, 11811, 20008, 11819, 11823, 3635, 20022, 11831, 11838, 3646, 3650, 3655, 3657, 3665, 11859, 3668, 11861, 20058, 20061, 20067, 3684, 20069, 11878, 3705, 11900, 3712, 11910, 11911, 11912, 3720, 11915, 20110, 3736, 3740, 20125, 3749, 20137, 11948, 11955, 20149, 3768, 11963, 11967, 11978, 3803, 20191, 12003, 20199, 3818, 20204, 3823, 3825, 20214, 12035, 3847, 20236, 3855, 3866, 3875, 12071, 20268, 20269, 12076, 12081, 20285, 3902, 20294, 3911, 20299, 3917, 3918, 20305, 12123, 3931, 12127, 20322, 3939, 12135, 12139, 12140, 3955, 3957, 20349, 3965, 3967, 20359, 3977, 20361, 3980, 3990, 20379, 20389, 4014, 20402, 12218, 12225, 4035, 12227, 12235, 4043, 20440, 12250, 20446, 4071, 12265, 4077, 12270, 20467, 4088, 4094, 12286, 20484, 4100, 4102, 4107, 20492, 4124, 4129, 20523, 12335, 4145, 4151, 12345, 4155, 20540, 4157, 12350, 20554, 12372, 20568, 4184, 20573, 4189, 12386, 12394, 4210, 12407, 4220, 12413, 20607, 4224, 20611, 12443, 12445, 4258, 12453, 12475, 12484, 4292, 20678, 4298, 20687, 12503, 12505, 20705, 4323, 4330, 4344, 12541, 12545, 4365, 12560, 12565, 4374, 4378, 12572, 20765, 12584, 4394, 4397, 12595, 4404, 4410, 4412, 4418, 12612, 4421, 20810, 12638, 4447, 4448, 4456, 4466, 12658, 4475, 12681, 12684, 20877, 12687, 12688, 12690, 4500, 12694, 4508, 12707, 12717, 20911, 12723, 4544, 20929, 12740, 4554, 12746, 4556, 12748, 12756, 4564, 20950, 12761, 12764, 4573, 20958, 20956, 20961, 20970, 4592, 20981, 12790, 4602, 12797, 4605, 4608, 12805, 4618, 4620, 21007, 4625, 4632, 4662, 12857, 21054, 12865, 4677, 12875, 4691, 21086, 21098, 4726, 12930, 12935, 4744, 21129, 21135, 12947, 21140, 21139, 12950, 21142, 12955, 4765, 21149, 21154, 12971, 4783, 4788, 4809, 21196, 21211, 21216, 13027, 21220, 21239, 4858, 4865, 21261, 13070, 21277, 21281, 4915, 21303, 4929, 21317, 21319, 13129, 21325, 4943, 13137, 4954, 13149, 4959, 21347, 13157, 13160, 21359, 21367, 21369, 21374, 21378, 21381, 5002, 13200, 21404, 5022, 13219, 13222, 13236, 5052, 5054, 13252, 13253, 13270, 13271, 5084, 21488, 21509, 5129, 5134, 5135, 21521, 21524, 5155, 13348, 5169, 13362, 13366, 13367, 13370, 5179, 13372, 21579, 5197, 21582, 21584, 21588, 21591, 5207, 21592, 5211, 21597, 13406, 13407, 5217, 21613, 13434, 21633, 21634, 5252, 21641, 5274, 13466, 13470, 21664, 13474, 13481, 5293, 21679, 5296, 13495, 21688, 13506, 13507, 21707, 5327, 21715, 13529, 21722, 13531, 13541, 13556, 13558, 21758, 21771, 5389, 21782, 5399, 21802, 21807, 13624, 13631, 13637, 5447, 13641, 21836, 5452, 21841, 21850, 5472, 13664, 13670, 5482, 5486, 13680, 5496, 13691, 13693, 21898, 5519, 5528, 5529, 13726, 21924, 5541, 21931, 21937, 21939, 5557, 13763, 5574, 13767, 21964, 21974, 5591, 13782, 21984, 13793, 13795, 5609, 13801, 21995, 21993, 21997, 22000, 13819, 5628, 13820, 5636, 13837, 13851, 5670, 13867, 13872, 13875, 22068, 5686, 5690, 22078, 13889, 5698, 13903, 5719, 22117, 13927, 13935, 5748, 13941, 5757, 5758, 22143, 13967, 5775, 22161, 5777, 22160, 22167, 5788, 5789, 13983, 13985, 13989, 13999, 5807, 14004, 22203, 22222, 5846, 14041, 5850, 22235, 14048, 22244, 22249, 22251, 22252, 14060, 22253, 22257, 5879, 22263, 14074, 5883, 22268, 14077, 14076, 5890, 22287, 14096, 5908, 22294, 22298, 14108, 14114, 22310, 22315, 5936, 14140, 22338, 5960, 14152, 5962, 22350, 5966, 14164, 14173, 14178, 14180, 22375, 14190, 22384, 14196, 22391, 14199, 22404, 22407, 22411, 6032, 14225, 22419, 14230, 6046, 22430, 14241, 22446, 6066, 22456, 14267, 22460, 6080, 22470, 22486, 14307, 22505, 6125, 6128, 6156, 14349, 6167, 14360, 22555, 6199, 22587, 14405, 6217, 14412, 22610, 6230, 22615, 6237, 22623, 22626, 14434, 14439, 6247, 22636, 6255, 14460, 22656, 14464, 22657, 14468, 22662, 6281, 14477, 22671, 22677, 14487, 6296, 22682, 14491, 6300, 6305, 22689, 22701, 14512, 22706, 14515, 14517, 6326, 6325, 22711, 14533, 22730, 6347, 6355, 6369, 14563, 14568, 22761, 6384, 6385, 14582, 6397, 6399, 6408, 22811, 14620, 22816, 22822, 22830, 14656, 6464, 14664, 14682, 22874, 14685, 14687, 14688, 22879, 14690, 14693, 22890, 22899, 22900, 22901, 14710, 6522, 14715, 22917, 14726, 22921, 22927, 6544, 14739, 6548, 22941, 22950, 14759, 14760, 6577, 22984, 22987, 6605, 6607, 6608, 14816, 6634, 23027, 23032, 6653, 6654, 6657, 14855, 23048, 23054, 14864, 23062, 14882, 14884, 14893, 23088, 14896, 6720, 23104, 23111, 14925, 14934, 6748, 23139, 14949, 14953, 23154, 23161, 23164, 23166, 23167, 6784, 23173, 14987, 6809, 23198, 6816, 15014, 23216, 23223, 23227, 15039, 23234, 6861, 23252, 23254, 23257, 23259, 15073, 15076, 23274, 6890, 23277, 15086, 23292, 23293, 15102, 15106, 6915, 23299, 23321, 15130, 15133, 6949, 15146, 15147, 23343, 6966, 15167, 23363, 15174, 15187, 23387, 7005, 15199, 7007, 15207, 15212, 7022, 7023, 7032, 23424, 23426, 7047, 7050, 23445, 15262, 15266, 7077, 23463, 23471, 23476, 7104, 7111, 7112, 15304, 15315, 7126, 7149, 15343, 15345, 7167, 15360, 15362, 23554, 23556, 15366, 7179, 7184, 23569, 7186, 15379, 23572, 23573, 7216, 7233, 7237, 23625, 23634, 15443, 15447, 23639, 15457, 23652, 23655, 7276, 23663, 23678, 7295, 7302, 23689, 23703, 15527, 15534, 15536, 7345, 7347, 15540, 7350, 23746, 7365, 23750, 7370, 23758, 7385, 7386, 7388, 15593, 23785, 7403, 23788, 7407, 23792, 7408, 7421, 7426, 15634, 7450, 23838, 15651, 15657, 15660, 15666, 23858, 7479, 15686, 23880, 7498, 23895, 7513, 23902, 23909, 23912, 15720, 23913, 15724, 23921, 15734, 7552, 7554, 15750, 7563, 23957, 7579, 7584, 7598, 7601, 23986, 7608, 7609, 15807, 7620, 7621, 7623, 15826, 24022, 24025, 15848, 15850, 15851, 7673, 24063, 7682, 15880, 7696, 24092, 24110, 15931, 7745, 24134, 7750, 7756, 7758, 24148, 7769, 7774, 7778, 15976, 24170, 24171, 24173, 24176, 15990, 7800, 24188, 7805, 24192, 7819, 24208, 16019, 7830, 24218, 16032, 16038, 24235, 24250, 7869, 16062, 7872, 7878, 24263, 16075, 16085, 24280, 24284, 7908, 24302, 24313, 24316, 16124, 7940, 7950, 16145, 7955, 24348, 7968, 24366, 16182, 7996, 16212, 8023, 8027, 8035, 8037, 16230, 24423, 24426, 8044, 24440, 8065, 8066, 8073, 16267, 16272, 16276, 8084, 8085, 16294, 16295, 8104, 24489, 24496, 8113, 24497, 8114, 16308, 24501, 8118, 8140, 16339, 16342, 8159, 8172, 16370, 16372, 16374, 8183, 16377, 16378, 16380, 8190}, 2: {3, 7, 8204, 13, 16397, 16399, 15, 8221, 32, 8225, 38, 16423, 8235, 8238, 8239, 47, 16435, 57, 62, 16446, 64, 16457, 79, 8273, 8274, 8275, 16470, 16475, 97, 99, 8296, 16490, 16495, 115, 8311, 16507, 16508, 8326, 8330, 16526, 8342, 16534, 8345, 8362, 179, 181, 8375, 8377, 8383, 8384, 198, 8397, 16596, 8405, 225, 16613, 16622, 251, 16640, 16645, 269, 8463, 272, 16656, 16663, 8479, 8485, 8486, 297, 299, 8491, 16685, 8499, 310, 8503, 8505, 16697, 8513, 330, 16717, 8531, 16728, 349, 16745, 16748, 365, 16751, 16756, 16758, 376, 8578, 386, 16781, 16791, 16793, 16797, 415, 16804, 8625, 8628, 437, 8634, 8651, 16853, 8661, 16854, 16857, 16861, 16868, 8690, 8695, 504, 8697, 511, 8718, 528, 16915, 531, 16917, 547, 548, 16938, 557, 16942, 8753, 562, 8758, 8760, 8766, 16959, 580, 16966, 16974, 8786, 16985, 604, 16995, 8806, 630, 17019, 637, 8835, 8840, 17035, 8847, 8848, 8852, 661, 663, 665, 666, 17054, 17064, 8873, 8875, 685, 686, 689, 692, 8894, 8895, 8898, 17092, 8910, 17107, 727, 17114, 8928, 17120, 17123, 8931, 8937, 8945, 8946, 758, 8964, 776, 17168, 791, 8983, 8986, 8988, 796, 17182, 17184, 8993, 808, 809, 17192, 17219, 9033, 9040, 17243, 9053, 9063, 873, 876, 17261, 17272, 890, 9088, 9089, 17295, 911, 17314, 935, 17337, 9151, 9153, 967, 989, 17373, 17375, 9184, 17378, 9190, 9199, 9203, 17399, 17402, 1026, 17419, 17421, 9236, 17431, 1052, 17436, 17449, 1065, 1067, 1075, 1077, 9277, 1087, 9280, 1091, 1099, 1102, 9297, 9302, 17496, 17498, 1124, 9322, 9323, 17516, 17521, 9332, 1141, 1146, 17538, 17541, 17543, 17548, 9360, 9366, 1185, 1194, 17579, 17580, 9390, 1212, 17598, 17608, 1231, 9424, 1239, 1242, 1244, 17632, 9444, 1253, 17640, 9456, 9457, 1271, 1273, 17661, 1282, 17666, 17678, 9500, 1320, 17712, 1328, 1343, 9542, 1351, 17735, 9550, 1376, 1395, 9588, 9593, 1403, 9602, 17796, 17799, 9611, 1420, 17804, 1424, 17811, 1428, 1432, 17817, 9627, 9629, 17825, 9635, 9640, 1449, 1452, 1467, 1477, 9672, 1483, 17872, 1488, 17875, 1494, 1499, 1503, 9738, 9740, 17946, 9755, 1564, 9761, 17955, 9763, 17957, 1574, 1573, 9769, 9776, 1584, 1591, 17980, 17986, 17991, 17995, 1620, 9820, 9835, 9837, 9839, 18032, 9842, 9844, 9847, 9849, 1659, 9858, 1668, 1674, 9867, 18065, 18068, 9881, 9882, 18077, 9897, 9905, 18097, 18104, 1720, 18117, 9928, 1744, 18131, 9940, 1750, 1755, 1771, 1785, 9978, 9980, 1790, 9983, 9992, 1805, 9999, 1813, 1815, 18203, 18206, 10022, 1836, 18223, 18229, 18234, 1850, 1852, 18237, 10045, 1857, 10049, 1867, 10062, 18260, 1877, 10077, 18270, 18289, 1914, 1920, 10115, 1926, 10135, 10138, 1949, 18333, 1957, 18349, 1975, 18365, 1982, 18372, 10182, 1992, 18377, 10192, 10199, 2009, 18395, 18416, 18421, 2043, 10245, 2069, 18457, 18458, 2081, 2091, 18496, 10309, 18505, 2136, 18523, 2142, 18532, 10341, 18538, 10348, 10360, 2177, 10372, 10383, 18577, 2194, 2199, 18583, 10397, 10398, 18593, 18601, 10410, 18624, 10436, 2248, 10441, 18637, 2255, 10451, 2261, 10459, 10462, 18655, 2274, 18664, 2287, 18672, 10493, 18693, 10502, 18695, 18708, 2325, 10522, 18716, 10530, 2340, 18730, 10551, 10558, 2368, 18762, 18770, 2398, 10592, 10593, 18794, 18798, 10612, 2423, 18817, 2438, 18822, 18827, 10636, 2453, 18840, 2470, 10663, 2473, 2476, 2477, 2478, 18862, 18864, 18865, 18884, 2514, 18911, 18912, 18917, 10726, 2535, 2539, 10736, 10738, 18944, 10752, 10754, 18951, 18953, 2575, 18965, 2586, 18974, 2591, 10784, 10796, 10806, 2623, 2624, 2630, 19017, 19022, 19031, 19034, 19035, 2656, 19042, 10872, 19070, 19075, 10887, 10888, 19079, 19089, 2706, 10903, 10909, 19102, 10914, 19107, 2725, 19114, 19116, 10925, 10930, 2738, 19137, 10949, 2757, 10962, 10965, 19159, 2779, 10971, 19165, 10996, 19216, 19228, 2846, 19232, 2851, 19243, 19244, 19246, 11056, 11060, 11062, 11068, 2887, 11080, 19296, 19299, 2916, 11110, 11113, 2924, 19317, 19318, 2937, 19327, 11139, 11141, 19336, 19348, 19357, 11166, 19359, 11174, 2984, 11182, 11192, 19400, 11209, 3016, 11210, 11212, 19410, 11221, 11225, 11227, 19420, 3038, 3041, 19437, 19443, 3059, 19446, 3063, 19448, 11263, 11264, 3072, 19458, 11267, 3078, 19466, 11274, 19486, 11297, 3114, 11308, 11309, 11310, 19505, 19506, 11319, 19514, 11323, 19522, 11335, 19529, 11342, 3151, 11354, 19547, 11361, 19572, 3188, 11391, 11392, 19595, 11405, 11409, 3220, 11423, 11448, 19643, 11453, 19646, 11458, 3267, 11466, 19659, 19674, 3293, 19678, 11485, 3296, 19683, 11493, 19696, 3313, 3316, 3320, 19705, 11517, 3327, 3328, 19715, 19718, 3337, 19722, 11546, 19739, 11570, 19763, 19775, 3394, 11590, 11620, 19813, 19819, 3438, 3441, 11657, 11664, 19858, 11672, 19871, 11685, 19888, 3509, 19919, 19935, 11748, 11753, 11754, 11758, 11768, 19963, 11787, 11792, 19994, 19999, 20005, 3631, 3633, 3636, 3640, 11833, 11837, 20032, 20040, 20045, 3663, 11858, 11864, 3676, 3677, 11872, 20076, 11905, 3716, 3725, 20112, 3733, 11928, 3738, 11932, 11934, 3742, 3753, 11956, 3765, 11961, 11962, 11975, 11992, 20188, 3807, 12005, 3814, 12011, 20205, 12018, 12027, 12045, 3856, 12049, 12050, 3860, 3864, 3870, 3871, 12064, 20261, 12074, 3884, 3889, 3891, 12084, 20282, 12091, 12104, 12107, 20307, 20308, 20311, 3928, 12129, 12130, 3937, 12133, 12136, 12138, 20331, 20333, 20341, 12153, 12156, 3968, 20354, 3983, 20368, 20371, 3989, 12183, 20380, 12201, 4011, 20404, 12214, 12216, 4025, 20411, 20412, 20413, 12222, 12233, 4044, 4045, 12243, 4052, 4061, 12256, 20453, 12263, 20457, 12278, 4087, 4099, 12300, 20497, 12311, 4119, 4123, 12318, 12320, 20514, 12323, 12324, 4130, 20522, 4146, 4147, 20533, 20537, 20545, 20553, 20565, 20584, 4205, 20589, 4211, 12429, 12432, 12433, 20630, 12451, 4262, 4263, 20651, 12466, 12470, 12471, 4286, 12479, 12486, 12490, 4303, 20689, 20693, 12501, 20696, 20697, 20703, 4324, 4347, 20731, 4349, 12549, 20749, 12557, 20774, 12582, 20777, 20780, 4405, 20798, 4414, 4422, 4423, 20832, 20833, 20838, 12657, 12661, 12670, 12674, 12675, 4483, 4488, 12683, 20880, 20881, 4501, 12710, 4527, 12725, 4540, 20928, 4549, 20938, 4566, 20954, 12766, 4577, 4582, 12779, 12788, 20986, 20990, 12799, 12800, 20993, 12803, 4612, 4613, 20999, 21000, 4621, 12822, 21015, 4634, 4639, 4663, 4664, 4668, 4670, 4676, 4685, 21074, 12884, 21080, 12905, 4719, 4728, 12925, 4733, 21117, 21122, 4740, 4751, 12944, 12948, 12951, 12958, 21151, 21156, 4777, 4789, 4798, 4800, 4803, 13009, 21207, 13015, 4825, 13020, 13021, 21227, 4844, 4843, 4851, 4852, 21267, 13084, 13091, 4899, 13093, 4908, 13101, 13102, 21295, 21299, 4918, 13113, 4922, 21308, 13120, 21314, 13122, 13124, 21321, 4941, 4942, 13135, 13146, 21340, 13161, 4973, 4977, 13172, 4981, 21368, 21372, 4998, 21385, 13196, 21395, 5013, 21397, 21399, 5014, 5025, 21409, 5027, 21410, 13226, 21420, 5037, 5041, 13238, 5046, 21434, 21447, 21448, 5065, 13272, 5094, 13293, 13304, 13317, 13320, 5128, 13325, 13327, 21526, 5146, 21534, 13346, 5157, 21545, 5173, 13377, 21573, 21576, 5199, 21583, 13394, 5209, 21594, 13417, 13420, 13426, 5239, 13431, 21627, 13439, 21631, 13444, 13447, 13453, 21645, 13459, 5277, 21674, 21675, 13484, 5308, 5317, 21702, 21708, 21716, 13528, 21726, 5358, 13551, 5360, 21753, 21756, 13572, 21765, 5385, 21772, 5405, 21793, 5409, 13616, 13618, 13621, 21819, 5445, 21840, 5461, 5503, 21889, 21890, 13699, 5508, 13704, 5517, 13712, 13718, 5535, 21922, 21923, 13743, 13747, 5556, 13752, 13758, 13764, 13765, 21959, 13777, 21970, 13779, 13778, 21971, 5593, 21980, 21986, 21989, 5616, 13811, 13821, 5637, 13830, 13833, 22036, 22038, 22040, 13848, 13859, 22058, 13882, 5710, 13911, 22106, 22109, 13918, 22115, 5733, 5755, 5759, 5769, 22156, 13969, 13992, 13995, 22188, 13997, 14000, 14010, 5820, 22213, 14021, 5832, 5834, 22218, 22219, 14034, 22231, 5848, 14044, 14051, 5865, 14067, 14068, 5886, 22275, 5892, 22277, 14086, 5901, 5902, 14095, 22308, 5925, 14116, 14132, 14136, 5944, 22330, 14150, 14156, 22355, 22358, 6004, 6014, 14227, 22420, 6037, 14235, 14237, 6055, 6062, 22453, 6070, 6071, 6091, 14284, 14286, 22485, 14310, 14311, 22506, 14329, 22524, 14334, 6150, 6152, 6157, 6161, 22546, 6162, 22545, 22566, 14381, 22578, 22583, 6201, 6202, 6203, 6207, 22598, 22601, 6226, 22612, 14425, 14437, 22631, 6251, 14453, 14461, 22659, 6279, 6289, 6292, 22684, 6303, 6304, 14506, 6320, 22705, 22710, 14526, 14528, 22733, 14543, 6361, 14554, 6363, 22751, 22752, 22757, 6375, 22760, 22762, 6378, 6379, 22784, 14598, 6407, 22794, 22797, 14608, 22836, 14646, 14651, 14654, 6463, 22847, 22851, 14663, 14667, 14668, 6478, 22865, 14674, 6482, 22866, 22872, 14681, 6491, 22882, 6499, 22885, 14694, 22895, 22903, 14714, 14723, 22916, 6534, 22920, 22923, 14731, 22926, 22929, 14743, 22938, 22940, 22943, 22949, 6570, 14763, 22959, 22963, 6581, 14774, 22971, 6588, 14788, 6598, 14795, 6611, 14814, 6626, 6630, 23016, 23020, 6639, 6640, 23039, 14848, 23043, 6661, 6663, 6685, 23071, 14887, 14888, 6701, 6704, 6712, 6724, 14920, 6729, 6732, 23118, 6734, 6740, 14939, 14945, 23142, 23149, 6777, 6780, 6782, 23170, 14979, 6794, 6795, 14992, 23184, 23190, 23195, 6814, 6815, 15010, 6829, 23214, 15023, 6832, 6840, 15040, 23236, 15046, 23247, 6868, 15064, 6873, 6872, 23261, 15072, 6884, 15084, 6893, 23280, 6899, 23289, 15098, 15097, 23291, 23295, 15105, 6916, 23307, 6944, 23329, 23331, 6951, 15144, 23338, 15148, 15152, 15153, 15155, 23351, 15166, 6980, 15176, 6984, 15178, 23369, 23377, 23383, 7004, 23392, 7018, 7020, 7028, 23439, 7055, 23446, 15256, 7069, 15264, 7074, 7087, 15288, 23487, 15301, 7113, 7118, 15313, 15318, 15326, 15328, 7136, 23520, 23527, 23535, 7159, 15354, 23548, 23555, 15372, 15374, 7185, 23571, 15381, 23580, 15389, 23585, 23589, 23590, 23595, 15406, 23609, 23610, 23620, 15429, 7241, 7243, 23635, 15446, 7254, 7261, 23649, 23651, 15461, 23659, 7285, 23670, 15485, 15503, 15504, 23697, 15523, 15526, 15532, 23727, 23728, 15539, 15541, 7351, 23737, 7360, 15557, 15573, 23767, 23769, 23772, 23781, 15595, 23799, 23807, 23815, 7440, 23826, 23831, 7455, 7463, 23849, 23862, 7478, 23874, 23877, 7502, 15699, 7527, 7529, 15722, 15727, 7535, 7537, 15732, 15762, 23960, 15768, 15772, 23982, 23985, 23989, 7606, 23995, 7613, 7619, 24019, 15830, 15833, 24028, 15839, 7649, 7650, 24035, 24036, 15863, 7674, 24058, 15868, 7676, 7678, 24062, 24067, 15876, 24075, 15896, 7718, 15913, 24113, 7741, 15938, 24131, 24132, 24143, 24153, 15966, 7780, 24168, 7792, 15984, 7794, 24183, 16004, 24197, 24198, 16008, 7818, 24204, 7831, 24221, 7838, 24229, 16040, 7848, 7854, 24240, 24248, 16058, 16063, 16070, 16071, 7886, 7897, 24283, 16094, 16101, 16102, 7911, 24307, 24317, 7944, 24329, 24331, 16146, 24339, 7958, 7959, 24342, 16161, 24355, 16166, 24380, 7997, 16198, 8010, 16203, 16211, 8020, 16217, 24410, 24413, 16221, 8031, 16227, 24420, 8046, 8048, 24434, 16246, 16251, 24455, 24460, 8079, 8080, 24468, 8086, 8087, 24478, 8101, 24488, 24506, 16314, 8129, 8137, 16341, 16345, 16354, 16356, 16360, 8178, 8181}, 3: {16386, 8198, 8200, 16416, 8228, 8231, 16429, 8254, 16448, 8258, 70, 71, 16460, 16467, 8276, 8304, 8318, 8321, 8327, 145, 150, 8348, 8349, 158, 8355, 173, 16559, 183, 16568, 8379, 197, 8391, 16586, 8404, 16601, 16605, 16606, 228, 8423, 232, 8430, 8433, 8440, 16636, 8451, 268, 278, 282, 8478, 16673, 16693, 313, 8507, 16702, 328, 340, 8533, 345, 357, 366, 16769, 398, 16786, 8596, 405, 8603, 8607, 418, 8613, 8624, 436, 8646, 16842, 8659, 16855, 16856, 476, 482, 8678, 16873, 8682, 491, 16891, 16894, 8702, 517, 8714, 541, 8733, 543, 544, 16929, 16931, 16934, 553, 555, 8754, 8763, 585, 16978, 597, 16981, 16982, 8795, 608, 609, 612, 8804, 16998, 614, 17000, 8807, 8810, 17002, 8809, 623, 17009, 8818, 17014, 17015, 17024, 648, 8849, 660, 17044, 8854, 8867, 17068, 17069, 8886, 8890, 8908, 8917, 729, 730, 8924, 17125, 17126, 17129, 746, 8941, 17136, 8947, 761, 764, 8958, 17157, 8973, 17169, 8990, 799, 17187, 803, 811, 813, 17205, 17212, 828, 9029, 17223, 17230, 17232, 9045, 17239, 857, 9065, 874, 9068, 882, 17267, 9076, 9086, 896, 898, 907, 9103, 17297, 9121, 943, 17334, 958, 9154, 966, 17351, 968, 978, 9178, 991, 17377, 995, 999, 1000, 1004, 1010, 1019, 1022, 17407, 1024, 17424, 9235, 9242, 17439, 1055, 1058, 9255, 17448, 17447, 9260, 1070, 17462, 1088, 17472, 9284, 9286, 17482, 9294, 1111, 1114, 1125, 17514, 1132, 17519, 1140, 9334, 1143, 1149, 17546, 1163, 9358, 1189, 9384, 9397, 17597, 17599, 17602, 1221, 9420, 1229, 9425, 9427, 17620, 17621, 9431, 9435, 9439, 9440, 17641, 9449, 17645, 17646, 9455, 17653, 9464, 9465, 17658, 17659, 9469, 17665, 1290, 1295, 9491, 9494, 9504, 9508, 17709, 9517, 9524, 17721, 17726, 1346, 1348, 1352, 17746, 17748, 1365, 1369, 17761, 9569, 17773, 1392, 1396, 9591, 1406, 17793, 17795, 1415, 9613, 17816, 1454, 9647, 1461, 9654, 1464, 1468, 17855, 1472, 9670, 1478, 17868, 17881, 17882, 9710, 17902, 9714, 1525, 9718, 17920, 9730, 9732, 9735, 9742, 1550, 17941, 17943, 17962, 1579, 1578, 9777, 9778, 1587, 17970, 1589, 1585, 9785, 1593, 1597, 1603, 18001, 1633, 1635, 9838, 9848, 1658, 9851, 9862, 1676, 9869, 18061, 1685, 18074, 18075, 9886, 1701, 18089, 1707, 1715, 18103, 9925, 18118, 9930, 1743, 9936, 18128, 1751, 18138, 1754, 9954, 1762, 18154, 9963, 9964, 18168, 18171, 18173, 9982, 1793, 9995, 18202, 10016, 18209, 10019, 10023, 1833, 18218, 18227, 10041, 10043, 10050, 1863, 18255, 1881, 18272, 18279, 1895, 18287, 10099, 1907, 18293, 18297, 10109, 1922, 18319, 10134, 18330, 10139, 1948, 18332, 1950, 18336, 18340, 18358, 1991, 1993, 18396, 18401, 18402, 18408, 2031, 10226, 18418, 10243, 10244, 2058, 10252, 18447, 10261, 2085, 10279, 10286, 2096, 2097, 18483, 10294, 2104, 18491, 18507, 2124, 2134, 18527, 18528, 18531, 2149, 10350, 2164, 2165, 2172, 10371, 10381, 18576, 10396, 10401, 2209, 2210, 10406, 18606, 10418, 10419, 2226, 2229, 2230, 2231, 10425, 18622, 18628, 18636, 2256, 18646, 10460, 10466, 18660, 10471, 10480, 2292, 10487, 10494, 2305, 18700, 2321, 18719, 18721, 18725, 2369, 18763, 18768, 18769, 2390, 2391, 18790, 18793, 18806, 2424, 18809, 18818, 10628, 2444, 18843, 2460, 18846, 18851, 10669, 2483, 2487, 2499, 10692, 10698, 18892, 18901, 2530, 2534, 18919, 10730, 18922, 2538, 10735, 2551, 10748, 10749, 2558, 10755, 2568, 2570, 10763, 2577, 18961, 18963, 2587, 2589, 18983, 10805, 2620, 19007, 2641, 2655, 19041, 2660, 19053, 2680, 19066, 2698, 2699, 19083, 2708, 19092, 10907, 2720, 10923, 2733, 2744, 19130, 10948, 19147, 10961, 10964, 19158, 2777, 2778, 19166, 2785, 10983, 2798, 11005, 11007, 2823, 2824, 2826, 19219, 2844, 11040, 11045, 19240, 19260, 19265, 2883, 19271, 2892, 2894, 11098, 19303, 19323, 11132, 2945, 19332, 19334, 2955, 11151, 2964, 2966, 19352, 2969, 19353, 19354, 19361, 11175, 2997, 11199, 3011, 11203, 19397, 19402, 3020, 3022, 3027, 11220, 3034, 3036, 11239, 11245, 19439, 3057, 3058, 19452, 3074, 19470, 3087, 19472, 11282, 11287, 19483, 3103, 19488, 19489, 11299, 11301, 3117, 11317, 3125, 3138, 3142, 3158, 19551, 11372, 19571, 11383, 11399, 11400, 3209, 3212, 19603, 19607, 3227, 3228, 11426, 19619, 11433, 3242, 11450, 11454, 3266, 19651, 11459, 19656, 19658, 19660, 3276, 19662, 3280, 19672, 19673, 11482, 11487, 11489, 3297, 11492, 19703, 3321, 19707, 3341, 19726, 11536, 11538, 19733, 19735, 19738, 19748, 3366, 11572, 11580, 11581, 11598, 19791, 19795, 3416, 11611, 3423, 11616, 3429, 11628, 19822, 19836, 3465, 3468, 11665, 19862, 19864, 19867, 11678, 11682, 11696, 11698, 11699, 11702, 19900, 11715, 19912, 11721, 19918, 3540, 19926, 19932, 11744, 3554, 19945, 19956, 19958, 19960, 11783, 19980, 19991, 11816, 11825, 3634, 11827, 11852, 11853, 11855, 3666, 20056, 20063, 3680, 11874, 3683, 20070, 3691, 3696, 20081, 3702, 20087, 20091, 11903, 3718, 3724, 20109, 11918, 3728, 11926, 11929, 11938, 3750, 11943, 11950, 11957, 3776, 11969, 3783, 11976, 3785, 11977, 20178, 11987, 11989, 20183, 20200, 12020, 12022, 20216, 3841, 20230, 12052, 20265, 12073, 12078, 3894, 3895, 12087, 20287, 20297, 20306, 12119, 20312, 3932, 20324, 12144, 12152, 20355, 12163, 3978, 20364, 3988, 20376, 12193, 4003, 4008, 20394, 12205, 4015, 12217, 12246, 12254, 20463, 4082, 12280, 4090, 4093, 20490, 4108, 20499, 4117, 4121, 20507, 4128, 12325, 4135, 20519, 20524, 12337, 20530, 20535, 20538, 20552, 20555, 4171, 4174, 12371, 4180, 4190, 12383, 12385, 20582, 20586, 20587, 20592, 12421, 20617, 4234, 20620, 12428, 4245, 20632, 20638, 12449, 4269, 4270, 20658, 20670, 20674, 12487, 20683, 4299, 20686, 20691, 20692, 20702, 4334, 12528, 12534, 20727, 12536, 4348, 20735, 4360, 4370, 4371, 4376, 20760, 20762, 12568, 12574, 12576, 20778, 12587, 4398, 20783, 4403, 4407, 12618, 12624, 4436, 20822, 12632, 12633, 4445, 4452, 12645, 12649, 4463, 4464, 12662, 4470, 20857, 12667, 4477, 20863, 20868, 20869, 12676, 20872, 4492, 4495, 20886, 12699, 12700, 12705, 4516, 20900, 20905, 4536, 12751, 20946, 12758, 20951, 12762, 4571, 4580, 20966, 20991, 20997, 12806, 4624, 21010, 4626, 12821, 4631, 12824, 21016, 21023, 21028, 21033, 4652, 21037, 21043, 21044, 21045, 21050, 21051, 12863, 4688, 21072, 21075, 12885, 12891, 12896, 21090, 4706, 12902, 21096, 12906, 12916, 12923, 21124, 12938, 4747, 21133, 4758, 4763, 12959, 12961, 12963, 4779, 12973, 21174, 12993, 13000, 13006, 4816, 13010, 4819, 21218, 21236, 4859, 13057, 21250, 21255, 13064, 4876, 4878, 21268, 13078, 4890, 4910, 4917, 4919, 4923, 4924, 4944, 13138, 13143, 4951, 21348, 4966, 13174, 4987, 21377, 4994, 13188, 21383, 5003, 13198, 5011, 5018, 13216, 21419, 5040, 21425, 5043, 13243, 13245, 5055, 21442, 5058, 21446, 5068, 13264, 13265, 5082, 13275, 13282, 13284, 13285, 21476, 5102, 5105, 21494, 21495, 13308, 5117, 13310, 21503, 21504, 21512, 21516, 5133, 5145, 21543, 5160, 13353, 5162, 13356, 5166, 13361, 21561, 5191, 21577, 13402, 13404, 21602, 5220, 21606, 13415, 21608, 21614, 21617, 13433, 5243, 5244, 13445, 5253, 21646, 13457, 5271, 5278, 13475, 5289, 5290, 13487, 21685, 13494, 21693, 5314, 13508, 13512, 21712, 5329, 5334, 5338, 5342, 5347, 21738, 13547, 5356, 5359, 5361, 13583, 5398, 21790, 21800, 21805, 5427, 5429, 21817, 13626, 5440, 5441, 5442, 21828, 5446, 21835, 5457, 21842, 13660, 21855, 13666, 21867, 13677, 5490, 21877, 5497, 21887, 13697, 13700, 21906, 21909, 5527, 21916, 21921, 5538, 5540, 13737, 21947, 21950, 21954, 21960, 5576, 21966, 5588, 13794, 21994, 13804, 5618, 13816, 13818, 5632, 13825, 22021, 5638, 22025, 13834, 13855, 13862, 5676, 13869, 5678, 22071, 13888, 5702, 22089, 5711, 13912, 22119, 22121, 5742, 22127, 13939, 22134, 5751, 22138, 13948, 13971, 22166, 13977, 5786, 22173, 13994, 22202, 22207, 22215, 14028, 22239, 22248, 22254, 22260, 22264, 22266, 14080, 14083, 22284, 14100, 22292, 22295, 22297, 22301, 5921, 14117, 5928, 5935, 22319, 14129, 22322, 5943, 14135, 22335, 22336, 5952, 5957, 14170, 14171, 14172, 22365, 14174, 5983, 14179, 5989, 22377, 14197, 14207, 22424, 22427, 14246, 14248, 6059, 14257, 14260, 6084, 6095, 14289, 22492, 14301, 22494, 22493, 22501, 6120, 22504, 6126, 14323, 6135, 6138, 6141, 14336, 14343, 14346, 6164, 22548, 14364, 14365, 6176, 6179, 6186, 14380, 22573, 14383, 22575, 22577, 22580, 14395, 14398, 22591, 22593, 14410, 14419, 14423, 6232, 6233, 22619, 6241, 6249, 14441, 14451, 14454, 6266, 22654, 6271, 22663, 22681, 14492, 22692, 14501, 14504, 14505, 22712, 6332, 22722, 14530, 22732, 6354, 14549, 14553, 14562, 22766, 14574, 14579, 14583, 22782, 22796, 14612, 14613, 6429, 22820, 14629, 22827, 6445, 6446, 14657, 14658, 14666, 22870, 6490, 22875, 22887, 22898, 14711, 22908, 14719, 6538, 14741, 22937, 14753, 6571, 22968, 22978, 6595, 22982, 14793, 22997, 14806, 14809, 23003, 14821, 6631, 6633, 6647, 14839, 23041, 14851, 14852, 14860, 6671, 23057, 23066, 14874, 6683, 23076, 23077, 23087, 23089, 23096, 6730, 23116, 14929, 14936, 6749, 6754, 14954, 6764, 6769, 23168, 6785, 6786, 23175, 23182, 6799, 14996, 15003, 23196, 23197, 6825, 15018, 15021, 6830, 15022, 6847, 15041, 23237, 15048, 23241, 23248, 6869, 23256, 23258, 6875, 6879, 23265, 23266, 23273, 6889, 23276, 6901, 23288, 15099, 6920, 6921, 6922, 15113, 23315, 6943, 15140, 6958, 23344, 23352, 15171, 15175, 15179, 15184, 6997, 15195, 15200, 7009, 23399, 15210, 7019, 15223, 15225, 15228, 23423, 15233, 15242, 7051, 7071, 7075, 23460, 7078, 23466, 15278, 23484, 23486, 7106, 15309, 7132, 15330, 23533, 15341, 23539, 7156, 15348, 23550, 7172, 7177, 23581, 23586, 23592, 15405, 15413, 23608, 7227, 7228, 23618, 15433, 7247, 23632, 7251, 23645, 7262, 23656, 15469, 15471, 15472, 15478, 7290, 23679, 23682, 15490, 7318, 7319, 15513, 15515, 23708, 7329, 15528, 15529, 7344, 7346, 15542, 15543, 7354, 15552, 23748, 15556, 15559, 23752, 15560, 7382, 23766, 23771, 15580, 15581, 7398, 7401, 7402, 7406, 15602, 23795, 7413, 7417, 15610, 7419, 23802, 23806, 7430, 7431, 7447, 15645, 7456, 15653, 15663, 23865, 23867, 23869, 23876, 7492, 15690, 15691, 23887, 15703, 15707, 7518, 23903, 23906, 7523, 15721, 23917, 15728, 15736, 7545, 7555, 23942, 7560, 7583, 23970, 7588, 23976, 15785, 23980, 23983, 23984, 15795, 15801, 15812, 15822, 7645, 15840, 7648, 15842, 15853, 24046, 15855, 7675, 7677, 7684, 7685, 15883, 15889, 24081, 7703, 15897, 15899, 15902, 7711, 7715, 24104, 7726, 24112, 24115, 24123, 24124, 24141, 15956, 24151, 7779, 15978, 7787, 7801, 15996, 24190, 24194, 7817, 16010, 16020, 24216, 16028, 24222, 7840, 7844, 16051, 7859, 7861, 7862, 16057, 24254, 24260, 16073, 7883, 16076, 24271, 24273, 16083, 16088, 16093, 7902, 24286, 7912, 7922, 24321, 7947, 7949, 16144, 16157, 7970, 16167, 24360, 24370, 16180, 7990, 24378, 7998, 24402, 16214, 24407, 24409, 8032, 16228, 24427, 16236, 16237, 16239, 16241, 8052, 8056, 8061, 8063, 24450, 24452, 8069, 8070, 8082, 8088, 24473, 8090, 16283, 24476, 24481, 8107, 16300, 16303, 24504, 8124, 8126, 16323, 16324, 16326, 8145, 8148, 16350, 8163, 16362, 16366, 8188}, 4: {16391, 16393, 8205, 16405, 8219, 8223, 37, 16421, 45, 8237, 8241, 16439, 16441, 63, 68, 77, 82, 8277, 86, 89, 16482, 98, 16483, 102, 8299, 114, 8309, 16513, 132, 8328, 137, 8332, 146, 147, 16531, 8340, 16535, 154, 8364, 187, 188, 16574, 193, 16577, 16582, 16588, 204, 206, 16595, 16599, 217, 16608, 8417, 8429, 8432, 241, 8437, 249, 8441, 16642, 16644, 16649, 265, 273, 8466, 8467, 8468, 276, 16666, 8475, 8484, 8493, 16687, 8497, 16696, 16704, 321, 320, 8521, 350, 16735, 352, 8542, 354, 16739, 8547, 8546, 351, 8562, 16754, 8573, 8574, 16774, 8588, 8590, 8593, 8600, 409, 8610, 8611, 16809, 8618, 426, 427, 8621, 16816, 439, 16825, 8642, 8643, 16838, 8664, 16862, 8674, 8683, 8685, 493, 499, 8691, 16896, 8704, 514, 8730, 16922, 545, 8738, 16940, 8750, 8751, 564, 16954, 8764, 8771, 16964, 581, 16971, 16987, 16989, 16990, 8802, 610, 8812, 17004, 628, 634, 8830, 8838, 17031, 17036, 17049, 8861, 8862, 8866, 676, 8877, 690, 695, 701, 714, 17104, 720, 724, 17111, 734, 8944, 8967, 8969, 786, 17177, 8985, 8987, 8992, 801, 8994, 804, 805, 8998, 8999, 17197, 819, 17208, 17209, 17213, 9022, 9023, 9028, 17221, 839, 9034, 9044, 17242, 861, 9059, 17266, 9091, 900, 9094, 903, 904, 9099, 908, 909, 9102, 17298, 9109, 9113, 17318, 17324, 9136, 949, 952, 953, 17345, 964, 9158, 979, 17367, 9185, 17393, 9207, 1015, 1017, 9218, 9223, 9230, 17423, 17426, 1043, 1042, 1046, 17432, 1048, 9263, 17457, 9269, 17468, 1089, 1094, 17480, 17490, 1106, 17497, 1122, 9314, 1128, 1131, 17515, 9327, 17532, 17535, 1153, 1168, 9362, 17556, 17559, 1179, 1181, 9402, 9409, 1219, 17613, 1230, 1234, 9434, 17627, 9436, 1247, 17631, 1249, 17634, 17644, 1262, 17662, 17663, 17667, 17671, 9488, 1299, 9505, 1327, 1329, 17719, 9528, 17730, 9543, 17740, 1358, 1361, 9559, 17751, 9568, 9573, 1393, 1401, 1404, 1407, 17798, 9614, 17806, 1427, 1439, 1445, 17829, 9644, 17842, 1459, 1460, 9658, 9660, 1487, 1493, 9690, 17897, 1521, 9723, 1540, 1543, 1547, 1548, 1571, 17956, 1576, 1594, 1601, 1602, 9803, 9809, 1630, 18016, 18017, 18018, 18021, 1638, 18023, 1644, 18043, 18046, 1662, 9864, 1678, 1703, 1704, 1705, 1709, 9909, 9920, 18119, 9934, 18133, 9950, 18145, 9956, 18157, 9970, 1778, 9973, 18172, 18176, 9988, 1797, 18188, 18194, 10008, 18207, 1824, 18214, 18220, 10031, 18228, 10044, 1860, 10055, 18251, 10061, 18259, 10070, 1879, 18288, 10104, 10107, 18307, 10120, 18312, 10121, 10125, 1934, 10130, 10143, 1962, 1994, 10189, 18382, 2003, 18392, 2013, 2025, 2029, 18413, 10234, 10238, 10240, 10241, 18438, 10247, 2066, 18455, 10265, 18461, 10269, 18463, 18470, 10278, 18472, 2089, 2090, 10283, 10292, 10296, 10300, 18499, 18502, 10313, 2123, 10316, 2126, 2137, 18522, 2139, 10333, 10340, 2159, 18550, 10358, 10362, 2171, 18557, 2182, 10376, 2184, 2190, 10386, 18581, 18589, 18597, 10405, 10417, 2228, 10420, 2236, 18621, 18632, 10443, 18644, 10456, 2268, 10461, 10470, 18668, 2286, 2291, 2299, 10492, 2302, 10500, 18701, 2318, 10520, 18713, 18714, 10535, 2352, 18737, 18739, 2356, 2359, 2363, 10561, 10567, 10577, 2388, 10581, 10585, 2393, 2402, 10600, 2418, 18804, 10613, 2421, 10615, 18810, 10619, 2430, 2436, 10635, 10639, 10641, 2454, 10650, 18849, 2467, 10670, 18870, 2491, 2497, 10693, 18887, 18889, 18897, 10709, 2518, 18907, 10715, 18910, 2527, 2531, 10727, 2547, 18948, 10761, 2569, 2580, 10782, 18978, 10787, 18980, 2595, 2598, 10791, 10793, 18987, 2604, 2603, 18988, 2608, 18998, 18999, 2617, 10817, 10828, 10833, 19028, 19048, 2675, 19060, 10867, 19063, 10893, 19088, 2705, 10900, 19096, 10910, 10912, 19110, 2730, 19118, 2739, 19124, 2742, 19126, 2743, 10937, 19136, 2773, 10967, 10977, 19179, 19181, 10991, 19186, 2804, 10998, 11004, 2815, 19206, 19207, 2827, 2831, 11033, 19227, 19236, 2855, 19251, 2880, 19279, 2900, 2901, 19290, 19294, 2930, 11122, 11125, 19320, 19326, 19328, 2949, 19341, 11152, 2960, 2961, 11155, 11156, 2970, 19355, 11169, 2982, 2983, 19380, 19384, 11195, 3007, 11214, 19407, 19418, 3042, 11235, 19428, 3047, 3048, 11248, 19449, 3069, 19454, 3075, 19467, 19474, 19476, 19477, 11292, 19487, 11298, 3111, 11304, 19504, 3129, 3134, 19534, 3150, 11346, 3154, 11351, 3162, 19554, 3179, 11377, 19579, 11403, 19596, 19609, 19613, 11422, 19615, 19616, 3233, 3235, 11432, 11437, 3247, 19633, 11446, 3255, 19650, 19652, 19664, 19666, 3312, 11505, 19698, 11508, 11511, 3326, 19710, 11524, 19717, 11530, 11531, 3348, 11550, 3374, 3380, 11574, 19771, 19773, 11587, 3400, 11597, 3407, 3411, 3414, 3428, 19817, 3436, 11637, 19840, 3458, 19849, 3471, 19856, 19863, 3482, 11674, 3485, 19872, 11691, 3506, 3507, 3516, 19902, 11713, 11726, 19920, 19939, 3555, 11752, 11770, 3578, 19966, 19972, 19975, 11790, 19985, 3603, 19992, 3623, 20009, 3625, 11818, 20014, 20016, 20017, 20018, 3639, 11835, 3648, 3649, 11848, 11849, 11854, 11869, 20064, 11881, 3690, 3693, 20080, 3699, 3701, 20088, 11898, 20092, 20094, 20097, 20100, 20104, 11916, 20113, 20114, 3731, 20126, 3746, 20151, 11974, 20173, 3791, 3792, 3794, 20181, 3799, 20187, 11996, 12004, 20208, 12016, 3828, 3836, 3840, 12036, 3845, 20232, 20233, 12051, 20245, 12060, 12067, 3876, 20266, 20271, 20278, 3899, 20291, 12101, 3912, 12108, 3921, 3925, 3929, 12124, 20317, 20321, 3940, 20329, 20332, 3954, 3975, 12174, 20375, 20377, 20388, 4004, 12199, 20397, 4020, 12230, 20422, 20431, 4054, 4057, 20444, 20449, 12262, 4080, 12285, 12291, 20486, 4103, 12297, 12298, 12303, 20496, 4116, 20501, 20504, 20509, 12321, 4141, 20539, 12354, 12357, 4166, 12365, 4177, 12370, 20567, 12380, 4191, 20577, 20581, 4198, 4203, 12397, 12401, 12406, 4217, 12420, 12422, 12424, 4238, 12435, 20627, 20629, 20634, 4252, 12450, 12459, 12467, 12473, 4283, 4285, 20669, 4289, 4291, 20677, 4309, 20701, 4320, 12513, 12514, 12516, 12517, 12524, 12527, 4345, 4355, 12562, 12569, 20761, 4382, 4389, 4390, 20775, 20782, 20790, 4411, 12606, 12609, 20801, 4420, 12622, 4431, 4434, 12626, 12630, 4439, 12641, 4450, 20849, 4480, 4485, 4489, 4493, 20878, 12686, 12692, 12693, 4506, 4510, 20904, 4521, 20922, 4543, 20932, 4548, 4551, 4562, 12759, 4572, 4574, 20960, 20963, 12772, 12780, 12782, 4594, 20982, 12794, 12801, 20996, 4617, 4627, 21011, 12827, 4636, 21032, 21040, 12848, 21046, 21055, 21058, 4674, 12870, 12878, 12881, 21083, 12895, 21091, 4712, 12909, 4724, 4727, 4731, 21116, 21115, 12926, 21119, 21118, 12932, 21127, 4761, 4772, 4782, 21172, 12981, 21184, 4801, 21190, 21204, 4820, 21215, 13025, 4836, 21223, 4846, 13042, 13043, 21243, 4864, 21249, 21256, 4884, 21269, 21279, 21285, 13094, 13100, 13107, 13115, 21315, 13128, 4949, 21336, 13147, 21365, 13179, 4990, 13192, 5009, 13203, 13209, 5021, 21411, 21413, 21422, 21423, 5053, 13247, 5062, 13258, 5079, 5080, 13274, 13278, 21480, 21481, 21498, 21517, 5138, 5140, 13340, 21535, 13359, 5167, 5171, 21558, 21568, 21569, 21581, 5204, 13408, 21601, 13422, 21623, 5242, 13441, 5254, 13455, 21668, 5287, 21671, 5292, 21680, 13491, 13504, 5318, 5323, 5324, 13518, 5326, 13520, 13525, 13526, 5336, 13537, 5349, 5351, 13548, 21741, 13555, 13561, 13562, 21761, 5378, 21768, 13586, 21778, 13588, 13595, 13596, 5406, 21797, 13607, 5415, 13608, 21804, 5421, 13636, 5448, 13649, 13655, 13658, 13665, 21865, 13674, 21868, 5484, 21874, 13683, 5491, 21879, 13692, 21894, 13710, 13714, 5525, 21910, 13732, 5542, 5552, 5555, 5558, 5564, 13760, 21953, 21957, 5573, 5575, 5597, 13791, 5602, 21988, 13802, 5613, 5622, 5626, 5627, 5631, 22018, 5634, 13829, 13840, 5649, 5652, 13847, 5656, 22043, 5667, 13866, 22059, 13868, 22062, 5684, 5687, 13879, 13880, 13881, 22079, 22082, 22084, 13893, 13899, 5709, 13906, 13909, 22104, 13919, 5732, 5736, 22120, 5739, 13933, 22144, 13953, 5760, 13957, 5771, 5790, 13984, 22179, 13996, 14001, 14002, 22198, 14007, 22216, 14024, 14029, 22223, 14035, 22236, 5856, 22240, 22245, 22255, 5875, 14069, 22262, 14073, 5881, 5884, 22270, 22276, 22285, 5911, 22296, 14106, 22300, 22306, 14115, 22312, 22320, 14131, 22325, 22326, 5946, 5949, 22340, 14155, 5963, 22349, 14157, 22356, 5976, 5990, 5993, 22381, 22382, 22394, 22395, 22398, 14208, 6018, 22408, 22412, 14244, 14250, 22442, 6079, 6085, 14277, 14279, 14287, 6097, 14292, 6104, 6113, 14306, 6116, 6123, 14322, 6131, 14326, 22521, 14335, 22533, 22535, 14347, 14356, 22551, 6168, 6180, 6193, 6196, 22582, 14401, 22595, 22603, 14418, 14424, 22621, 6243, 6246, 6250, 6256, 6261, 6262, 22645, 22650, 14481, 14493, 14495, 22691, 6313, 14511, 22704, 6319, 6322, 14519, 14523, 22720, 6341, 6342, 14534, 14537, 22735, 14544, 6356, 6362, 22749, 14558, 14559, 22758, 6382, 14577, 14581, 6390, 14588, 22783, 6400, 6416, 6421, 14617, 14619, 14647, 14659, 22858, 6474, 14671, 6480, 6481, 22869, 6486, 6488, 22873, 6489, 14684, 22886, 14705, 14716, 14717, 6546, 22932, 6558, 6564, 6566, 22983, 14794, 14801, 22994, 6610, 6612, 6614, 6615, 14807, 14818, 6638, 23030, 6648, 14842, 23036, 23038, 6658, 14857, 6665, 6667, 14866, 6684, 14879, 14881, 6690, 23084, 23105, 14915, 6727, 6733, 14926, 6736, 14930, 23123, 23128, 14943, 6755, 23141, 6766, 6773, 14967, 6787, 6792, 14985, 23183, 15008, 15025, 6836, 23230, 23233, 6853, 6856, 15050, 6858, 23243, 15057, 15059, 15061, 15063, 15066, 6874, 23269, 6891, 23282, 23301, 15112, 23306, 6924, 6927, 6935, 15134, 23341, 15157, 15165, 15169, 23374, 23375, 23382, 7003, 15198, 23397, 7021, 23406, 23408, 7025, 7034, 15229, 23421, 7042, 15241, 7056, 7059, 15252, 15253, 15254, 7065, 23450, 23454, 23459, 7079, 23467, 23469, 15286, 23480, 15289, 23482, 7100, 15294, 15316, 15319, 15323, 7131, 15332, 7146, 15346, 23540, 15352, 7174, 15369, 23562, 7191, 7197, 23583, 7200, 23588, 23598, 15408, 15409, 23614, 7240, 23630, 23631, 23640, 7265, 23657, 15470, 23680, 7301, 15494, 23686, 23688, 7306, 7310, 15509, 7322, 23707, 15514, 7332, 15530, 23722, 23735, 7355, 7373, 7376, 7377, 15576, 23774, 7394, 23783, 15598, 23796, 15606, 7415, 23800, 15612, 23804, 23811, 15619, 15621, 23812, 15625, 23821, 23830, 15640, 23832, 23835, 15655, 23851, 15661, 23853, 15664, 23860, 15670, 7481, 7488, 15684, 23879, 15689, 23882, 15693, 15695, 23889, 23897, 7521, 23915, 23916, 23932, 15754, 7573, 23965, 15780, 15782, 23974, 15788, 7597, 23996, 7614, 23998, 7632, 7636, 7637, 15829, 15843, 24039, 7657, 24044, 7661, 24047, 7670, 24057, 15869, 15871, 24078, 15892, 15900, 24096, 24097, 24099, 7728, 7736, 24121, 7737, 7739, 24125, 15946, 7757, 24144, 24145, 7762, 24154, 7776, 15973, 24169, 7788, 7789, 7790, 15985, 24178, 15995, 24203, 7822, 24217, 7839, 24227, 16036, 7856, 24242, 24246, 24247, 7865, 24252, 16068, 16069, 16096, 24290, 16120, 24323, 16131, 16133, 16134, 7952, 24340, 7957, 24341, 16153, 7961, 7964, 24351, 24353, 16163, 7972, 24365, 24368, 16179, 16183, 24385, 16225, 8047, 16240, 8050, 24443, 8060, 8064, 24451, 8074, 24458, 24462, 24471, 24472, 8102, 16311, 16312, 16316, 16321, 8136, 16331, 16334, 8167, 16375, 16379, 16381}, 5: {16385, 5, 16395, 8209, 16406, 8216, 29, 8230, 39, 16424, 41, 16431, 16436, 8245, 54, 8262, 8264, 76, 8280, 94, 8294, 103, 8295, 8305, 16498, 16499, 118, 16518, 136, 8331, 16528, 8338, 16551, 176, 8372, 16569, 16573, 196, 16580, 199, 16590, 8399, 8410, 16614, 16615, 238, 16629, 245, 16641, 8459, 271, 8469, 16661, 280, 16667, 16668, 8487, 304, 307, 16694, 315, 316, 16715, 332, 334, 8528, 8537, 8548, 8549, 8558, 16750, 16755, 396, 8608, 422, 438, 8631, 16827, 444, 16831, 8645, 461, 16846, 16848, 8657, 8658, 467, 468, 473, 475, 483, 8681, 16874, 16876, 502, 8699, 8700, 16901, 8711, 8712, 524, 8717, 8720, 540, 16935, 8746, 16939, 554, 567, 570, 571, 579, 8777, 8800, 613, 8815, 8816, 631, 17016, 8823, 642, 8837, 8841, 17037, 17039, 17040, 17047, 17060, 8880, 693, 17085, 709, 8914, 17119, 17127, 748, 753, 17138, 755, 17139, 8957, 17159, 8972, 787, 17175, 793, 17180, 8989, 17189, 806, 17191, 821, 17206, 822, 9015, 827, 9020, 833, 838, 9032, 17237, 9049, 17244, 9058, 872, 9066, 877, 17262, 9071, 9087, 17280, 17293, 9112, 924, 9120, 931, 933, 936, 9129, 941, 17325, 17330, 9145, 17338, 9147, 9159, 9161, 9162, 9166, 17359, 992, 17383, 17396, 17401, 9210, 17404, 9215, 17409, 17425, 1057, 9250, 9251, 9258, 9259, 9264, 9265, 17463, 9275, 9279, 17481, 1097, 17483, 9292, 17485, 1116, 9313, 9321, 9331, 9337, 17536, 9345, 17537, 17547, 17549, 1166, 17552, 17563, 1183, 17568, 9377, 9379, 17576, 9386, 17592, 17596, 17600, 17604, 1222, 1225, 17609, 17612, 1238, 17633, 9441, 1268, 17657, 1288, 17672, 17676, 9485, 17682, 1304, 17691, 17693, 9502, 1333, 17723, 17724, 17739, 1355, 9552, 1362, 1367, 17754, 17757, 17763, 17764, 1387, 1398, 17787, 17788, 1409, 1416, 1419, 17813, 17826, 9634, 17830, 17831, 17839, 9650, 17850, 17852, 9662, 9669, 1481, 9676, 9684, 17877, 9689, 17888, 9699, 9700, 1510, 1513, 1523, 9719, 17914, 17917, 1539, 17938, 1554, 17940, 1559, 9752, 9757, 17961, 9774, 17974, 1592, 1596, 17989, 9799, 17992, 9802, 9804, 17997, 17996, 1615, 1616, 18002, 18009, 18010, 9831, 9841, 18045, 1665, 1671, 9868, 1683, 1684, 18069, 9879, 9887, 18098, 18100, 1718, 9913, 9918, 9923, 9927, 18130, 9955, 1775, 9967, 1777, 18163, 1786, 9985, 18183, 9996, 1810, 10002, 18216, 18222, 1843, 1844, 10037, 1845, 18238, 10053, 18252, 18257, 10069, 18265, 18267, 10078, 18271, 18278, 10102, 1911, 10108, 10110, 18320, 1939, 1945, 18342, 10150, 10153, 18347, 10159, 18355, 10170, 18363, 18364, 10173, 18369, 18371, 1996, 2001, 18386, 2004, 18403, 18406, 18409, 18419, 18420, 10227, 10229, 2039, 10232, 18435, 2052, 18436, 10248, 10250, 10257, 2068, 2074, 2078, 10272, 2083, 10280, 10281, 2100, 18490, 2109, 18494, 2114, 2117, 10318, 18519, 18520, 2141, 2146, 10339, 10338, 18554, 18556, 2176, 2181, 2183, 2186, 18575, 10391, 18586, 2208, 10409, 2223, 2224, 10426, 10430, 18629, 10442, 2252, 2253, 2258, 10454, 2267, 18665, 10485, 18679, 18682, 2301, 10496, 18692, 10501, 2311, 10505, 10510, 18704, 2326, 18710, 18717, 2342, 18727, 10542, 18738, 10549, 18742, 2362, 18750, 18753, 18755, 2371, 2373, 10563, 2384, 18771, 18773, 18795, 10610, 2420, 18807, 18811, 18824, 10637, 18854, 18856, 10674, 2495, 2500, 10700, 18902, 2520, 2528, 18920, 2537, 10732, 2541, 10733, 18930, 10743, 10744, 18936, 10746, 2556, 18950, 18959, 2585, 2605, 10800, 18994, 10807, 10813, 10835, 10837, 2647, 2652, 10848, 10849, 10853, 10854, 2665, 2667, 19052, 2671, 19058, 10883, 2700, 2703, 19093, 2716, 2724, 19111, 2727, 10927, 19121, 2740, 19125, 2747, 10941, 19135, 19142, 2758, 10970, 10974, 19167, 2787, 10981, 19177, 2800, 19189, 19190, 19194, 2821, 11014, 11019, 11024, 19220, 19223, 2852, 19237, 19238, 11047, 11049, 11065, 19261, 19273, 19276, 11087, 2899, 2907, 19295, 2912, 2913, 11105, 11111, 2927, 11120, 2929, 2931, 11131, 2951, 11148, 2967, 11162, 2971, 19360, 19362, 2980, 2989, 11184, 2995, 11188, 19387, 3008, 19396, 3017, 11217, 11219, 11224, 11228, 11232, 11236, 11243, 19435, 19442, 11252, 11253, 11254, 3066, 11262, 3086, 19471, 11284, 11291, 3104, 19498, 3118, 11311, 3124, 19509, 3128, 11325, 19520, 11330, 11332, 11334, 11340, 11345, 3156, 3159, 11365, 3177, 11373, 3184, 19570, 3189, 3192, 19578, 3195, 19582, 19585, 3202, 19587, 11398, 11402, 3217, 3219, 19604, 11413, 19605, 3224, 19612, 19621, 19647, 11461, 3277, 11484, 19682, 19693, 19700, 3317, 19706, 11516, 19711, 3333, 11537, 11542, 11553, 19749, 3373, 19766, 3383, 19768, 11576, 3398, 3399, 19786, 11606, 3415, 3417, 11610, 3432, 3442, 19826, 3443, 3448, 19843, 3460, 11667, 19860, 19874, 11686, 3498, 19894, 11709, 3519, 3520, 19908, 3528, 19914, 19917, 19922, 19923, 11733, 19928, 11737, 19930, 11742, 3552, 11747, 3557, 19946, 3568, 11764, 11767, 11774, 19969, 3589, 11785, 3595, 19982, 11795, 19996, 11807, 11809, 20001, 3620, 3622, 11820, 20012, 20027, 11844, 20043, 3664, 20062, 11877, 11884, 11887, 3700, 11892, 3703, 20102, 20105, 20107, 11917, 11922, 20118, 11933, 20129, 11940, 20142, 3760, 20145, 11958, 20150, 11960, 11970, 3787, 11981, 20175, 3793, 11986, 20180, 11998, 20193, 12002, 20195, 20196, 20201, 12013, 12019, 3831, 3835, 20220, 12034, 3851, 3859, 20247, 3865, 12059, 3872, 12068, 20270, 20277, 12088, 20284, 12094, 20296, 12117, 20323, 20326, 12141, 12147, 20351, 3971, 3987, 3991, 3994, 20381, 20393, 12206, 20405, 20407, 12219, 4033, 4042, 20436, 12244, 12253, 20447, 20451, 20455, 20456, 20459, 12268, 12272, 20466, 12283, 12289, 12290, 20489, 4112, 4115, 12309, 12316, 4139, 12333, 4142, 20527, 20529, 4150, 12343, 4153, 4154, 4158, 4160, 12360, 12363, 20564, 20572, 4193, 12387, 12391, 4212, 12411, 4225, 20614, 20618, 12434, 20626, 4243, 12447, 12455, 20647, 4265, 12472, 20676, 4304, 4305, 12504, 20709, 12518, 4328, 20714, 20722, 20724, 12535, 4350, 4357, 20742, 12551, 4361, 20746, 12564, 12567, 4383, 20769, 4386, 20776, 4401, 4402, 12600, 20793, 12603, 4424, 4425, 12625, 20824, 12634, 20829, 4449, 12644, 20837, 4459, 4467, 12664, 20858, 20865, 20867, 4491, 4499, 20888, 20889, 20895, 4512, 20902, 12711, 4526, 20913, 4530, 20919, 20942, 12753, 4561, 4568, 12767, 12769, 4584, 4588, 4590, 12783, 20976, 20978, 12786, 20985, 4611, 21005, 21012, 21025, 4641, 12840, 12846, 21039, 12856, 12858, 12859, 4672, 21060, 4680, 12879, 21079, 12888, 4703, 12897, 4705, 4707, 4711, 21100, 21110, 12924, 4734, 21120, 4741, 12942, 21137, 21138, 12949, 4760, 12965, 4774, 21162, 21163, 12975, 4791, 21183, 12994, 21200, 4824, 13023, 13033, 13056, 13065, 21258, 13072, 4881, 13074, 13077, 21270, 13080, 13081, 4896, 4901, 13103, 21297, 13105, 13112, 4932, 4936, 21322, 21329, 21332, 21337, 21344, 13165, 4975, 4976, 13170, 13175, 4986, 13182, 21380, 13189, 21390, 5010, 21398, 21402, 21429, 21444, 5063, 21451, 13276, 21471, 13280, 13294, 5107, 13301, 5111, 21515, 13329, 13334, 5143, 5144, 21533, 13343, 21541, 5177, 5183, 5187, 13379, 13383, 5193, 21578, 13388, 21589, 21590, 21595, 5231, 13424, 5233, 5234, 13427, 5247, 21635, 21637, 5256, 21642, 5259, 5258, 13458, 21661, 5282, 21670, 5291, 21677, 21678, 21681, 13489, 21686, 5312, 21704, 5331, 13530, 13532, 13533, 21727, 21736, 5357, 21745, 5364, 13564, 5384, 13580, 21779, 13593, 21786, 13610, 21803, 5419, 13612, 5428, 21813, 5433, 5439, 21829, 13640, 13644, 5456, 21847, 21848, 13659, 5467, 21858, 13667, 21870, 21873, 13688, 5502, 5511, 13703, 5516, 13711, 5521, 21907, 21911, 21912, 13719, 13722, 5530, 13720, 13725, 21918, 5536, 5537, 13738, 13740, 13744, 21943, 21945, 5562, 13757, 21952, 5579, 5581, 13774, 13781, 21975, 5595, 5604, 5611, 13807, 5625, 22012, 22017, 13831, 13832, 22026, 22031, 22041, 22046, 5663, 22050, 22051, 22065, 13874, 22077, 13892, 13894, 13897, 22092, 13913, 22107, 13928, 13931, 5750, 13942, 13950, 5768, 5774, 22159, 5780, 13974, 5785, 22169, 13978, 22183, 5802, 5819, 22206, 22210, 5828, 5835, 22225, 22226, 22228, 5853, 5862, 14057, 14059, 5874, 14072, 22282, 14091, 22288, 22303, 22304, 14121, 22314, 14130, 22323, 14137, 22329, 5958, 14162, 14166, 5975, 5981, 22369, 22378, 5995, 22379, 6007, 6008, 22393, 22400, 6028, 14222, 14224, 14233, 6042, 22428, 14239, 22439, 14258, 22450, 14259, 6074, 22458, 14269, 22464, 6098, 14291, 22482, 22487, 14299, 14315, 14325, 6133, 14332, 6149, 22534, 14344, 22536, 6155, 22544, 22552, 14361, 14366, 14377, 22569, 22576, 14385, 6195, 6206, 6212, 14404, 14406, 22602, 22605, 6223, 22617, 14430, 22624, 22625, 22635, 14445, 14450, 6259, 22653, 6273, 22668, 6298, 22695, 22698, 14507, 22703, 6327, 6333, 14532, 6349, 6351, 14547, 22741, 22742, 22754, 14564, 22756, 14566, 22772, 22776, 6392, 22778, 14587, 6395, 22788, 14600, 14602, 6413, 22799, 22806, 14615, 6425, 14622, 6433, 22818, 6439, 6443, 14638, 22833, 6449, 6453, 22844, 22852, 14661, 6471, 22862, 14675, 6503, 6505, 14699, 14706, 6516, 6533, 14725, 6536, 14729, 14734, 6553, 22944, 22958, 6578, 22962, 22972, 14797, 14805, 23002, 6632, 6641, 14849, 14850, 6660, 14858, 23050, 23059, 14870, 6681, 14880, 6689, 23074, 6694, 23080, 14889, 6699, 6703, 6717, 6719, 14922, 23119, 6739, 6762, 14965, 6789, 14981, 14983, 23186, 14995, 23188, 23192, 15004, 15005, 15012, 15015, 6839, 6852, 15053, 23249, 15071, 6894, 15089, 6900, 6905, 6906, 6907, 6911, 6912, 23296, 15107, 23308, 6925, 6933, 6934, 23319, 15128, 6937, 6938, 15132, 23324, 6942, 6948, 23336, 6957, 6961, 23348, 6977, 6979, 6985, 15177, 15191, 15196, 7008, 15204, 23400, 15214, 15215, 23411, 15227, 15230, 15232, 7046, 23441, 15261, 15267, 15274, 15280, 23474, 7090, 15283, 23485, 15293, 23489, 7109, 7115, 7134, 15334, 15338, 23531, 23532, 15347, 23542, 7161, 23546, 15358, 23552, 23559, 15368, 15373, 23591, 15401, 23594, 23597, 23601, 15412, 15414, 15420, 15421, 7230, 7232, 15426, 15431, 15434, 23629, 7250, 23638, 7258, 15452, 15453, 15459, 15465, 23658, 7279, 23666, 15475, 15482, 7292, 7312, 7313, 23701, 15510, 7325, 23710, 15525, 23720, 23734, 23742, 23753, 15566, 15582, 15585, 15589, 23782, 15594, 7405, 7410, 23797, 15620, 15622, 7434, 7441, 23833, 15642, 7452, 7464, 7467, 15662, 7477, 23864, 15673, 7484, 23872, 7503, 7511, 7512, 15706, 23899, 7516, 23910, 23918, 7538, 15731, 15733, 23926, 7541, 7548, 23937, 23946, 7576, 15769, 15786, 23991, 15800, 23993, 7610, 7611, 15802, 23994, 24000, 15811, 15821, 24024, 7652, 15854, 15858, 7667, 7668, 24060, 24061, 15879, 24072, 7690, 7706, 7710, 15903, 24094, 24101, 15910, 24103, 24102, 7723, 24108, 15919, 24117, 15928, 7738, 7743, 15942, 24138, 15949, 15954, 24155, 7772, 24159, 7775, 15969, 16001, 16016, 24212, 7833, 7836, 7853, 16048, 24245, 24253, 7876, 24268, 16077, 16081, 24275, 24297, 24305, 24308, 24318, 16126, 24322, 7941, 7942, 7943, 16139, 16155, 7963, 24349, 24352, 7971, 16164, 7975, 7980, 16177, 16178, 24372, 24375, 24376, 8002, 24391, 24399, 24400, 16213, 8024, 16218, 16219, 16220, 24411, 24417, 24419, 24421, 24431, 16263, 24479, 8099, 16292, 24490, 16298, 24498, 16307, 8123, 8125, 16319, 16325, 16328, 8161, 8169, 8179, 16371, 16376}, 6: {8195, 8199, 20, 50, 16437, 8251, 8260, 16455, 8263, 74, 8268, 85, 16485, 16489, 105, 16493, 16501, 16504, 8314, 124, 16509, 16511, 8320, 135, 8337, 8343, 155, 8347, 8350, 8353, 8356, 16549, 166, 16552, 171, 16557, 175, 16561, 8373, 182, 190, 16583, 8401, 221, 8415, 224, 16616, 8425, 16618, 237, 239, 8439, 8442, 16646, 8464, 275, 8473, 16670, 322, 327, 16720, 336, 342, 8538, 8566, 16764, 382, 8577, 16772, 8582, 392, 8585, 16778, 16783, 400, 8595, 16792, 416, 8609, 421, 423, 424, 16807, 16812, 16817, 16818, 16829, 8637, 449, 450, 16836, 8649, 8653, 8656, 8666, 477, 16866, 16870, 16872, 16875, 495, 500, 16890, 516, 534, 16921, 8732, 546, 8743, 552, 16946, 8761, 8767, 578, 589, 592, 16977, 596, 8801, 16999, 17001, 618, 8821, 8825, 8828, 8832, 644, 8842, 17038, 656, 8855, 671, 677, 680, 8883, 17076, 8885, 17083, 8893, 8897, 17089, 17097, 8907, 17099, 719, 17103, 8923, 17131, 757, 8955, 763, 17147, 17153, 17154, 8970, 8977, 17173, 17186, 17193, 812, 17198, 815, 17203, 9016, 826, 830, 9039, 9041, 17235, 878, 879, 17264, 9083, 17284, 17287, 906, 9108, 918, 9114, 929, 934, 17320, 938, 9137, 17331, 9148, 957, 9155, 17348, 17352, 969, 974, 976, 988, 1001, 1006, 17395, 1012, 17410, 1029, 1032, 9232, 1044, 9237, 9262, 1084, 1085, 17469, 17471, 9283, 1100, 17488, 17499, 17507, 1127, 17511, 17527, 9335, 1147, 17533, 9346, 1159, 9352, 17544, 1162, 9363, 17558, 1187, 9380, 17577, 1197, 9395, 1206, 9398, 1208, 1209, 1211, 9411, 1224, 1235, 1237, 17624, 9437, 9443, 9454, 9459, 17660, 1283, 17669, 9482, 17677, 9493, 17696, 17697, 1315, 1318, 9523, 1332, 1338, 9531, 17729, 9546, 9548, 17749, 9563, 1377, 17767, 1389, 9582, 17775, 9581, 9583, 17790, 9599, 9615, 1431, 17819, 9630, 9636, 9638, 17835, 1453, 17837, 17840, 9649, 17844, 9656, 1466, 9663, 17869, 1491, 1502, 17890, 1511, 1512, 1517, 1524, 17910, 9726, 17918, 9737, 1546, 1569, 17954, 9771, 9780, 9784, 9788, 17982, 17990, 17994, 17999, 18000, 1617, 9811, 18005, 1623, 18008, 1639, 18024, 1645, 1646, 1648, 18036, 1654, 18039, 18040, 1655, 18048, 1666, 18051, 9861, 18054, 9863, 18059, 1679, 9874, 1694, 9892, 18095, 18096, 9915, 1729, 18115, 18140, 1772, 9974, 9977, 9981, 18175, 18177, 1811, 18198, 1821, 10025, 18219, 10029, 10030, 18231, 1849, 1851, 10047, 18240, 1870, 10063, 1871, 1873, 10072, 1885, 18269, 1891, 10090, 10092, 18295, 1915, 1927, 1929, 18315, 10127, 18321, 10133, 18337, 10148, 18341, 10151, 18343, 10174, 10177, 10183, 18376, 10188, 1997, 1998, 18383, 2002, 10202, 10212, 2020, 10235, 18440, 10254, 10258, 18451, 10275, 2087, 18473, 18476, 18478, 18482, 10291, 18495, 2122, 18509, 18510, 2127, 2125, 2131, 10329, 2147, 2148, 10342, 18542, 10351, 2166, 2167, 10361, 10365, 18559, 18565, 10373, 18580, 2200, 10416, 18612, 18623, 2245, 10446, 10465, 18657, 10473, 10479, 18680, 18689, 18691, 2309, 10509, 2324, 2331, 10531, 2345, 18729, 18732, 10545, 2361, 2367, 10570, 2383, 10584, 10587, 18779, 18781, 18787, 10596, 10597, 10598, 18791, 2414, 18799, 2426, 18816, 10626, 10629, 2439, 2447, 18832, 18837, 2455, 2458, 10651, 18847, 10660, 18857, 2474, 18868, 18869, 2496, 18881, 18882, 18883, 18894, 18895, 18908, 2544, 18933, 18934, 2555, 2562, 2563, 2574, 18960, 2579, 10772, 10774, 18967, 10776, 18968, 2599, 18989, 18995, 18997, 19001, 2621, 19014, 2631, 19023, 19027, 10836, 2653, 10846, 2662, 10855, 2664, 19049, 10860, 10863, 10864, 19057, 2672, 2673, 2678, 19069, 10880, 2690, 2696, 19084, 10904, 2713, 10916, 2731, 19117, 2735, 19123, 19127, 10954, 10955, 2765, 2766, 2768, 19156, 19157, 19161, 19172, 2790, 19178, 10992, 2801, 10997, 2809, 2810, 11008, 11013, 11015, 19209, 11020, 11023, 2832, 19224, 19226, 11036, 2866, 2869, 19256, 2884, 19270, 19272, 19277, 11097, 11106, 11107, 2915, 19302, 2928, 2934, 2942, 2947, 11145, 2954, 2956, 2959, 11161, 11163, 19356, 11168, 2978, 11170, 2985, 19375, 11183, 19378, 3015, 19404, 11215, 3029, 11226, 11231, 3039, 19426, 19430, 19434, 3060, 19444, 11255, 3064, 11268, 11269, 19479, 11289, 3112, 11306, 3119, 3123, 11316, 3126, 11324, 11348, 3163, 3164, 19549, 3166, 3172, 3174, 3182, 3185, 3196, 11393, 11404, 19606, 11414, 19610, 3229, 3241, 3244, 3256, 3258, 3262, 19657, 3274, 11469, 3278, 3284, 11483, 3292, 3291, 19695, 19702, 19708, 3334, 19727, 3345, 3346, 11539, 3349, 3363, 19754, 11565, 19760, 3386, 11582, 19774, 19798, 19799, 19800, 11619, 11623, 11630, 11631, 3450, 19837, 11645, 11647, 11650, 11651, 11655, 19848, 3472, 3477, 3480, 19885, 3504, 19889, 11704, 19903, 11719, 3530, 3533, 11731, 19924, 19929, 3547, 11740, 11749, 11757, 3571, 19957, 11766, 19959, 3587, 11796, 11800, 20000, 20002, 20011, 11821, 11826, 3637, 11834, 3658, 11850, 20052, 3669, 20057, 3674, 11873, 11875, 20068, 20077, 11889, 20082, 20101, 3721, 3723, 20124, 20127, 20132, 3751, 11944, 3754, 20148, 3771, 20157, 11965, 3773, 11973, 20167, 11980, 11983, 11988, 20186, 3812, 20211, 12030, 12031, 12038, 12039, 12042, 20238, 12055, 20255, 12065, 3874, 20259, 3877, 20262, 12070, 3878, 3881, 3890, 12097, 3907, 20292, 12116, 12122, 12125, 12126, 20320, 20327, 3946, 20334, 20335, 20336, 20343, 3959, 12160, 3974, 3985, 12178, 20372, 12181, 12190, 4017, 12212, 20409, 20410, 20417, 20418, 4036, 20420, 4039, 12232, 20425, 4046, 20433, 4050, 12241, 20441, 12255, 4065, 12257, 4067, 20458, 12267, 20461, 20469, 4086, 20472, 12282, 20483, 4110, 20495, 12307, 4125, 20516, 4134, 20534, 4156, 20541, 20544, 4165, 20550, 20551, 4167, 4169, 12367, 4183, 4188, 4195, 4196, 12404, 4218, 4222, 12416, 12417, 20609, 20619, 20621, 20624, 20625, 20637, 4253, 20639, 4259, 4260, 12463, 4272, 12468, 20663, 12478, 4295, 20682, 12492, 4302, 20688, 4307, 20694, 4316, 20706, 20721, 12531, 20729, 20748, 12571, 20771, 20773, 20785, 4406, 20795, 20796, 4413, 20811, 4443, 20828, 12637, 12639, 12643, 12654, 12655, 20855, 20856, 4473, 4472, 4486, 4487, 4502, 12695, 12697, 4519, 20906, 4535, 12728, 12730, 12733, 20943, 20944, 12757, 4575, 12770, 4579, 20964, 12781, 4589, 20975, 20998, 21001, 12809, 4628, 4633, 4635, 4640, 21027, 12836, 12835, 21031, 12853, 4666, 21052, 12864, 21061, 21063, 12876, 12883, 12887, 12890, 4701, 12894, 4708, 12901, 4709, 21101, 21114, 12929, 21132, 4753, 4754, 12945, 12953, 12956, 21157, 12968, 12969, 12980, 12989, 4799, 21188, 21192, 21194, 21195, 13005, 4815, 21206, 21212, 4835, 13030, 4848, 4850, 13045, 4857, 21251, 4868, 21253, 21257, 4874, 4875, 21260, 4879, 4887, 4891, 13087, 21287, 4903, 13104, 21306, 21311, 21320, 13131, 21328, 21330, 4950, 13144, 4957, 13162, 13177, 13178, 4991, 21382, 5005, 13199, 21391, 21393, 5015, 21405, 21407, 5028, 13220, 5031, 13229, 21435, 21437, 13256, 21456, 21463, 5085, 21469, 5096, 5098, 21484, 13297, 13298, 21497, 13305, 5121, 5125, 13332, 21540, 13355, 13358, 21551, 5168, 21553, 5170, 21562, 5180, 13373, 5182, 5186, 13382, 5206, 13405, 5213, 21600, 5216, 13413, 13414, 5228, 5235, 21622, 5246, 13438, 5262, 5264, 13456, 5266, 5265, 13461, 13462, 13477, 21673, 13483, 21682, 13501, 21695, 21700, 21701, 13510, 21703, 21709, 21714, 13524, 5340, 5348, 5353, 5354, 13550, 13553, 5362, 21747, 21750, 13559, 5370, 13563, 5371, 21757, 13567, 5377, 21767, 13578, 21773, 5392, 5393, 13587, 13597, 21798, 21799, 21818, 13635, 13639, 13642, 5458, 13672, 5480, 13675, 21872, 13686, 21884, 5506, 21901, 21902, 21905, 5531, 5543, 21927, 21930, 13739, 21933, 13741, 21935, 13746, 21940, 21941, 21946, 13759, 5567, 13761, 21955, 13771, 21963, 13773, 21969, 21976, 21977, 13786, 21981, 5600, 5608, 21996, 5624, 13824, 13826, 5643, 22029, 5654, 22044, 13857, 22049, 22052, 5672, 22056, 22066, 5695, 5712, 13915, 22112, 5731, 13924, 5738, 5740, 22126, 13940, 5749, 22145, 22150, 5772, 22163, 5791, 5792, 5793, 5795, 5797, 22189, 22199, 5815, 22201, 14027, 14031, 5840, 14033, 5842, 5843, 14037, 22234, 22237, 22242, 22243, 14052, 14053, 22246, 22247, 14056, 5868, 14064, 22258, 5880, 5882, 14078, 14093, 14099, 5910, 5912, 22299, 14110, 5919, 14111, 5918, 14119, 22311, 5948, 5954, 22342, 14160, 5970, 14167, 22359, 22362, 22363, 22364, 14188, 6006, 14200, 14201, 22399, 22410, 6035, 22426, 22429, 14245, 22438, 6067, 14263, 6087, 6090, 14283, 14298, 6108, 6111, 6115, 22502, 14319, 22511, 22515, 22518, 6147, 6163, 22547, 6165, 22554, 22556, 6174, 6178, 14373, 14393, 22586, 14400, 6214, 22606, 14420, 14433, 22634, 6254, 22641, 22649, 22655, 22660, 14471, 22669, 14478, 14488, 6306, 14498, 14500, 6318, 22716, 22717, 6343, 14538, 22739, 6358, 14556, 22774, 14592, 22785, 6401, 6403, 14601, 6411, 6412, 14603, 22798, 14607, 22803, 14611, 22808, 6430, 14630, 22826, 14637, 6450, 14644, 6454, 6456, 22841, 6484, 6487, 6495, 14696, 22891, 14700, 14701, 22893, 6511, 6530, 6531, 22914, 14728, 6543, 14738, 22930, 14744, 22942, 14758, 22953, 22957, 14766, 6584, 22977, 6596, 22981, 22985, 6603, 6604, 22990, 14799, 23004, 6627, 23013, 14834, 23035, 6656, 14861, 23055, 6672, 23075, 23085, 14902, 23095, 23109, 23113, 23117, 23127, 6743, 6746, 23140, 6759, 6761, 14957, 6771, 14970, 6781, 6790, 23181, 23202, 6820, 6822, 23217, 23221, 15031, 15033, 15035, 15038, 15042, 23242, 6860, 23246, 15055, 6864, 6866, 6881, 6882, 23271, 6888, 23278, 23283, 15100, 15101, 6913, 23298, 6917, 15116, 15118, 23313, 23316, 15131, 6954, 23354, 23361, 23365, 23370, 6988, 6992, 23381, 7002, 15201, 7010, 7011, 7014, 15206, 15209, 15226, 7038, 15235, 23428, 7043, 23433, 7060, 23448, 7067, 7072, 7082, 15275, 23481, 23490, 15302, 23497, 7117, 15311, 7119, 23505, 7122, 23510, 15325, 7135, 7137, 7138, 23524, 7141, 7151, 23536, 23543, 7164, 15356, 7166, 7173, 15371, 23565, 7188, 15385, 7193, 7198, 15395, 7210, 23603, 7220, 23605, 23613, 7235, 15427, 23626, 15435, 15450, 23646, 23648, 7268, 7284, 23668, 15483, 15484, 23677, 23696, 15512, 7321, 23714, 23716, 7333, 7338, 15554, 23761, 7383, 7389, 23779, 15590, 15596, 23791, 15601, 7425, 15638, 15639, 15646, 15647, 23840, 15654, 23854, 23855, 23857, 23868, 15682, 7505, 7507, 15708, 15709, 15711, 23924, 7551, 7553, 15746, 7557, 7565, 15767, 7577, 23972, 23975, 23977, 7603, 23987, 7607, 15804, 15808, 15809, 24005, 24010, 15823, 24018, 15827, 24020, 24021, 15832, 7644, 15841, 7651, 7655, 7664, 7665, 24051, 7669, 15865, 15870, 15877, 7688, 7691, 15885, 7699, 24083, 24088, 7709, 7720, 24105, 7722, 7725, 15918, 24111, 7727, 24116, 24126, 7749, 15944, 24147, 7764, 7768, 7773, 24161, 24162, 24164, 24165, 7785, 15980, 15986, 15992, 15997, 24196, 7813, 16006, 16018, 7829, 16021, 16026, 24232, 24236, 24239, 7860, 7873, 16072, 7884, 7890, 24278, 7898, 16091, 7905, 24299, 16108, 16109, 24301, 16112, 24306, 16117, 24312, 7937, 16130, 16132, 24332, 16142, 24334, 24337, 24338, 16173, 7988, 7991, 16184, 8003, 8009, 16202, 8036, 16229, 16234, 24435, 24439, 24446, 8067, 16259, 24453, 24456, 24459, 8076, 8083, 16278, 8095, 16290, 8105, 16302, 16305, 8115, 8130, 8139, 16337, 16343, 16355, 16364, 8182}, 7: {9, 11, 16403, 16404, 8214, 8226, 35, 16425, 49, 16433, 8244, 8252, 73, 8269, 16461, 8278, 90, 8287, 8288, 96, 104, 113, 16503, 119, 125, 126, 127, 128, 8317, 16510, 16520, 143, 16533, 152, 16537, 153, 16541, 16548, 16550, 8358, 8369, 16564, 184, 8380, 210, 8409, 16603, 16611, 16619, 16627, 16631, 16637, 16638, 263, 8457, 16665, 8476, 8477, 8500, 16700, 16705, 8519, 16718, 16721, 338, 16723, 343, 16733, 8556, 368, 16757, 8567, 384, 393, 16777, 16787, 16795, 413, 16799, 16802, 432, 435, 16820, 440, 16828, 8636, 16833, 16834, 16835, 16840, 465, 486, 488, 8693, 16889, 509, 16895, 16904, 8713, 529, 537, 539, 551, 8755, 16948, 8757, 8768, 16962, 16968, 16975, 602, 8797, 16996, 8813, 622, 17020, 8829, 641, 17025, 8836, 8851, 8853, 17046, 664, 8859, 17053, 672, 8868, 679, 683, 684, 17074, 17075, 17077, 17082, 8892, 17086, 17090, 708, 17094, 715, 8909, 8911, 721, 17113, 735, 8933, 17133, 756, 17141, 762, 17160, 17161, 781, 17165, 783, 785, 8979, 17183, 802, 9000, 810, 17194, 814, 9014, 17210, 9024, 17218, 835, 9042, 852, 17238, 17241, 862, 863, 875, 17265, 17269, 17270, 887, 894, 901, 17289, 9098, 919, 921, 17307, 17309, 9119, 17312, 9125, 17319, 9139, 17333, 17335, 17354, 17356, 17382, 9208, 1018, 9211, 9212, 9213, 9216, 9217, 9220, 17413, 1028, 9225, 9245, 17440, 17442, 1060, 17456, 9273, 1086, 17470, 17475, 17494, 17500, 17502, 17503, 1120, 1126, 17513, 9325, 1138, 17528, 9338, 17534, 9359, 1167, 17553, 17554, 9365, 9375, 17570, 9381, 9383, 17582, 17586, 9407, 1217, 9417, 17610, 1236, 1241, 9450, 17647, 9471, 9477, 1286, 9480, 9481, 1293, 9489, 17683, 9495, 1306, 9499, 9503, 17700, 1319, 1322, 17711, 17717, 9532, 9535, 17727, 17731, 17734, 17737, 17741, 17742, 1368, 1371, 1372, 1378, 17765, 17766, 1385, 9578, 9586, 17782, 17783, 9608, 9623, 9626, 1435, 9645, 17843, 17847, 1469, 17854, 1471, 1476, 1482, 9675, 17867, 17871, 1489, 9695, 1504, 9698, 9704, 9705, 1515, 1516, 9708, 1535, 9731, 17925, 17930, 9741, 1556, 1561, 1567, 9766, 17960, 9773, 1583, 1595, 17984, 9810, 9815, 9816, 18013, 18014, 9823, 1634, 18020, 1651, 9854, 9857, 18055, 18056, 9872, 18072, 9884, 9885, 9902, 18099, 1716, 1717, 1734, 9931, 1741, 9944, 9947, 9948, 1759, 9951, 9953, 18149, 18155, 1773, 9969, 18170, 1789, 18174, 18191, 18192, 18193, 10001, 1814, 1820, 10012, 1826, 10026, 18230, 10042, 18242, 18248, 18250, 10066, 10075, 1884, 1886, 10080, 1897, 10091, 18283, 10097, 1912, 1913, 10129, 1961, 1966, 1987, 1989, 18380, 10198, 10201, 2012, 10214, 10218, 10224, 2032, 2037, 18424, 2042, 10242, 2051, 2055, 10255, 10268, 18462, 2082, 10276, 10277, 2092, 10288, 10290, 10295, 18488, 2107, 2111, 2112, 2118, 2129, 10327, 10328, 18524, 10343, 2151, 2152, 2162, 2169, 18558, 18562, 18573, 18584, 2212, 18600, 10413, 18618, 2235, 18626, 18640, 18642, 2264, 10457, 2266, 2279, 2280, 10477, 2300, 18690, 10499, 2310, 2314, 18703, 10514, 10515, 18711, 2338, 10538, 2348, 10564, 18758, 10574, 10580, 2396, 2397, 2399, 10591, 2408, 10601, 18796, 10607, 2416, 2427, 2428, 18821, 18825, 18826, 18834, 10643, 2461, 10654, 2469, 2475, 2485, 2489, 2492, 10688, 10694, 10696, 2506, 2512, 2513, 10713, 18906, 2536, 18925, 18928, 2552, 2553, 18939, 18942, 18943, 2560, 18949, 10757, 18955, 10771, 2581, 18977, 2594, 18984, 18991, 10803, 2619, 19004, 19011, 2629, 19013, 2636, 10830, 19026, 2649, 10843, 19050, 10865, 2676, 10870, 2684, 2695, 10890, 10892, 10899, 2709, 2711, 2714, 2721, 19113, 2734, 19132, 19133, 10943, 10944, 19149, 2776, 2784, 2788, 10988, 10989, 10990, 19183, 10993, 19195, 2828, 19221, 19241, 2860, 11053, 11055, 2871, 2886, 2891, 11093, 19292, 2917, 2918, 19304, 11133, 11134, 11137, 19338, 19339, 19342, 11159, 11160, 19358, 2976, 19365, 19368, 19373, 3006, 3009, 3021, 3025, 19419, 19424, 11234, 19431, 19438, 11258, 3071, 11272, 11275, 11278, 3102, 3105, 3106, 19492, 11305, 11307, 11318, 11331, 19524, 3144, 3146, 11341, 19536, 19539, 3157, 19542, 3169, 11362, 11366, 11367, 19560, 19561, 3180, 3183, 3191, 19581, 11397, 19594, 3210, 11411, 3221, 11415, 3225, 11428, 3243, 19629, 19640, 3264, 11457, 3269, 11470, 3279, 19667, 11478, 3286, 3298, 3299, 11501, 19694, 3309, 19714, 3330, 19719, 11532, 19725, 11534, 11548, 19759, 3376, 3379, 19769, 3391, 3393, 11589, 19783, 19784, 11593, 3409, 19796, 11612, 3427, 11624, 11627, 11632, 11639, 11644, 19847, 19851, 19853, 19870, 3488, 3490, 3497, 11690, 11692, 3505, 19893, 3510, 19895, 11708, 19904, 3522, 11716, 3524, 19911, 19915, 11724, 11734, 3543, 11735, 19927, 19931, 3549, 3556, 19941, 3559, 3564, 3581, 11776, 3593, 19988, 3607, 11814, 20010, 3628, 11824, 20019, 20024, 20025, 11847, 3659, 20051, 11866, 3678, 3679, 20066, 20072, 3704, 20089, 3711, 11906, 20103, 3729, 3737, 11936, 11946, 3756, 20140, 3762, 20155, 20159, 20163, 3782, 3788, 3790, 11984, 3796, 11990, 11993, 3805, 11999, 12000, 3808, 3811, 20202, 12015, 3829, 12023, 20217, 12026, 12025, 3839, 3842, 3849, 12044, 12048, 20241, 3857, 3858, 20240, 12053, 12058, 3867, 20258, 3883, 20274, 12083, 12086, 20279, 20281, 12093, 12102, 12103, 20300, 20304, 3922, 20337, 20339, 12149, 3962, 20347, 12159, 12161, 12173, 3982, 12175, 12176, 20365, 12189, 4001, 12198, 4007, 12200, 4013, 4021, 20406, 4022, 20408, 4026, 20416, 12226, 4038, 20424, 12239, 4047, 4068, 12271, 12276, 20473, 4089, 20482, 12293, 4101, 4113, 20513, 4132, 20521, 12339, 4163, 12356, 20549, 4173, 4178, 20566, 20570, 20580, 12392, 12393, 12395, 20590, 4209, 20596, 4214, 12414, 20616, 20636, 12444, 12452, 20657, 4279, 4280, 20672, 20675, 20680, 12493, 4308, 4312, 4314, 12512, 20710, 4329, 20715, 12525, 12526, 12532, 20726, 12546, 4373, 20758, 12577, 4385, 4395, 12604, 20803, 20807, 20809, 4428, 20813, 20818, 12627, 12629, 12631, 20825, 20841, 20845, 20847, 20853, 4471, 12668, 12669, 20875, 20884, 4504, 20899, 12714, 20914, 12729, 20923, 12738, 4547, 20930, 4552, 12744, 20936, 4558, 20945, 12754, 20965, 12775, 20977, 12787, 20984, 4606, 4616, 12816, 21009, 4630, 4638, 12843, 4659, 21049, 4669, 12872, 21070, 4686, 4692, 4696, 21095, 12907, 4717, 4718, 12911, 21102, 21108, 12920, 4730, 4742, 21128, 21136, 21144, 21145, 4767, 4768, 21152, 12960, 12964, 4775, 12970, 4780, 4781, 4785, 12983, 4792, 4807, 13004, 4818, 13011, 4827, 4840, 13037, 21237, 4854, 13049, 4869, 13063, 4872, 21259, 21271, 21292, 21300, 21310, 4939, 21323, 13133, 21327, 13139, 21346, 4967, 21353, 13167, 4978, 21362, 4980, 4983, 13183, 13187, 13190, 5004, 13207, 5016, 5020, 13214, 13218, 5030, 13228, 5038, 13233, 21428, 13239, 21431, 5050, 5056, 13251, 13255, 21453, 21458, 21459, 5075, 5083, 21472, 21474, 5091, 13292, 5103, 13309, 5122, 21507, 21506, 21514, 5131, 21518, 21520, 21522, 13341, 5163, 21549, 13371, 13374, 13375, 21572, 21574, 13389, 5203, 13395, 5205, 5212, 5227, 21615, 13425, 5236, 21626, 5250, 21636, 21638, 21640, 5257, 21651, 5272, 13467, 13473, 21665, 5283, 21669, 5297, 5299, 13492, 5302, 13496, 21690, 5309, 21711, 5333, 5339, 13534, 5346, 13538, 21732, 13540, 21735, 5352, 13573, 21766, 5387, 5388, 5403, 21788, 13603, 21795, 13609, 21816, 13625, 5436, 13629, 13628, 13633, 5443, 5451, 13645, 21856, 5476, 5485, 13679, 5488, 5489, 5499, 13730, 5545, 5549, 5568, 5570, 13769, 5587, 21972, 5590, 5596, 21983, 13805, 22001, 22002, 5619, 22006, 5639, 22033, 13852, 22048, 5666, 5682, 5685, 22070, 5688, 5689, 5691, 22075, 5708, 22095, 5717, 5724, 13916, 22116, 5735, 22122, 13937, 13938, 13946, 13952, 5761, 13956, 13962, 5770, 13966, 13972, 13981, 22176, 14019, 14022, 5838, 5839, 5845, 22230, 5871, 22256, 5885, 22280, 5897, 14090, 22281, 5904, 5909, 14113, 5923, 14124, 5932, 22318, 5933, 5942, 22331, 14143, 22337, 22348, 14176, 14177, 14191, 22387, 14195, 14205, 6016, 22403, 6021, 6027, 14223, 22423, 22425, 14240, 6051, 22435, 6053, 6057, 14254, 14255, 6064, 6065, 14265, 22465, 22467, 6088, 22472, 14280, 14285, 6096, 14288, 22483, 6101, 22488, 22491, 14300, 14302, 22497, 6114, 14308, 14312, 22516, 6134, 22527, 22528, 14342, 14353, 14368, 6182, 22570, 6188, 14391, 6210, 22600, 6218, 22611, 6234, 6235, 6238, 14436, 22630, 14448, 14449, 6258, 22651, 6268, 14462, 14470, 6283, 22670, 22672, 6288, 14484, 6293, 22680, 22687, 6307, 14502, 22697, 22709, 6328, 14525, 14529, 22728, 22729, 6348, 14542, 22743, 14557, 6367, 14560, 22755, 6372, 14573, 6389, 6396, 22795, 22800, 14610, 14621, 22817, 6441, 6448, 22835, 22837, 6455, 6457, 6459, 22848, 22863, 14683, 22877, 6498, 14697, 14698, 6506, 6514, 6518, 6523, 6525, 14718, 14720, 22924, 6545, 6552, 14750, 6565, 6573, 6576, 22960, 6580, 22964, 22966, 14776, 14783, 14785, 6594, 14791, 6599, 14800, 14802, 23000, 14808, 6620, 23022, 23024, 6643, 23029, 14840, 6666, 23051, 14863, 14869, 6677, 6679, 23065, 14875, 14886, 6695, 14892, 6700, 6707, 14906, 6716, 6723, 14919, 23114, 14928, 23121, 14935, 14940, 23137, 14946, 23144, 23145, 23146, 6765, 23155, 23157, 23162, 23169, 6788, 14988, 6797, 6806, 6807, 23191, 6811, 6812, 6817, 15011, 6821, 15016, 23209, 6833, 23218, 15026, 23220, 15028, 23219, 6841, 23229, 6849, 23239, 6859, 23245, 6865, 15058, 23253, 15067, 15075, 15080, 6896, 23281, 23284, 6902, 6904, 23300, 15111, 6926, 6930, 15126, 6936, 6940, 6945, 6947, 15150, 6959, 23353, 23358, 23367, 23368, 15197, 15202, 7027, 15224, 23416, 23418, 7045, 15244, 7057, 23444, 23458, 15271, 15272, 15276, 15279, 15284, 23477, 15290, 15291, 7102, 15305, 15310, 23504, 23507, 7128, 23517, 23525, 7142, 15340, 23534, 7154, 7165, 15357, 23551, 23549, 23558, 7181, 7189, 23575, 15387, 7195, 15392, 7203, 7204, 15407, 15410, 15415, 7224, 15428, 7239, 15445, 15463, 7273, 7278, 7288, 15488, 23687, 7305, 15501, 15519, 7339, 15531, 15535, 23729, 23731, 15546, 15548, 15550, 7363, 23773, 23787, 23789, 15604, 23798, 15607, 7420, 23805, 15615, 15623, 23819, 7439, 15633, 15637, 15641, 7461, 15658, 7466, 7469, 15669, 7485, 15681, 23875, 7496, 15697, 7509, 7510, 15712, 23908, 7530, 7533, 7546, 23931, 23935, 15743, 15745, 23939, 23947, 7569, 7570, 7575, 15773, 15775, 15779, 7595, 7596, 7617, 24009, 24013, 24015, 24027, 7646, 15838, 15846, 15847, 24042, 15852, 7666, 7671, 15866, 24065, 24066, 15874, 24071, 15886, 7695, 15888, 7694, 15893, 15894, 15907, 15908, 15911, 15914, 15917, 7734, 15927, 24122, 15933, 15934, 7742, 7748, 15941, 7760, 24149, 7766, 15961, 15968, 7777, 15977, 24175, 15987, 15988, 15991, 24191, 7814, 24205, 16017, 24210, 7834, 24224, 16033, 16035, 7846, 24230, 16039, 24234, 7863, 16055, 7870, 7881, 24267, 24272, 7895, 16089, 16095, 7904, 7909, 24298, 7918, 7924, 16121, 24320, 7936, 16141, 16149, 7960, 24346, 24347, 16171, 16172, 7985, 7986, 24388, 8007, 24392, 24395, 8012, 16205, 24398, 24406, 24418, 8039, 8041, 24428, 24437, 24438, 16253, 24448, 16257, 8072, 24466, 24474, 16285, 8094, 16287, 16293, 24487, 8108, 8109, 24500, 8132, 8138, 8147, 8158, 16352, 8165, 16365, 16368, 8180, 8184, 8189}, 8: {1, 16387, 10, 16396, 14, 8207, 17, 8211, 22, 16408, 16419, 16427, 16428, 43, 48, 56, 16442, 8256, 65, 83, 16474, 8283, 92, 16477, 93, 8291, 16484, 8306, 16506, 16522, 16524, 148, 16536, 8351, 159, 16546, 163, 164, 169, 8363, 8368, 180, 186, 8381, 192, 16585, 16597, 218, 16604, 8422, 233, 244, 16634, 16662, 8471, 281, 8483, 16682, 16683, 16698, 16699, 16701, 8509, 323, 324, 326, 16712, 8523, 8525, 16724, 16732, 16734, 8544, 8550, 16747, 364, 8559, 8565, 8571, 16773, 8581, 16779, 399, 408, 8612, 16805, 16822, 442, 8638, 447, 458, 8655, 8671, 16869, 8687, 8692, 8694, 503, 16886, 512, 16899, 16902, 16906, 8716, 16913, 8724, 8737, 16933, 8742, 556, 16951, 16953, 16967, 8776, 8775, 583, 16973, 8784, 8792, 16992, 615, 17003, 620, 17007, 8820, 17013, 17021, 17032, 8843, 8845, 17062, 8872, 8874, 694, 17079, 8888, 696, 704, 707, 17108, 8920, 17115, 17116, 733, 17128, 745, 8938, 751, 8953, 17146, 17150, 767, 768, 769, 770, 774, 17162, 778, 17164, 17163, 8978, 790, 800, 807, 17195, 9004, 17200, 17202, 832, 17225, 17227, 9038, 17236, 853, 17246, 17248, 865, 867, 868, 17254, 17257, 9075, 9077, 9078, 889, 897, 9092, 9095, 914, 917, 9111, 9116, 9118, 927, 9123, 942, 9134, 950, 955, 17343, 972, 17357, 9165, 17363, 980, 17366, 982, 990, 9187, 17379, 997, 17380, 9192, 17386, 17391, 9219, 17414, 9224, 17427, 17429, 1051, 9243, 1056, 1059, 17454, 9266, 9270, 9272, 9281, 9282, 1090, 9285, 17484, 17492, 1110, 9305, 9308, 17504, 9317, 9340, 9343, 1152, 9348, 1164, 17560, 17575, 17583, 9406, 9412, 17619, 17623, 17642, 17648, 9463, 1278, 9487, 1301, 17689, 1309, 9510, 17702, 17707, 9516, 1337, 1341, 17728, 1344, 9551, 17743, 17745, 9554, 1379, 17768, 17770, 17777, 9587, 17801, 1421, 1429, 1433, 17828, 1447, 17833, 17834, 17848, 9661, 1470, 17858, 9666, 9677, 9678, 17886, 9696, 17894, 9707, 9711, 9715, 17907, 1526, 9721, 1533, 9733, 17933, 1551, 9746, 1558, 1562, 1575, 1580, 9772, 9779, 1588, 17977, 9786, 9789, 1600, 1604, 9819, 18022, 1640, 1641, 9845, 1653, 18041, 9855, 9856, 1667, 9866, 9871, 1680, 9875, 18070, 18079, 9922, 9924, 18120, 1749, 9943, 9960, 1781, 1794, 1798, 9993, 9994, 1803, 9997, 1808, 18195, 10004, 18196, 18205, 10014, 18211, 1837, 1840, 10032, 10036, 1858, 18246, 10056, 1868, 18253, 10064, 1872, 10065, 10067, 1878, 1899, 18284, 1901, 1902, 18286, 18292, 18299, 18300, 10114, 18310, 10124, 10126, 1935, 1942, 10136, 1960, 10154, 18348, 1971, 10166, 18360, 18362, 1988, 1995, 18385, 10195, 2006, 18394, 2014, 10216, 18411, 2033, 18423, 10231, 10233, 2044, 18430, 18431, 18450, 10263, 18456, 10266, 2077, 10285, 2105, 10304, 10315, 2128, 10320, 18514, 18515, 10324, 10331, 18525, 18526, 2144, 2153, 2160, 2161, 10366, 10367, 18578, 10387, 10392, 2201, 2206, 2207, 2211, 18596, 2214, 10411, 18608, 2225, 10421, 18613, 18616, 10427, 10428, 18619, 18627, 2244, 10440, 2254, 2260, 18652, 2269, 10468, 18666, 2282, 18669, 18670, 18675, 10488, 10491, 2304, 10498, 10507, 18709, 10521, 10527, 18723, 10532, 18734, 18740, 2357, 18744, 2365, 18752, 18756, 2374, 2378, 10573, 18767, 10576, 2389, 2392, 10590, 18782, 18783, 18800, 10620, 18815, 2432, 10632, 18833, 10642, 18838, 10649, 18844, 2462, 2463, 2464, 10662, 10665, 18858, 10672, 10678, 10705, 10708, 10717, 10718, 2526, 18929, 18937, 2554, 10750, 18947, 10766, 18958, 10769, 2582, 10779, 10780, 10790, 10799, 2612, 2614, 2618, 10814, 2622, 2626, 2627, 10824, 2637, 10829, 19024, 2643, 2661, 19046, 2663, 10862, 2681, 2682, 10877, 19071, 10882, 2692, 19086, 19119, 2737, 10929, 2741, 10934, 19129, 10940, 2751, 2755, 19144, 10952, 2775, 2783, 10978, 2797, 19184, 2802, 2819, 19203, 11022, 11026, 11028, 11030, 11031, 2841, 2848, 19233, 11042, 11043, 2850, 2858, 11051, 19245, 11064, 19266, 19269, 11081, 19275, 11084, 2895, 19280, 11088, 19286, 2909, 11108, 11109, 19300, 19305, 19307, 2926, 11119, 19314, 2944, 11140, 11146, 11150, 19344, 2974, 2977, 2981, 19366, 19367, 11177, 19369, 19370, 19379, 2999, 3003, 11197, 19394, 19398, 3026, 19425, 3045, 11237, 19436, 3055, 19441, 19445, 19447, 11280, 19475, 11288, 3100, 19501, 19507, 3130, 11322, 11343, 3153, 19540, 19545, 11359, 11360, 11374, 3186, 19574, 19597, 11434, 19627, 19628, 11435, 11442, 11443, 11444, 3263, 11462, 11463, 19655, 3271, 3288, 3289, 3294, 11491, 19686, 3310, 11503, 11507, 3315, 3319, 11525, 19720, 19730, 3355, 3361, 3365, 19751, 11566, 3375, 11567, 19761, 11575, 3388, 3390, 3408, 11600, 3433, 3435, 19832, 3452, 11649, 19842, 19845, 3462, 11658, 3466, 19852, 19857, 3481, 11675, 3486, 3495, 11688, 11695, 3503, 19890, 11706, 3521, 19907, 3532, 11725, 3538, 11745, 3562, 19950, 19952, 19953, 19955, 3577, 11779, 3588, 19986, 3606, 3608, 3611, 3614, 20004, 20007, 20013, 20015, 11828, 11829, 20023, 20029, 11842, 20037, 3654, 11857, 11863, 20060, 20073, 3695, 11890, 20083, 20086, 3715, 20115, 3743, 3745, 20134, 20136, 20143, 11953, 3763, 3770, 11966, 20165, 20169, 20172, 3789, 20177, 3800, 20185, 3801, 3804, 3806, 12001, 3810, 3813, 20198, 12008, 3817, 12010, 3819, 3821, 12021, 20222, 20223, 20226, 3844, 3862, 20248, 12057, 20250, 12062, 3882, 12079, 20272, 12080, 20280, 3904, 20298, 3927, 12120, 3930, 12142, 3951, 3953, 12150, 12151, 20345, 12154, 20352, 20356, 20358, 12172, 3984, 20374, 3993, 12186, 12191, 20384, 4005, 12197, 4009, 20399, 12209, 4018, 4019, 4029, 12229, 4048, 20432, 20434, 4051, 12258, 4074, 20460, 12287, 4096, 20491, 20500, 20515, 4144, 4149, 4159, 20546, 12366, 4175, 12374, 20575, 20576, 12389, 4197, 4201, 12400, 12409, 20602, 12412, 12415, 20608, 12419, 12425, 4236, 12430, 12431, 20623, 4246, 4248, 20633, 4254, 12448, 4257, 12454, 4271, 4278, 12477, 20671, 12482, 20679, 4297, 20690, 12499, 4321, 20711, 12521, 12533, 4342, 4343, 4346, 12543, 4353, 20741, 12552, 20747, 20750, 12586, 20779, 4400, 4416, 20802, 12617, 20812, 12621, 4429, 20820, 4438, 12642, 20840, 12650, 4462, 12656, 4468, 12666, 20859, 12671, 12673, 20866, 4507, 12701, 20894, 4515, 12709, 4518, 12719, 20912, 12721, 4532, 4533, 20920, 20921, 4539, 4541, 20941, 4565, 20949, 4578, 4593, 4599, 4603, 4609, 20995, 12811, 12814, 4623, 12815, 4622, 12825, 21017, 12834, 4658, 12851, 21047, 12869, 21066, 12877, 21082, 4700, 21089, 21093, 4715, 12908, 21099, 12912, 12914, 21130, 12952, 4771, 4776, 21164, 21166, 12976, 12979, 21173, 4790, 4793, 21189, 4822, 21209, 4828, 4831, 13024, 21217, 21222, 21228, 13039, 21232, 21234, 21235, 4853, 4862, 13059, 21265, 4892, 13086, 21283, 21284, 13099, 13108, 13111, 13114, 21313, 4947, 21331, 4952, 13150, 13155, 4964, 13163, 21358, 13166, 4979, 13171, 4984, 21370, 4988, 4992, 13185, 4999, 5001, 13194, 21392, 13215, 21408, 21414, 13231, 13235, 5044, 5047, 13240, 21441, 13249, 5064, 5072, 13269, 5077, 13273, 5092, 13286, 5095, 21479, 21485, 5110, 5115, 13318, 13319, 5132, 13324, 5136, 13328, 21523, 21528, 13337, 5148, 13344, 13350, 5161, 21554, 13368, 5181, 21570, 21571, 5188, 5189, 13384, 13386, 13387, 5196, 13392, 21586, 13400, 5210, 13403, 13411, 21603, 5221, 21620, 13428, 13442, 13443, 5255, 13451, 5261, 21649, 21650, 5276, 5294, 5305, 13500, 13505, 13509, 5322, 13517, 13527, 21719, 21739, 21743, 21744, 13557, 5366, 5383, 13577, 21775, 21780, 5400, 13602, 5424, 21809, 13620, 21815, 21837, 13646, 5455, 13647, 13656, 21851, 5469, 21854, 13671, 5498, 13701, 5515, 13708, 13709, 21913, 5534, 13727, 21925, 13745, 13749, 5572, 13766, 13770, 13772, 5582, 21968, 5589, 5599, 5605, 22004, 22009, 13822, 13835, 5648, 13843, 5660, 5668, 5673, 5675, 5677, 22063, 22067, 22081, 13891, 5699, 5704, 5714, 5715, 13907, 22108, 5725, 22111, 5734, 13929, 13930, 5744, 5745, 13943, 22137, 5753, 13945, 22146, 5763, 22157, 22170, 22178, 22180, 22184, 5803, 22191, 22197, 5814, 5821, 22212, 14020, 22214, 14043, 5852, 5870, 14065, 5878, 22265, 22269, 22274, 14082, 14089, 5905, 5914, 22309, 5929, 5931, 5934, 5950, 5951, 14142, 5955, 22352, 22354, 22360, 5980, 5982, 22370, 22371, 14185, 14198, 6020, 6029, 6031, 22421, 22422, 6060, 6061, 22447, 22452, 14261, 22454, 6076, 22462, 14274, 6082, 6089, 22475, 6092, 22477, 22480, 22484, 22496, 22498, 22517, 14327, 22525, 22529, 6146, 22538, 14348, 14350, 22543, 22557, 6185, 6189, 14387, 22581, 6200, 6204, 6209, 14402, 6211, 22596, 14407, 6227, 6236, 6242, 22632, 14440, 6252, 22638, 22640, 14465, 22658, 6276, 6277, 14469, 22664, 14479, 22675, 6295, 6297, 14490, 22686, 22693, 6316, 14514, 14524, 22721, 22724, 14555, 22748, 6373, 14571, 22767, 22769, 6388, 22777, 22781, 6405, 6417, 22802, 14614, 22807, 6427, 14628, 14633, 6444, 22831, 6447, 6462, 22850, 6468, 6470, 6472, 14672, 14680, 22881, 6504, 6508, 14703, 22911, 22918, 14730, 14732, 14733, 14740, 6554, 22939, 14749, 22946, 22954, 6572, 22967, 14775, 6586, 14780, 14782, 6590, 22980, 14790, 22988, 14796, 22993, 6613, 6617, 14811, 6621, 23009, 23014, 14823, 23033, 6650, 23049, 6674, 14871, 23067, 14876, 14877, 14890, 6708, 23093, 14908, 6718, 6721, 14932, 14933, 6744, 14937, 14948, 14950, 14955, 23150, 14960, 14961, 14963, 6775, 14968, 14982, 14984, 14994, 15002, 23201, 6818, 23203, 6819, 15013, 23207, 23208, 6846, 23232, 6850, 23238, 6855, 23244, 15056, 23255, 23262, 6878, 23268, 15077, 15079, 6892, 6898, 23287, 15103, 15117, 23310, 15120, 6932, 15129, 15137, 23335, 23345, 15159, 6968, 6969, 15164, 23360, 15170, 23366, 23371, 6990, 15189, 23398, 23401, 23403, 15216, 23409, 15217, 23412, 15221, 23417, 23419, 15231, 23429, 15238, 7052, 23437, 23447, 23455, 15265, 23464, 7088, 7092, 15287, 23488, 7108, 23496, 23501, 23502, 23511, 23513, 7130, 23522, 15337, 23538, 23547, 7169, 15364, 23557, 15367, 15383, 23576, 23577, 23578, 15386, 15393, 7213, 23600, 23607, 7231, 7253, 23641, 23642, 15449, 23643, 7270, 15462, 23661, 7280, 15477, 7286, 23675, 7294, 7297, 7315, 15516, 23711, 23718, 7335, 15533, 23732, 15544, 23738, 15549, 7357, 7358, 23745, 7364, 23756, 23762, 15572, 15579, 7396, 7397, 15597, 15600, 23793, 15605, 15616, 7427, 7428, 23813, 7433, 15627, 7437, 15629, 23823, 7454, 15648, 23841, 23848, 23850, 7473, 7474, 7480, 23866, 23871, 7487, 23873, 15685, 7499, 23890, 7506, 23894, 7514, 7517, 15710, 15729, 23923, 7540, 7542, 15741, 23933, 7550, 15749, 15752, 7562, 15756, 15758, 7566, 23951, 7572, 15776, 15794, 7605, 23992, 24003, 24007, 15815, 15820, 7629, 24014, 7633, 15828, 7641, 7643, 24029, 7647, 7658, 15861, 15862, 15864, 7672, 24064, 7681, 7700, 24087, 15898, 7719, 15920, 7730, 15930, 15939, 15948, 7759, 15952, 15955, 24160, 7781, 15979, 7796, 7815, 24202, 7821, 24228, 16042, 16043, 16044, 7866, 7877, 24277, 16092, 24287, 16098, 16111, 16114, 7934, 16128, 7951, 16152, 16160, 7969, 24358, 7977, 16169, 7979, 7983, 16176, 7992, 24384, 16194, 16200, 8011, 8018, 24412, 8040, 24429, 24430, 16242, 8055, 16250, 24442, 24447, 24454, 8071, 16277, 24470, 8097, 24484, 24491, 24499, 8119, 24503, 16313, 24509, 8152, 8153, 16357, 8173, 8176, 16369, 8185}, 9: {16384, 16, 16407, 16411, 16422, 16444, 16452, 16453, 16456, 8270, 16462, 8272, 80, 16468, 16472, 16473, 8284, 8292, 16492, 108, 112, 8310, 120, 8324, 142, 16527, 149, 8366, 16558, 8371, 16570, 16576, 16581, 16591, 8400, 220, 16607, 226, 227, 8420, 8419, 8428, 16626, 243, 8438, 16630, 16632, 252, 253, 255, 256, 8448, 16648, 16651, 274, 8474, 283, 8480, 8481, 293, 8488, 16681, 8494, 8495, 306, 16695, 312, 8510, 8524, 16722, 16726, 16727, 355, 362, 367, 374, 8568, 16763, 8572, 380, 387, 391, 8586, 16780, 16784, 8594, 8605, 8614, 8617, 428, 8622, 431, 8627, 16826, 8635, 446, 16832, 8648, 457, 492, 8688, 8689, 16883, 508, 513, 8705, 515, 519, 16910, 8719, 536, 8728, 8729, 16928, 8745, 563, 16949, 16956, 573, 8765, 8783, 593, 595, 8790, 8799, 16991, 607, 617, 17018, 17023, 8833, 17027, 17042, 8856, 667, 8864, 8871, 17072, 698, 17084, 17088, 705, 8899, 710, 712, 717, 718, 8925, 17118, 738, 740, 744, 17134, 17143, 8951, 17149, 766, 8959, 17152, 17155, 779, 782, 8975, 784, 17172, 792, 17188, 9009, 823, 840, 850, 864, 9057, 9061, 871, 9069, 9073, 17268, 886, 9096, 17292, 17299, 17300, 17305, 17313, 17315, 9124, 9132, 17327, 17336, 9149, 17344, 17350, 17353, 9163, 975, 9168, 17368, 9182, 17381, 9194, 9196, 1007, 17394, 17397, 17411, 9221, 9222, 1031, 1041, 17428, 1049, 17433, 17438, 17441, 1066, 17455, 17461, 1079, 1093, 17478, 1103, 1104, 17495, 1113, 1117, 1118, 9315, 17522, 17523, 1144, 1150, 1151, 1156, 17542, 9350, 9353, 17550, 17557, 1175, 9369, 1180, 17569, 1191, 1195, 9389, 9391, 9392, 17585, 17587, 17591, 1207, 9401, 17595, 1216, 17601, 17611, 9422, 9429, 17625, 9438, 1250, 1256, 17654, 1275, 9467, 9475, 17668, 1284, 17670, 1287, 17674, 9497, 17699, 9507, 1316, 9511, 9514, 1324, 9519, 1330, 17716, 9527, 1345, 9538, 17732, 17736, 1354, 9557, 17755, 17759, 17780, 1397, 9590, 1399, 17786, 1402, 1410, 17800, 9612, 17812, 1440, 1441, 17827, 1443, 17832, 9641, 9646, 9648, 1456, 1463, 9659, 1473, 1474, 17870, 1486, 9681, 9686, 1495, 17884, 17889, 17891, 9703, 9709, 9713, 9717, 1528, 1536, 1544, 17935, 17952, 9762, 9768, 17965, 1590, 17981, 1606, 1607, 9800, 9801, 1612, 1614, 18006, 18007, 9818, 18011, 9821, 9822, 1631, 18019, 9830, 18026, 18027, 1652, 1664, 18058, 1675, 1677, 1691, 18076, 1713, 9921, 9926, 18123, 1742, 1745, 18132, 18137, 18147, 1766, 18152, 18158, 18164, 1791, 9984, 18178, 1800, 10000, 10003, 1812, 10007, 1817, 1828, 18212, 18221, 1841, 10034, 1847, 18235, 1853, 18239, 10052, 18244, 1880, 18266, 10074, 18268, 1893, 1896, 10088, 18291, 18294, 1919, 18303, 18306, 18309, 10118, 10119, 1930, 18316, 1933, 18327, 1944, 18329, 18334, 1953, 1958, 18344, 10156, 10160, 10162, 18359, 10171, 1981, 1984, 1985, 10209, 10211, 18405, 18407, 10217, 18410, 10221, 18426, 18434, 18439, 18441, 10256, 2072, 2073, 18459, 2095, 18486, 2103, 2110, 18501, 10323, 18518, 2140, 18529, 18540, 18545, 18551, 18564, 18574, 10384, 2198, 2204, 18590, 18591, 10400, 18592, 2237, 10447, 18639, 18641, 10452, 10455, 10458, 18654, 18659, 10469, 18663, 2290, 2293, 18683, 18694, 10504, 18699, 10513, 18707, 10518, 10525, 2341, 2343, 18728, 10544, 10546, 10547, 10550, 10554, 18747, 18749, 2366, 10559, 10566, 10568, 2386, 10582, 18776, 18777, 10586, 18778, 18786, 10599, 10606, 18805, 2429, 10625, 2437, 10634, 18831, 2448, 2457, 2466, 10661, 18860, 18861, 18863, 10679, 2490, 18875, 2503, 2509, 2510, 10710, 18921, 10729, 10731, 18954, 10773, 18966, 18971, 2588, 2592, 18976, 18982, 18990, 18992, 10809, 2625, 19015, 2632, 2639, 2645, 19037, 2659, 2666, 19051, 19059, 19072, 19074, 19081, 10894, 10908, 2717, 10918, 2728, 19115, 19120, 10932, 2745, 10939, 10953, 19150, 10959, 10960, 2771, 10966, 19171, 10985, 10987, 19185, 2803, 19191, 2808, 19192, 10999, 2814, 11009, 11016, 19212, 19213, 2830, 11027, 2859, 11052, 2861, 19249, 11058, 2873, 11070, 2881, 11075, 11076, 11079, 11085, 19284, 11092, 2903, 19287, 19301, 2919, 11130, 2939, 19325, 19330, 2953, 2963, 19349, 11158, 2979, 11171, 2987, 2990, 2991, 19376, 19383, 3000, 19386, 11196, 19405, 3031, 3037, 11249, 11257, 11265, 3077, 19469, 3091, 3094, 19485, 19495, 19496, 3113, 11315, 19515, 19517, 19521, 3139, 11347, 19550, 19555, 3171, 11369, 19565, 19568, 19569, 19573, 3194, 11388, 3197, 11390, 3216, 3223, 11419, 11420, 11425, 11427, 11438, 3248, 11472, 19665, 11480, 3295, 3300, 3301, 11499, 11504, 19697, 19704, 19709, 19712, 11526, 3351, 3352, 11543, 3354, 3357, 11551, 19746, 3362, 3364, 11558, 11559, 3369, 3370, 11563, 11564, 11571, 11577, 3389, 11591, 19792, 19794, 11603, 3418, 3421, 11615, 19812, 3434, 19825, 11635, 11641, 11643, 3453, 19841, 19844, 19846, 19854, 3476, 3478, 11671, 3484, 11677, 3487, 19878, 19880, 19882, 3500, 19884, 19886, 3508, 3515, 11717, 11728, 19921, 11732, 3541, 3546, 11739, 11750, 19949, 19954, 3576, 19961, 19964, 3586, 11780, 19974, 19977, 3594, 11786, 3596, 3598, 3600, 11793, 3604, 11797, 3605, 3609, 11812, 3629, 11822, 11839, 11840, 3652, 11856, 20054, 3671, 11862, 11867, 3682, 11879, 11902, 11904, 3722, 20108, 20111, 11923, 11927, 3741, 3747, 11941, 11949, 20166, 20179, 20182, 11994, 3802, 20190, 20192, 3809, 12006, 3824, 12024, 3833, 20219, 3848, 20239, 20249, 12063, 12072, 3885, 20273, 12096, 12100, 3909, 12109, 20310, 12131, 3958, 3964, 20348, 3970, 3972, 12164, 3986, 3995, 12187, 3999, 20383, 20385, 20386, 12195, 4010, 12208, 20423, 20426, 12236, 4049, 12242, 4055, 12249, 4059, 20445, 4066, 12259, 12260, 12261, 4073, 4078, 20465, 20468, 12279, 20475, 4092, 20481, 12292, 20494, 12305, 4118, 12314, 20517, 4136, 12331, 20526, 4143, 20536, 12346, 20548, 20556, 20557, 20560, 20563, 20571, 12390, 4199, 12403, 12408, 20615, 12426, 12427, 12436, 20628, 4255, 4264, 20650, 12461, 20655, 12465, 20659, 4277, 12469, 12474, 20684, 4301, 12502, 4310, 12507, 20704, 20712, 12522, 4340, 20730, 12542, 20738, 12554, 20752, 4369, 4372, 12566, 4381, 20767, 4384, 4387, 20781, 12592, 20788, 4415, 20805, 12615, 4430, 4440, 4441, 12635, 4446, 12647, 20844, 20851, 12665, 20861, 20874, 4494, 4498, 12712, 12715, 20910, 12732, 12736, 12739, 20935, 4553, 4555, 12749, 12763, 4581, 4583, 12778, 4596, 12789, 20980, 12798, 4607, 20992, 12802, 4614, 12808, 21018, 4642, 12839, 4653, 21038, 21042, 21064, 4681, 21067, 21069, 12880, 12882, 4697, 12892, 12893, 12917, 21113, 12934, 12940, 21147, 21155, 21158, 12992, 12999, 13001, 21197, 13007, 13012, 13018, 21213, 13031, 13038, 13052, 4861, 13068, 13073, 21266, 4883, 4895, 13088, 21290, 21293, 4912, 4914, 21312, 4934, 21335, 13152, 4960, 4963, 4965, 21352, 4971, 21360, 13169, 4985, 4989, 4993, 4995, 13201, 13202, 21406, 13217, 5026, 21416, 13230, 21427, 5045, 21432, 5061, 21452, 13260, 13277, 5089, 5097, 13290, 5100, 13300, 5114, 13307, 21500, 13314, 5124, 13321, 13331, 13335, 5159, 13352, 21547, 5174, 5176, 13378, 13390, 21593, 21596, 5214, 5222, 21607, 21609, 21612, 21619, 21652, 13460, 21655, 13469, 5280, 13482, 5298, 5307, 5325, 13535, 21728, 13539, 21733, 13542, 13546, 21740, 13549, 5367, 13560, 21752, 5372, 13571, 5382, 21770, 13584, 5397, 13590, 13592, 5401, 21792, 5410, 21796, 5430, 5434, 13627, 5453, 13651, 5466, 13661, 21859, 21862, 13673, 5481, 13681, 5507, 21892, 21897, 21899, 21900, 13707, 13731, 5546, 21936, 5554, 5559, 5566, 13784, 5598, 13798, 5607, 5621, 22008, 22010, 5644, 5646, 13842, 13845, 13850, 5659, 13853, 13860, 13870, 5679, 13873, 22069, 22072, 13886, 22080, 5701, 22085, 22087, 5707, 22093, 13905, 22097, 5713, 13910, 5721, 5729, 22118, 5737, 5743, 5747, 22135, 5756, 22148, 13973, 22168, 13980, 5794, 5798, 5801, 22185, 5806, 22192, 5810, 5813, 14008, 22200, 14015, 5836, 5837, 5841, 14038, 14042, 14045, 22238, 5854, 14049, 14050, 5857, 5866, 14061, 22261, 5877, 22272, 5888, 22273, 14102, 14107, 14112, 5922, 14126, 14127, 5939, 14138, 14141, 5953, 22345, 22346, 14159, 5967, 5971, 22373, 14186, 5994, 6005, 6009, 14202, 6011, 14206, 22401, 6019, 14211, 6036, 6038, 14231, 14234, 14238, 6058, 14253, 22457, 14273, 6083, 14278, 14314, 14317, 22519, 6139, 22523, 6140, 22531, 6148, 22532, 22539, 14351, 6160, 22549, 14358, 6166, 22553, 6177, 22562, 6183, 14379, 6187, 14389, 14392, 22588, 6208, 14403, 22599, 6229, 22614, 6231, 22618, 14426, 14431, 14435, 6245, 6248, 22639, 14452, 22646, 22648, 22666, 6286, 14482, 6291, 6321, 6323, 6337, 14540, 14545, 22738, 6357, 6360, 6365, 14569, 14580, 6391, 14585, 6404, 22792, 22801, 22804, 6422, 14623, 14627, 22824, 22825, 14634, 14641, 14642, 22834, 6458, 22843, 14653, 22845, 22853, 6473, 14677, 6494, 22884, 22888, 6513, 14708, 6520, 6521, 6524, 6526, 6532, 6535, 22925, 14752, 22956, 14777, 22969, 6587, 6601, 22996, 6618, 14812, 6623, 23010, 14820, 14827, 23021, 14831, 23025, 6646, 23044, 6662, 14854, 23053, 14862, 23060, 6687, 14895, 14900, 14904, 14909, 23102, 23103, 23106, 23110, 23112, 14923, 14924, 6735, 6742, 6756, 23143, 14958, 6767, 23151, 14971, 14977, 6796, 6800, 14999, 6810, 6813, 15007, 15019, 23213, 6831, 23225, 6844, 23231, 15045, 6876, 15068, 23264, 6885, 6887, 23272, 6897, 15092, 6908, 15109, 6918, 6919, 15114, 6928, 6929, 23322, 23325, 23328, 6946, 15141, 23334, 23333, 15143, 6955, 15149, 6962, 23349, 6965, 6967, 15161, 15163, 6973, 23357, 6978, 23362, 6994, 6998, 23384, 15194, 23388, 23389, 23395, 15208, 15222, 7030, 23425, 23431, 23438, 15247, 7061, 7064, 23465, 7081, 7093, 7096, 23499, 15312, 7121, 15324, 7133, 23530, 7148, 15344, 23545, 15363, 7171, 7175, 23563, 23564, 23566, 15377, 15378, 15380, 15382, 7192, 15388, 7196, 7201, 7208, 23593, 23596, 7214, 15411, 7221, 23615, 23616, 23617, 23619, 23621, 23622, 23624, 23633, 15444, 7257, 7259, 15454, 23653, 15466, 15467, 15474, 7289, 7296, 15491, 7299, 23685, 15493, 23691, 15500, 23693, 15502, 7311, 7327, 15521, 7330, 7356, 23747, 7367, 23751, 15569, 15571, 15583, 15586, 23780, 23786, 15611, 7422, 23809, 23818, 7438, 7443, 23836, 23837, 15649, 15650, 7458, 7483, 15676, 15683, 7494, 15692, 23885, 15694, 15698, 15702, 15713, 7522, 23911, 7531, 7532, 15726, 7544, 15739, 15747, 15748, 23943, 15753, 15757, 23955, 23956, 23961, 23966, 15774, 23968, 23971, 15784, 23978, 7599, 15793, 15799, 23997, 15806, 24008, 7625, 7626, 15819, 15817, 7630, 24017, 7635, 15831, 24026, 15844, 7659, 7660, 24045, 24049, 15857, 24052, 15878, 15884, 24077, 7698, 24084, 7705, 7708, 15915, 24114, 7735, 15929, 24128, 24130, 7747, 7746, 24133, 24146, 15957, 7771, 24167, 7791, 24180, 15993, 15994, 7803, 7807, 24195, 16003, 7816, 7828, 16024, 16030, 24225, 24226, 24237, 7857, 7858, 7864, 7867, 7868, 7871, 16064, 16066, 24261, 7882, 16084, 7906, 16099, 24294, 16104, 16107, 7920, 16118, 7928, 7938, 24327, 16158, 7966, 7989, 24379, 24381, 16191, 16193, 8001, 24389, 8015, 24405, 24408, 8030, 16226, 8034, 24424, 16233, 8045, 16249, 8059, 8062, 16261, 16265, 8075, 8081, 16274, 16279, 8089, 16286, 16301, 24495, 16320, 8135, 16346, 8157, 8164, 8170, 16373, 8186, 8187}, 10: {8194, 4, 8210, 8215, 16409, 16410, 8222, 34, 40, 8234, 44, 8242, 16440, 60, 8253, 16449, 16454, 72, 16459, 16463, 16466, 8279, 16471, 91, 8290, 16488, 8297, 16494, 111, 16496, 16512, 130, 8323, 8325, 16525, 144, 8339, 8341, 16553, 16554, 16556, 16560, 16566, 8385, 8388, 16584, 209, 214, 215, 8413, 223, 16610, 230, 235, 16620, 16621, 8445, 8446, 8454, 16654, 16659, 277, 8470, 16669, 286, 285, 16675, 16676, 292, 294, 305, 8506, 8508, 16708, 8517, 8516, 8522, 8532, 8535, 347, 8543, 8555, 16749, 16752, 8560, 16760, 8570, 394, 16782, 16785, 16790, 406, 412, 8616, 16808, 16810, 8619, 16814, 16815, 433, 8630, 16847, 8665, 8684, 16877, 16880, 498, 8706, 16903, 16905, 16907, 525, 8722, 8723, 532, 533, 8725, 8731, 8734, 8744, 16943, 16947, 16950, 569, 8772, 8773, 8785, 16983, 8798, 611, 8814, 17011, 645, 650, 8846, 657, 8850, 17043, 17045, 17052, 669, 673, 17061, 17067, 17070, 8879, 17080, 8889, 17087, 8905, 17106, 723, 8919, 8942, 750, 8961, 8966, 17176, 8991, 825, 17216, 9025, 9026, 837, 843, 9036, 9037, 848, 9048, 9051, 17249, 17255, 9067, 9074, 883, 884, 17271, 17274, 891, 9084, 17277, 17275, 17281, 899, 17283, 9093, 17286, 9100, 17296, 9110, 920, 17310, 17317, 9126, 17321, 17322, 940, 9135, 17328, 946, 17332, 9141, 9140, 17346, 963, 17361, 9174, 17370, 987, 17372, 9186, 9188, 9195, 17389, 17392, 1009, 1016, 17412, 1034, 17420, 9244, 17437, 1061, 17446, 1063, 1069, 9271, 17464, 17467, 1095, 17479, 1098, 9291, 9307, 9309, 9318, 1135, 1145, 9339, 9344, 1174, 1176, 17561, 9371, 17567, 9376, 17571, 17572, 17574, 17581, 1198, 17584, 9394, 17588, 1205, 1204, 1210, 9405, 1213, 9414, 9428, 1243, 1245, 9442, 1254, 9448, 1258, 1267, 9461, 1289, 1297, 17684, 17687, 1303, 1305, 9496, 9498, 1312, 17704, 1325, 9521, 17713, 9533, 9536, 9537, 1347, 17733, 9549, 1363, 9561, 1382, 9577, 1386, 1388, 9585, 9597, 1417, 1418, 17802, 17807, 9618, 9619, 1430, 1457, 9651, 1465, 9657, 9665, 17859, 17860, 9688, 1497, 9692, 1507, 9702, 1519, 17904, 17906, 17913, 1531, 9724, 1545, 1557, 9753, 17949, 1582, 17973, 1605, 1611, 1626, 18044, 1661, 18049, 18050, 9860, 18062, 18067, 1688, 18073, 1690, 18082, 1698, 18084, 9898, 9900, 9901, 1710, 9906, 1719, 1721, 9919, 18114, 1735, 1739, 18124, 9937, 18135, 18136, 18141, 1761, 1764, 18151, 18160, 1780, 18182, 18186, 18189, 10017, 10021, 1830, 1831, 1834, 1842, 1859, 10051, 10054, 1862, 1864, 1865, 10058, 18249, 10060, 18256, 18263, 10073, 1883, 1887, 1905, 10098, 10101, 10111, 18304, 1923, 18311, 1931, 10128, 1943, 1951, 10149, 10152, 18352, 10164, 1978, 18373, 10186, 18379, 10194, 10205, 18400, 2017, 2018, 2026, 18417, 2040, 18425, 18433, 10251, 2060, 18444, 18446, 18449, 2076, 10270, 18466, 18467, 18469, 18479, 10289, 18489, 18498, 2115, 2116, 18504, 10314, 18508, 10330, 10332, 10334, 10335, 10345, 18543, 10364, 18560, 18561, 10374, 18568, 18570, 18571, 18572, 10389, 10394, 2202, 2205, 10399, 10404, 18599, 10408, 18604, 18611, 18614, 2242, 18634, 18635, 10449, 18643, 18647, 18653, 2271, 18658, 2277, 2278, 18671, 18674, 18676, 10484, 18677, 10490, 2316, 2319, 2322, 2328, 2329, 18715, 2334, 2336, 2337, 10540, 18735, 10552, 10556, 18751, 18759, 2376, 2380, 2385, 2395, 18784, 2403, 2407, 2411, 10614, 10616, 10627, 18823, 10631, 2443, 2445, 18830, 10648, 18842, 2465, 10664, 10666, 10668, 18871, 10680, 10681, 18877, 18885, 18888, 18891, 18893, 18904, 10714, 2524, 2525, 18909, 10719, 18918, 2545, 2546, 10745, 10770, 18970, 10781, 18975, 2593, 18993, 10804, 18996, 10820, 10826, 10832, 2642, 2644, 10839, 19039, 10851, 10852, 10859, 19055, 10871, 2683, 10876, 2691, 10886, 19082, 10896, 19095, 19101, 19104, 10913, 10919, 10922, 19122, 10938, 2749, 19151, 19155, 2772, 2782, 10975, 10976, 19168, 2793, 10986, 2795, 2794, 10995, 11001, 2813, 2829, 2835, 2838, 11032, 11035, 2845, 11041, 11044, 19239, 11054, 19248, 2864, 19250, 2875, 2876, 19262, 11071, 11078, 2888, 2906, 2914, 2920, 11114, 2932, 11124, 2948, 2952, 11149, 19345, 11167, 11178, 19377, 11190, 11191, 11193, 3004, 11205, 3013, 11208, 11218, 19412, 11223, 19416, 19415, 11230, 3040, 3043, 3049, 11247, 3061, 11260, 3068, 19462, 3082, 11276, 3093, 3095, 11290, 11294, 19493, 19499, 19508, 19510, 19525, 19528, 11339, 19537, 19548, 11356, 3173, 19580, 11389, 19588, 19589, 3207, 11401, 3213, 3214, 11424, 19622, 19634, 11445, 19641, 19644, 19648, 11465, 3281, 11475, 19668, 3285, 19671, 11481, 19675, 19685, 3307, 3311, 19701, 11514, 11515, 3324, 11520, 11522, 3336, 3339, 11545, 19737, 3359, 19750, 3368, 11561, 11562, 19764, 11578, 11588, 19781, 3402, 3403, 19790, 11602, 3413, 3439, 19824, 11640, 3449, 19838, 19855, 3473, 11669, 11684, 11687, 19881, 11694, 11697, 11700, 11720, 11723, 11730, 3544, 11746, 19943, 3560, 11762, 3570, 3580, 19967, 19970, 11789, 19981, 3597, 3602, 19987, 11801, 11802, 3610, 19998, 3619, 11815, 3624, 11830, 11832, 20026, 20028, 3644, 3651, 20036, 11843, 3670, 20055, 3675, 3681, 20065, 3692, 3697, 20084, 11896, 3706, 3707, 3709, 20093, 20095, 11907, 20106, 11919, 20121, 20122, 11931, 11935, 11939, 3748, 11952, 20147, 3778, 3786, 11982, 11985, 20184, 12017, 20209, 20215, 20218, 3837, 12029, 12033, 20227, 20234, 20235, 3850, 3854, 3863, 12056, 12061, 20256, 20264, 3897, 3900, 3906, 20290, 3910, 20295, 3916, 3923, 12115, 20316, 3934, 12132, 3941, 20325, 12137, 20338, 20340, 3956, 12162, 12165, 20357, 12171, 12192, 4002, 20390, 20396, 4012, 20400, 4030, 20414, 20415, 20419, 12228, 20427, 4053, 20438, 12247, 20439, 4058, 12251, 4062, 4072, 20462, 4079, 4081, 4085, 12296, 4105, 12301, 20508, 12317, 20512, 12326, 4148, 12341, 12347, 12355, 12361, 20559, 12369, 12377, 4187, 20579, 12399, 20593, 20604, 4221, 20610, 4233, 4250, 12446, 20642, 20643, 20648, 4266, 20652, 4268, 20656, 20666, 20667, 12489, 4306, 20698, 4315, 4318, 4322, 12519, 12520, 12523, 4332, 20718, 20719, 12529, 12530, 20723, 4352, 20736, 12547, 4359, 20755, 12570, 20763, 12579, 12589, 12591, 12596, 12597, 12599, 20797, 20817, 20823, 20834, 4454, 20852, 4469, 20871, 12682, 20879, 4497, 20887, 12696, 4511, 20896, 20897, 20903, 4525, 12718, 20909, 4528, 12724, 20924, 12734, 20931, 20937, 20939, 4557, 12750, 20952, 12774, 4587, 20971, 12793, 12795, 12807, 21003, 12818, 12828, 12829, 12833, 21026, 4646, 21030, 4651, 12852, 12860, 12861, 12862, 4687, 4699, 21087, 12900, 21103, 12913, 4723, 12921, 4729, 21121, 4748, 4750, 12943, 4759, 12954, 12966, 12967, 21168, 12982, 21177, 21182, 4804, 21193, 21214, 4832, 4834, 21219, 13028, 4837, 21226, 4849, 13044, 13048, 21242, 13051, 4860, 21245, 21244, 4863, 21248, 13060, 21254, 13069, 13071, 4889, 13089, 13095, 4904, 13106, 13110, 21309, 4948, 13141, 21338, 13148, 21342, 13154, 4962, 4969, 4982, 21379, 4996, 21384, 21389, 5006, 21394, 13210, 13223, 5039, 21424, 13232, 5042, 21426, 13237, 21436, 21438, 21443, 13254, 5078, 21470, 5087, 13291, 13295, 21490, 5108, 13302, 5113, 13311, 5120, 13315, 21513, 21527, 13338, 5147, 21532, 21536, 21537, 21548, 5165, 13364, 21556, 13376, 5190, 21575, 5194, 5200, 13396, 13410, 21605, 13416, 5225, 21610, 21611, 5229, 5230, 5232, 21616, 5237, 13436, 5248, 5249, 5251, 13446, 13450, 5270, 13463, 13465, 5275, 13471, 5279, 5284, 21676, 5295, 13488, 13497, 13498, 21692, 5311, 5315, 5319, 21706, 13521, 13543, 21746, 21749, 21754, 5373, 5379, 21764, 21769, 13581, 5390, 21777, 5395, 21784, 13599, 13604, 5413, 21801, 5425, 5435, 21824, 21830, 21832, 13657, 21853, 13662, 13689, 5501, 21888, 13698, 21896, 5514, 5523, 5524, 21917, 13728, 13736, 21932, 21942, 13753, 21956, 21965, 21990, 13803, 5612, 13812, 13813, 5635, 22022, 13836, 22035, 13844, 13846, 5655, 22042, 5669, 22055, 22061, 13876, 5697, 22083, 5703, 5706, 22091, 13904, 22100, 5723, 13921, 13932, 22125, 22132, 13944, 22139, 22140, 5766, 22153, 13968, 13970, 22164, 22172, 13982, 22175, 22177, 22181, 5805, 22190, 5808, 22194, 22195, 5816, 14012, 14036, 14063, 14066, 14075, 22267, 5896, 22290, 14103, 14109, 14118, 14120, 5930, 5937, 5945, 14145, 14147, 22344, 14163, 14169, 5985, 14187, 5997, 5998, 6000, 6003, 6023, 22418, 6040, 6043, 14236, 6045, 22437, 6054, 14251, 6063, 14256, 14264, 6072, 6077, 6078, 14271, 22466, 22473, 6102, 14295, 6103, 22489, 6106, 6119, 14313, 22510, 6132, 14338, 14340, 6172, 22560, 22574, 14384, 6197, 6198, 22589, 14409, 14416, 14417, 14429, 14438, 22633, 14444, 6253, 6269, 6270, 14463, 14473, 6287, 22673, 6290, 14483, 22678, 14489, 22683, 6301, 6311, 14508, 14521, 6338, 6346, 14539, 14546, 22740, 14552, 6368, 14561, 14565, 6376, 22765, 22768, 14578, 6386, 22773, 14589, 14594, 22789, 22790, 14599, 22793, 6409, 14605, 14606, 6415, 22814, 14625, 14632, 14635, 22828, 14643, 6452, 22840, 14650, 6460, 22857, 14670, 6483, 6507, 22892, 14702, 22897, 14713, 14742, 22935, 6551, 14745, 6555, 14751, 6560, 22945, 6562, 22951, 14765, 14768, 22970, 14784, 6593, 6592, 6602, 6616, 6619, 14828, 6636, 14833, 23026, 23037, 23046, 23052, 6669, 14865, 6675, 6678, 6680, 6686, 6693, 6702, 14898, 23090, 23092, 6706, 6710, 23094, 14911, 14912, 14914, 14916, 23120, 23124, 6741, 6745, 6747, 23133, 6752, 6753, 23138, 6758, 23147, 6763, 14964, 23159, 14976, 23179, 23180, 23185, 6802, 23187, 14997, 23189, 14998, 6808, 23193, 6823, 6827, 6828, 15020, 6834, 15027, 6837, 23222, 15036, 6848, 23240, 6871, 15069, 15074, 23267, 15090, 15104, 23297, 23303, 15115, 15119, 23327, 6953, 15145, 23337, 6960, 6963, 15162, 6975, 23364, 6991, 23394, 7024, 15218, 15219, 7036, 15237, 7048, 15248, 15249, 23442, 15258, 7073, 7076, 15269, 7084, 23468, 15285, 7099, 7101, 15298, 23492, 15300, 23503, 7123, 15317, 23512, 23516, 15327, 7139, 7140, 15335, 15336, 23528, 7152, 15349, 23544, 7170, 23561, 15391, 15397, 15399, 7225, 7226, 15417, 15419, 7242, 15436, 7244, 15439, 23636, 7252, 23637, 7256, 15455, 15458, 23650, 7269, 23664, 15473, 15480, 23672, 23676, 7293, 7303, 7304, 23694, 23698, 15508, 23705, 7324, 23712, 7328, 23713, 7331, 15524, 15520, 7340, 7342, 7348, 23736, 15563, 7374, 23760, 15568, 7378, 15577, 23770, 7404, 7416, 15614, 7423, 15617, 15626, 23825, 23828, 7448, 7460, 23847, 15656, 15659, 7475, 15677, 23870, 7489, 15687, 7501, 15700, 7515, 23900, 7520, 23905, 23907, 15716, 7526, 23929, 15742, 15751, 7564, 23949, 15760, 7568, 15765, 7578, 23967, 15777, 23973, 7590, 15783, 7594, 15787, 23981, 15792, 7602, 23990, 7616, 24006, 7622, 7631, 7639, 7642, 7656, 15860, 24053, 24056, 7683, 24073, 15904, 7713, 7712, 15923, 7731, 24119, 15932, 15940, 24135, 15945, 15959, 15962, 15965, 15974, 7799, 24189, 7810, 16011, 16013, 7835, 24223, 7841, 7851, 24243, 24262, 7880, 7896, 24285, 24291, 7910, 24300, 24303, 24304, 16113, 7923, 7925, 24314, 16135, 24328, 7945, 24335, 24336, 16151, 24344, 16159, 7967, 24354, 16165, 24362, 24377, 24383, 8005, 8006, 24393, 16206, 8014, 24401, 8029, 16222, 16223, 16224, 24414, 8043, 24433, 8051, 24461, 16273, 8098, 24486, 8116, 16309, 8122, 16338, 8149, 8156, 16351, 16359, 8174}, 11: {8202, 25, 30, 16415, 8224, 16420, 42, 16430, 8246, 8249, 16450, 8265, 16458, 88, 101, 8293, 117, 8319, 8322, 131, 16519, 8329, 8334, 8335, 151, 16542, 16545, 165, 8360, 8374, 185, 8396, 207, 208, 16592, 16600, 222, 8421, 236, 240, 8434, 246, 8447, 258, 8460, 8465, 16664, 16672, 289, 291, 8492, 301, 300, 8498, 16692, 16706, 8526, 16719, 337, 8530, 8534, 16736, 359, 360, 8552, 363, 372, 375, 16761, 383, 385, 16770, 8579, 8587, 403, 407, 16794, 8604, 417, 16811, 8620, 8623, 16830, 8644, 8650, 16844, 8652, 16845, 463, 16851, 16858, 16859, 8668, 474, 481, 8675, 487, 16871, 16912, 16916, 16918, 16919, 542, 16926, 8736, 16930, 16932, 8747, 16944, 8752, 8756, 8762, 8769, 590, 8782, 16976, 591, 8788, 599, 601, 606, 16993, 8805, 8811, 621, 17008, 17012, 632, 636, 17030, 17033, 668, 17055, 678, 681, 8876, 8878, 8882, 8887, 8891, 703, 8922, 731, 739, 743, 17140, 8954, 8962, 8971, 17170, 17171, 8984, 795, 797, 8995, 8996, 9005, 816, 17204, 9013, 829, 17234, 9043, 9046, 17245, 17256, 885, 9080, 17273, 892, 893, 9097, 9104, 9105, 916, 17303, 17308, 925, 9130, 17326, 17339, 17341, 959, 17358, 981, 983, 9179, 9181, 993, 17387, 17388, 9200, 1011, 17400, 17405, 17406, 1030, 9227, 1040, 1053, 9249, 17443, 1064, 1073, 1074, 17458, 9268, 1076, 9276, 9278, 17473, 1092, 9288, 9298, 1108, 9306, 1115, 17501, 17505, 1123, 17510, 1129, 17524, 17525, 1172, 17564, 1188, 9382, 9403, 1214, 9408, 1227, 9421, 17616, 17617, 17635, 1259, 9452, 9458, 1272, 17656, 9468, 9472, 9483, 17681, 17690, 17698, 9506, 1323, 9526, 17725, 9544, 9547, 1360, 9553, 1366, 9560, 9564, 9565, 1375, 17760, 17762, 1380, 9574, 1391, 17778, 17779, 1405, 9598, 17797, 1414, 17805, 9622, 1436, 9637, 1458, 17846, 9655, 17861, 1480, 9673, 9683, 1492, 9687, 1509, 17900, 9712, 1520, 1522, 17912, 1532, 17919, 9728, 1541, 1542, 9736, 17939, 9750, 17944, 9754, 17947, 17950, 17951, 9760, 1566, 1572, 9767, 1581, 17971, 17975, 17976, 17979, 9792, 9794, 17987, 9797, 1610, 18003, 18004, 9812, 18025, 9836, 18033, 9852, 18064, 1682, 18066, 1686, 1695, 18081, 9890, 1699, 1702, 18087, 18088, 18101, 9910, 9912, 18108, 1724, 1726, 18112, 18116, 18121, 1740, 9935, 1752, 1753, 18139, 18146, 9958, 1768, 9961, 1770, 18156, 9966, 1774, 9986, 18180, 18185, 18187, 9998, 18190, 10005, 18200, 1819, 10013, 1823, 1825, 18215, 1839, 18224, 10039, 10059, 1882, 10079, 18275, 18282, 10093, 10094, 1904, 1910, 18296, 10113, 1924, 10123, 1932, 1941, 1946, 10140, 1954, 1963, 18353, 18357, 1974, 18361, 10172, 10181, 18375, 10191, 10196, 10200, 2016, 18404, 2021, 10220, 2028, 2036, 18443, 2063, 10260, 18454, 10262, 18460, 2079, 10274, 18487, 10299, 2113, 10308, 18506, 10325, 10337, 2154, 10347, 10349, 10355, 18549, 2168, 10363, 10368, 10370, 18563, 2185, 10378, 2195, 18582, 18585, 18595, 2218, 2219, 10414, 10415, 10424, 2233, 2232, 10432, 10433, 10434, 18625, 18633, 10444, 2265, 18651, 2272, 2273, 10472, 2283, 2285, 18684, 18685, 18688, 10497, 2315, 2317, 18702, 10512, 18705, 2327, 10524, 18724, 10533, 10536, 10537, 10539, 2354, 2355, 10553, 2364, 10560, 10562, 10569, 2379, 18766, 2400, 2404, 18788, 2406, 18792, 2412, 10621, 10622, 2433, 2434, 18820, 18828, 10646, 2471, 10677, 2493, 2504, 2511, 10704, 10707, 2532, 10725, 10737, 2548, 2559, 10765, 18962, 18964, 10786, 18981, 10795, 19010, 2634, 10827, 19020, 2651, 10844, 19038, 2657, 2685, 2687, 2688, 19077, 2694, 2702, 10895, 2718, 10911, 2722, 10915, 10917, 2729, 2732, 10933, 19134, 2754, 2760, 19146, 2769, 2774, 19160, 19169, 19173, 19182, 2806, 19193, 19197, 19199, 19201, 19218, 2839, 2843, 11061, 19255, 19259, 19264, 2889, 2890, 19274, 11082, 19282, 19293, 11102, 2911, 11104, 11115, 11117, 19309, 19319, 19331, 11142, 11144, 2957, 2958, 19346, 2973, 19371, 19372, 19388, 3005, 11201, 11204, 19408, 19413, 3033, 19423, 11238, 11240, 19433, 19453, 3070, 19460, 11270, 19464, 3081, 3083, 11277, 3097, 11293, 11302, 3110, 19519, 11327, 11329, 3143, 3149, 11344, 11350, 11352, 3170, 11368, 19563, 19564, 3181, 19567, 3187, 19575, 11384, 19576, 19586, 11406, 3222, 11418, 3226, 3231, 19618, 19620, 3245, 19630, 19632, 3250, 19635, 3254, 3257, 11451, 3259, 3260, 3261, 3283, 11476, 19669, 19670, 3287, 19676, 3303, 19688, 3305, 19689, 11500, 11506, 3325, 11518, 3331, 3335, 11528, 11529, 3342, 19728, 11541, 3353, 3356, 11556, 19753, 19757, 19758, 3377, 3381, 19776, 3396, 3397, 19788, 19789, 11607, 19802, 19804, 19807, 19808, 19809, 19810, 19815, 19816, 3431, 19828, 3446, 3451, 19839, 11653, 3461, 3464, 3470, 11663, 3474, 11668, 3479, 3483, 19873, 3492, 3494, 3499, 11718, 11722, 3535, 11729, 3537, 19925, 3545, 19937, 11755, 11769, 19968, 3590, 19978, 19979, 3599, 11791, 3612, 11808, 3630, 20020, 20031, 3653, 20038, 20041, 20047, 20049, 20050, 20053, 3672, 11865, 11868, 11871, 3685, 3688, 11880, 20074, 11883, 20078, 3698, 11894, 11899, 20099, 3719, 11914, 20120, 3739, 20128, 3758, 20146, 20152, 11964, 3777, 3784, 3798, 11995, 3815, 12007, 12009, 20203, 20213, 3830, 12028, 20221, 3838, 20254, 3873, 20260, 20263, 3887, 3892, 3898, 12092, 12095, 3903, 12099, 3913, 3915, 20301, 20302, 12113, 20309, 12121, 20314, 3935, 12134, 12143, 12145, 3979, 20366, 12177, 12184, 20378, 12188, 20382, 4000, 4016, 12223, 4031, 4032, 12234, 20435, 12245, 4063, 4070, 12264, 12269, 20464, 12273, 12274, 12294, 20488, 20493, 4111, 20503, 12329, 12334, 20528, 20531, 12349, 20542, 12359, 4176, 4179, 4181, 12381, 20583, 12402, 20595, 20605, 12438, 4247, 12440, 12441, 12442, 20645, 4261, 12458, 4267, 12460, 12464, 4276, 12481, 20681, 4300, 12497, 12506, 12508, 20717, 4337, 12540, 4354, 4356, 20743, 20745, 4367, 20757, 20759, 12573, 20770, 12580, 4391, 12583, 4393, 4392, 4396, 20784, 12601, 12608, 12613, 12620, 20816, 4433, 4432, 12628, 4437, 12646, 12652, 12659, 12663, 12689, 4505, 20891, 20893, 4509, 12704, 4514, 4522, 20917, 12726, 20918, 4537, 20925, 20926, 12735, 4550, 20934, 12747, 12752, 12785, 20979, 20989, 4619, 21004, 21008, 12820, 21013, 21014, 12823, 4629, 12830, 12831, 21024, 4644, 12847, 4667, 4671, 12868, 4679, 12874, 4684, 21076, 12889, 4698, 4713, 12910, 21111, 21112, 12927, 4739, 12933, 4743, 4755, 21143, 21148, 12957, 12974, 21170, 21178, 21179, 12988, 4797, 21180, 12991, 21181, 21185, 21186, 21187, 12990, 4808, 4813, 21205, 13016, 4830, 13029, 4838, 4842, 13035, 4845, 13041, 13046, 21238, 4855, 13047, 21247, 13055, 4867, 21252, 13061, 4870, 4871, 21262, 4882, 13085, 21278, 21280, 13090, 4902, 4906, 4909, 4911, 4913, 4916, 21304, 4925, 4927, 4928, 4931, 21318, 4937, 13130, 4938, 4940, 21324, 4945, 4955, 4956, 21345, 4961, 21371, 13186, 13191, 13204, 5019, 13212, 13213, 5024, 21415, 5032, 13225, 5033, 13227, 21421, 5048, 13242, 13250, 13257, 5066, 13259, 5070, 5071, 21462, 21464, 5088, 21477, 13289, 21483, 5101, 21493, 21496, 21499, 13313, 21508, 13316, 5127, 13322, 13323, 5139, 21530, 21538, 13347, 21542, 13351, 5192, 5198, 5224, 13429, 5245, 13449, 21643, 5288, 13485, 5303, 5306, 13499, 13502, 13503, 21694, 5313, 21697, 21699, 13514, 21713, 21720, 21725, 5345, 21734, 21737, 21751, 13566, 21760, 5376, 21762, 21763, 13575, 21774, 13585, 13591, 5412, 13615, 5426, 21812, 21814, 5431, 13623, 21820, 5454, 13648, 5462, 21849, 21852, 13663, 5471, 5474, 13668, 5478, 5483, 13684, 5494, 13687, 5495, 21883, 5505, 5510, 21904, 21908, 13717, 5547, 21938, 13751, 13755, 13756, 13762, 5580, 13775, 5584, 13776, 5586, 21967, 5585, 21973, 5594, 21978, 13789, 21982, 21987, 5620, 5623, 13815, 22007, 5629, 5630, 22015, 22020, 13828, 22024, 5640, 13841, 5658, 5662, 22060, 13878, 5692, 5696, 5700, 22086, 13898, 13901, 22103, 5720, 22105, 13914, 5726, 13922, 13923, 13934, 22129, 13947, 22141, 5764, 5765, 22151, 22152, 22155, 13965, 22162, 5779, 13986, 13988, 22182, 22186, 22187, 5804, 14006, 5817, 14011, 22204, 22205, 5823, 5826, 22211, 5831, 14026, 14030, 22227, 5844, 5849, 5851, 14046, 5855, 5873, 14070, 5889, 14085, 14087, 5898, 5899, 14092, 5903, 5906, 5913, 5915, 5920, 22305, 22307, 5926, 14128, 22324, 14149, 22341, 5964, 14161, 5977, 5978, 5979, 14175, 22376, 5996, 14189, 22386, 22388, 22392, 6017, 22405, 14214, 6024, 22409, 14218, 6047, 22431, 22432, 6049, 22434, 22451, 6073, 6081, 6086, 22478, 22481, 14296, 14297, 6109, 14305, 22509, 22520, 14328, 6143, 14341, 6153, 6170, 6184, 22579, 14388, 22590, 6215, 14411, 22607, 6225, 6228, 14428, 22629, 14442, 22637, 6275, 14467, 22661, 22665, 14475, 22674, 14496, 6308, 6317, 14510, 22707, 22708, 6324, 14518, 14520, 6345, 22731, 6350, 14548, 14550, 22747, 22764, 6381, 6383, 22775, 14584, 22780, 14590, 6424, 22813, 14626, 14652, 22846, 14655, 6469, 22855, 14665, 6476, 22868, 14679, 22878, 6496, 6497, 14704, 14709, 22904, 22909, 6527, 22913, 6539, 14737, 22931, 22933, 6550, 22936, 14746, 6556, 6557, 6568, 22952, 6574, 6579, 22973, 6591, 22989, 22999, 6622, 23012, 6629, 14825, 6637, 14836, 6644, 23040, 23061, 14873, 23068, 6688, 14885, 6697, 23083, 23086, 6705, 14903, 23107, 6731, 23115, 14931, 23125, 14938, 23132, 14944, 14952, 6770, 6772, 23160, 14969, 14972, 23165, 14978, 6791, 23177, 14990, 6801, 6804, 15000, 15001, 6835, 15030, 15062, 15082, 6895, 15088, 23285, 15108, 23309, 15121, 23314, 6964, 23355, 6972, 15168, 6983, 6986, 6987, 15181, 15182, 23376, 6993, 15186, 23379, 23385, 15193, 7012, 23396, 15205, 23404, 23407, 7029, 7041, 7044, 15240, 15243, 23435, 7058, 15251, 7062, 15255, 7066, 7070, 15268, 23470, 7086, 15281, 7089, 7097, 15297, 23495, 23500, 23508, 7129, 15329, 7145, 7150, 15342, 23541, 7162, 15375, 7183, 7199, 23584, 15398, 7215, 7217, 23604, 15416, 7234, 7245, 15437, 7246, 7249, 15442, 15448, 23644, 7267, 7271, 7274, 7281, 7287, 15479, 15481, 23683, 23684, 23690, 7308, 7309, 23700, 23706, 7341, 23730, 15545, 7366, 7369, 23754, 7371, 7372, 23763, 7380, 23765, 15574, 23768, 23775, 7392, 7395, 15591, 15592, 15599, 7424, 7429, 15632, 15636, 7451, 7453, 23845, 23856, 15668, 15672, 15674, 7486, 7493, 7495, 23884, 7504, 23891, 23896, 23898, 15723, 15725, 7534, 7539, 23928, 15738, 7556, 7558, 7561, 7567, 23962, 7582, 7589, 7591, 7592, 15796, 15798, 24001, 15813, 7627, 24023, 15836, 24030, 24037, 7653, 15849, 15856, 24055, 15867, 7680, 7686, 7689, 24079, 15891, 7704, 7707, 15905, 7716, 7717, 15916, 24109, 7729, 15921, 7733, 15943, 7751, 15947, 7755, 7761, 7765, 24150, 15963, 15970, 24163, 15972, 7782, 15983, 15989, 7802, 7804, 15998, 16000, 24193, 24199, 24200, 16009, 7820, 16015, 7826, 16022, 24220, 16045, 24238, 16059, 24255, 24257, 24266, 24279, 24281, 7900, 16097, 24292, 16100, 16105, 7926, 16119, 16129, 16137, 24330, 7946, 16140, 16147, 24359, 7976, 7995, 24382, 7999, 24386, 8016, 16209, 16210, 8019, 24404, 8033, 24422, 16235, 16238, 16243, 16244, 16245, 16248, 16258, 8068, 24463, 24469, 16282, 16297, 8106, 24492, 24494, 8117, 24505, 24508, 16322, 8150, 16344, 8155, 16349, 8162, 16358, 16361, 16363, 8171}, 12: {2, 8197, 16389, 16394, 8208, 16402, 8212, 27, 28, 8220, 8227, 8232, 8240, 58, 16443, 16451, 69, 75, 8271, 81, 8282, 16476, 95, 16479, 16487, 110, 16497, 8316, 134, 140, 8344, 16540, 8354, 8357, 8365, 8367, 8370, 16562, 8382, 16575, 8387, 202, 203, 16589, 16602, 8414, 16624, 16625, 254, 8450, 259, 16650, 8461, 270, 16655, 290, 295, 298, 16689, 8511, 8527, 335, 346, 353, 16738, 358, 16743, 16746, 8554, 370, 377, 16762, 378, 8576, 16768, 389, 397, 8591, 16788, 8597, 8602, 16798, 419, 420, 8629, 8633, 448, 454, 455, 16843, 459, 466, 16863, 16864, 479, 8673, 16867, 8686, 505, 16897, 16900, 8708, 518, 8709, 523, 527, 16927, 16936, 16937, 559, 8778, 16979, 8791, 8794, 16994, 17005, 626, 633, 17029, 646, 647, 17041, 658, 670, 17056, 17057, 675, 687, 8881, 699, 700, 8900, 8903, 8912, 8913, 17112, 8921, 8927, 736, 737, 17122, 8932, 742, 8935, 8936, 8943, 8950, 17144, 8952, 17167, 9001, 17199, 9010, 818, 834, 9027, 844, 9047, 9050, 9052, 17252, 869, 9062, 17263, 9085, 9090, 905, 910, 9117, 926, 932, 937, 948, 951, 9170, 9171, 17376, 9189, 9193, 1002, 17385, 1005, 1008, 1013, 9206, 17403, 17408, 1027, 17416, 9226, 9233, 9247, 9248, 9254, 17459, 9267, 9274, 1096, 17486, 17487, 17489, 1105, 17491, 1109, 9301, 1112, 1119, 9312, 9316, 17509, 9320, 9324, 1142, 9336, 17539, 17540, 1161, 9355, 17551, 9361, 9368, 9378, 17573, 9385, 17578, 1193, 9387, 1196, 1202, 1203, 9400, 9404, 9416, 9418, 9432, 17630, 9445, 9446, 17638, 17650, 17651, 9460, 1279, 1285, 9486, 17679, 9492, 17685, 1307, 1308, 1313, 17701, 17703, 1321, 17708, 1331, 1334, 17718, 9530, 1349, 1353, 1359, 17753, 9571, 1384, 9579, 9580, 1394, 17785, 9604, 9606, 9609, 9610, 1423, 17809, 1434, 17820, 1438, 9632, 17841, 9679, 9680, 17878, 9691, 9693, 9706, 1514, 17903, 17909, 9720, 9722, 9729, 9734, 17927, 17936, 17942, 9751, 1560, 17948, 1568, 9764, 17963, 9775, 17969, 9782, 17978, 9787, 9793, 17998, 9806, 1618, 1619, 1624, 1628, 9825, 1636, 1637, 9834, 1643, 1647, 9840, 1650, 18037, 9846, 9876, 1689, 9883, 1692, 1693, 9903, 1712, 9911, 18106, 18109, 1730, 1731, 9933, 18126, 18125, 18129, 18143, 9959, 18153, 1776, 9972, 18165, 18169, 1787, 9979, 1796, 1799, 9991, 1806, 18201, 1818, 18210, 10020, 18213, 1856, 18245, 1866, 1869, 1875, 18261, 10083, 10084, 1894, 10087, 10096, 1908, 18305, 10117, 18317, 18318, 18325, 10144, 18338, 18339, 1956, 10157, 18350, 10158, 10161, 1976, 10168, 1977, 1979, 10169, 18368, 18378, 18388, 2008, 10206, 10207, 18399, 10213, 10215, 2024, 2038, 2045, 2046, 10246, 2059, 2061, 18452, 2071, 10267, 2080, 10273, 2084, 18474, 18485, 18500, 18503, 10311, 2121, 10317, 18513, 2133, 18521, 18533, 18544, 10356, 10357, 10359, 18553, 2170, 2174, 2178, 18569, 2192, 10388, 10390, 10395, 2213, 2220, 18610, 10423, 18620, 2238, 2241, 2263, 10464, 18662, 18667, 10483, 2295, 18686, 2306, 2308, 18706, 10516, 10519, 10528, 2350, 2351, 10548, 18748, 18764, 10578, 10583, 10588, 10595, 10605, 10608, 2419, 10618, 18812, 18814, 10624, 2442, 10638, 18835, 10647, 10653, 18845, 10655, 10656, 18848, 10658, 18852, 18853, 10671, 18866, 18867, 10675, 2488, 10685, 10686, 2498, 10690, 2501, 10695, 10697, 10701, 10702, 10716, 2540, 18924, 10739, 18932, 2549, 2557, 18946, 10759, 2567, 2571, 2578, 18972, 10785, 2597, 18985, 2601, 2611, 2613, 10808, 19002, 10812, 10816, 10825, 19021, 10838, 10840, 19036, 19044, 19045, 19047, 2668, 2669, 10861, 19054, 10869, 2679, 19094, 10906, 19099, 19100, 2719, 19103, 19105, 19108, 2726, 10921, 10924, 10926, 10936, 2746, 2763, 19154, 10963, 2780, 10982, 10984, 2792, 19187, 2816, 19200, 19202, 2822, 19211, 19214, 19215, 19217, 2837, 11037, 11039, 19235, 11046, 2854, 2857, 11057, 11066, 11067, 2874, 11073, 11074, 2882, 19268, 2885, 19278, 2898, 19285, 11094, 2904, 19298, 11112, 19308, 11118, 19316, 11128, 19350, 19363, 11180, 11181, 19374, 11187, 2996, 11200, 11202, 19406, 11216, 3024, 3030, 19414, 11222, 3035, 19421, 11242, 3051, 3054, 3056, 3062, 19456, 3073, 3089, 3092, 3098, 19490, 19503, 11313, 3121, 11320, 3133, 11326, 19527, 3145, 11338, 19532, 19538, 3161, 19556, 19559, 3176, 11375, 11381, 3198, 3199, 19584, 11394, 11396, 3204, 3211, 3215, 11410, 3234, 3240, 11436, 11439, 11441, 19637, 19642, 19645, 19649, 3273, 11471, 3282, 11477, 19679, 11496, 3306, 19692, 3308, 3322, 11533, 3344, 3347, 3350, 11547, 19742, 3358, 19744, 11555, 19747, 11560, 3371, 11573, 3384, 3406, 11605, 19805, 11613, 19814, 19818, 19821, 3445, 19831, 11642, 19850, 11670, 11673, 19866, 3491, 11683, 19877, 3493, 11689, 11701, 11703, 3513, 11707, 19905, 11714, 3527, 3534, 3536, 11738, 19934, 19944, 3563, 19947, 3565, 11761, 3569, 3583, 11775, 11777, 11781, 19993, 11803, 19995, 20006, 3643, 20030, 20034, 11845, 11846, 3656, 20042, 20044, 3686, 20075, 11886, 11897, 3710, 3713, 11921, 3730, 3734, 20119, 11930, 20123, 20130, 20131, 11942, 20135, 11945, 3759, 20153, 20154, 20158, 20162, 20164, 3781, 3797, 20189, 20194, 20197, 3827, 3834, 20224, 20229, 12043, 12046, 20244, 3861, 20251, 12066, 3880, 12085, 20289, 12106, 12111, 3920, 20313, 20318, 20330, 20342, 3960, 20346, 12158, 3973, 20360, 12169, 20362, 3981, 20367, 20401, 4027, 4028, 20421, 12238, 20442, 20443, 12252, 4060, 20448, 4069, 20454, 12275, 20470, 12281, 20476, 12284, 20478, 4095, 20485, 12302, 12304, 4120, 4126, 12327, 4138, 4140, 20543, 4172, 12368, 20561, 12382, 20578, 4204, 20591, 4215, 4216, 20612, 12423, 4239, 4273, 20662, 20664, 20668, 20673, 4296, 12491, 20685, 12500, 20695, 20700, 12515, 4326, 20728, 4364, 12556, 12559, 20756, 4380, 20766, 12575, 12578, 12585, 12593, 20787, 20791, 20792, 20806, 20814, 12623, 4435, 20821, 20836, 4455, 4460, 12653, 4461, 4465, 20854, 4478, 4479, 4481, 20870, 4496, 20885, 12713, 4546, 12755, 12760, 4569, 4576, 20962, 20969, 20972, 20973, 12792, 4601, 20994, 12812, 21019, 4637, 4647, 12842, 12844, 12845, 12855, 21053, 21057, 12866, 21059, 12867, 21073, 21078, 4702, 12898, 21097, 4714, 21104, 4722, 12918, 12928, 12931, 12937, 4746, 4749, 4757, 4764, 21150, 4769, 21153, 21159, 12972, 12977, 4786, 21171, 12978, 21175, 21176, 4805, 4810, 13003, 13002, 4814, 21201, 21210, 13032, 21230, 4847, 13040, 21233, 21231, 4856, 21246, 4873, 4877, 21263, 13075, 4886, 13082, 4897, 4898, 21282, 13096, 13097, 13098, 21305, 13118, 4926, 13127, 4946, 13140, 13142, 4953, 21339, 4968, 21364, 13173, 21373, 5008, 5012, 13221, 13224, 5034, 21418, 5036, 5035, 13234, 21430, 5049, 5051, 13246, 5059, 21455, 13263, 5074, 5076, 21468, 13279, 21478, 13288, 21487, 5104, 5106, 21491, 5109, 5112, 21501, 5118, 5119, 5123, 5126, 5130, 13326, 5137, 13339, 13342, 5151, 5158, 13354, 21546, 21550, 21560, 13369, 21564, 5184, 5195, 21587, 13397, 13401, 21598, 5215, 5219, 5226, 13418, 21625, 5241, 21628, 21639, 13454, 21656, 21658, 21662, 21666, 13479, 13480, 21683, 21691, 21696, 13513, 5321, 13515, 13519, 13522, 21718, 5337, 21723, 5350, 13545, 5355, 13552, 5381, 13574, 21776, 5394, 5402, 13598, 5407, 13601, 5414, 5417, 5423, 21811, 21826, 5444, 5449, 21845, 5464, 5475, 21861, 13676, 21871, 21876, 13690, 5500, 13695, 5509, 13702, 21895, 5532, 5533, 21919, 5548, 21934, 5551, 5550, 13748, 5560, 5565, 5569, 21958, 5583, 13785, 13788, 21985, 13799, 5610, 21998, 13806, 13808, 22005, 13814, 22013, 22014, 22019, 22023, 22028, 5650, 22037, 13854, 5674, 13871, 22064, 13877, 22074, 13884, 13885, 13887, 5705, 22114, 22124, 5746, 22130, 22131, 13949, 13951, 5762, 22147, 22149, 13963, 5773, 5778, 13975, 13976, 22174, 13987, 5796, 13990, 14003, 5812, 14009, 5818, 5822, 5824, 22224, 5858, 5863, 14062, 5876, 14081, 5891, 5894, 14088, 22283, 14097, 22291, 14105, 5924, 22313, 22316, 22321, 5940, 14133, 14134, 22332, 22334, 14151, 14153, 14158, 5968, 5973, 22372, 14181, 14184, 14193, 6001, 14203, 22397, 6015, 14210, 14216, 6025, 14219, 22413, 22415, 14226, 14228, 14232, 6041, 6044, 22455, 14266, 22459, 22461, 14270, 14282, 6094, 6099, 6100, 6107, 22495, 14309, 6121, 14321, 6130, 22513, 14324, 6136, 6142, 6151, 14345, 6154, 22542, 6159, 14359, 6169, 14363, 22558, 14371, 14375, 22567, 6190, 14386, 22584, 14399, 6216, 22608, 14422, 22620, 6244, 14443, 6257, 6263, 14456, 14458, 6272, 14466, 14476, 14480, 14486, 6294, 22679, 22685, 22688, 14497, 22696, 14513, 22713, 6330, 14522, 22715, 6334, 14527, 22725, 14535, 14541, 6353, 22745, 22746, 22753, 6371, 14570, 14576, 22771, 14586, 6398, 14591, 14593, 6410, 6420, 6426, 22810, 6434, 22821, 6438, 22823, 14631, 6437, 14636, 22829, 6451, 22842, 6465, 22849, 22854, 14662, 22856, 14669, 6477, 22864, 14673, 14678, 22871, 6493, 14686, 14689, 22883, 6500, 6502, 14707, 6519, 14712, 6537, 22922, 6547, 6559, 14755, 22948, 6569, 14761, 14771, 6585, 22991, 6609, 22995, 14803, 22998, 23001, 14810, 23005, 14813, 23008, 6625, 14819, 6628, 14822, 14826, 23018, 23023, 14835, 23028, 6649, 6651, 14845, 14847, 23045, 6670, 6682, 23072, 23081, 6709, 23097, 6714, 6715, 6713, 14905, 23098, 14910, 6728, 6737, 23122, 23126, 23129, 23130, 23131, 14941, 6750, 14959, 6774, 6776, 6779, 14975, 14989, 14991, 15006, 15009, 23204, 23206, 23212, 23215, 15029, 6838, 15034, 6843, 15037, 23235, 6877, 23263, 6883, 6886, 15091, 6910, 23305, 23317, 15125, 15138, 15139, 15142, 23346, 23356, 6974, 15173, 6982, 15180, 15183, 23378, 15190, 7001, 23390, 15211, 15213, 7026, 23414, 7033, 7037, 23432, 23434, 23436, 15246, 23440, 15250, 23443, 15259, 15260, 23452, 15263, 23457, 23461, 7083, 15277, 15292, 7103, 15296, 15295, 15303, 7114, 15307, 15314, 7124, 15321, 23515, 7143, 23529, 15339, 7153, 7157, 15365, 15370, 23568, 23570, 7194, 15396, 15402, 7218, 23606, 7222, 7229, 7236, 23623, 23627, 23628, 15440, 15441, 7255, 15456, 23665, 23669, 23673, 15489, 7298, 15496, 15506, 23699, 7314, 7316, 7326, 23715, 23717, 23723, 15538, 23733, 23740, 7362, 15555, 15561, 15570, 7381, 15575, 15578, 7387, 7390, 15584, 7393, 15588, 7399, 23790, 7409, 7411, 23801, 23803, 23810, 23814, 15624, 23817, 7432, 23822, 7442, 7445, 7446, 15643, 7468, 23852, 7472, 23863, 15671, 15688, 7500, 15696, 23892, 23893, 15701, 15704, 15715, 23930, 7549, 7559, 23945, 15759, 23954, 7574, 7587, 7593, 7604, 23988, 23999, 15810, 24016, 7640, 15837, 24032, 24034, 24041, 15859, 15875, 24068, 7687, 7693, 7697, 15890, 7701, 24089, 15906, 24100, 15912, 15922, 24120, 15937, 24136, 7752, 24137, 24142, 7767, 24152, 15960, 24157, 24158, 7784, 24182, 7806, 7812, 24201, 7825, 24214, 16023, 24219, 7837, 16029, 16034, 24231, 7849, 24244, 24256, 16065, 7879, 24274, 7891, 24276, 7901, 7903, 7907, 16103, 24296, 16106, 7919, 16115, 24309, 7929, 7931, 16125, 7933, 16143, 7954, 24345, 16156, 16162, 24357, 24361, 7984, 16185, 16187, 16189, 16195, 24387, 8013, 16208, 24403, 8021, 8025, 8026, 8038, 8042, 8054, 8057, 24444, 16255, 16260, 16264, 16271, 24464, 16280, 24475, 16284, 24477, 24482, 16291, 24483, 16296, 8110, 16304, 8121, 24507, 8128, 16327, 16329, 8142, 8143, 8151, 16347, 8168}, 13: {8192, 8203, 16400, 18, 24, 16413, 16417, 16434, 8243, 52, 16438, 8248, 8250, 59, 16445, 8261, 8267, 84, 16469, 87, 8285, 16480, 100, 8300, 109, 121, 16523, 8336, 8346, 160, 8361, 177, 16579, 8390, 205, 8402, 16594, 8403, 213, 16609, 8418, 16612, 231, 8426, 234, 8431, 8435, 8436, 16643, 262, 8455, 8456, 8458, 16653, 8462, 16658, 16674, 8482, 16677, 16678, 16679, 296, 302, 303, 16686, 8501, 311, 8504, 8514, 325, 16710, 8520, 329, 16714, 8529, 8539, 8540, 16731, 356, 16741, 16740, 8551, 16744, 8553, 369, 8563, 373, 8569, 16771, 390, 8589, 401, 411, 8615, 434, 443, 8640, 16839, 16841, 8660, 469, 8662, 471, 8663, 472, 8667, 8670, 16865, 8677, 490, 496, 16881, 501, 16885, 16887, 8701, 8707, 8710, 522, 16909, 530, 535, 16920, 538, 8748, 561, 16945, 565, 16957, 574, 575, 576, 16961, 584, 8780, 8787, 16980, 8789, 598, 16984, 16988, 635, 643, 655, 17051, 8860, 17058, 17066, 17078, 8902, 17095, 8906, 17100, 722, 8916, 17109, 732, 8929, 8934, 747, 8940, 749, 754, 8948, 17142, 759, 8965, 773, 775, 794, 17179, 798, 8997, 17190, 17211, 9021, 17215, 831, 836, 9035, 17229, 845, 847, 9054, 9056, 17251, 17258, 17259, 17278, 17279, 902, 17291, 912, 9106, 17316, 9138, 947, 954, 17342, 970, 973, 17362, 9172, 985, 986, 9183, 9197, 9202, 9205, 1014, 1020, 1023, 1033, 1036, 17422, 9231, 9238, 17430, 9246, 1054, 9257, 17451, 1071, 1082, 9303, 1121, 9319, 1133, 9326, 1136, 9329, 9342, 1155, 9347, 17555, 1171, 9367, 9372, 9393, 17589, 17593, 17606, 9415, 17615, 17618, 1240, 1248, 1251, 1252, 17637, 9451, 1264, 1265, 1276, 9470, 17664, 1281, 9473, 17673, 1294, 1296, 1298, 9501, 1311, 9522, 17715, 1339, 9540, 17738, 1357, 1370, 17774, 17781, 9592, 9596, 17792, 9600, 17794, 1413, 1422, 9617, 17815, 9624, 9625, 9628, 1446, 9643, 17845, 9664, 17864, 1485, 9685, 1498, 17883, 1500, 17885, 1505, 1506, 1508, 9701, 17893, 17895, 17905, 17908, 9725, 9727, 1538, 17923, 17929, 1549, 9745, 1555, 1565, 1586, 17972, 17985, 17988, 9808, 18012, 18015, 9824, 1642, 18034, 9843, 18035, 1663, 9873, 9896, 18090, 18092, 18093, 1708, 1714, 9907, 1728, 18122, 9939, 18134, 1756, 9949, 1760, 18144, 18148, 9957, 1765, 1767, 1769, 18159, 18162, 1784, 1801, 10006, 1816, 10009, 10010, 18204, 10015, 1827, 1829, 1832, 18217, 18226, 1848, 10046, 10048, 18241, 18243, 18254, 18264, 10076, 1890, 18274, 18290, 10105, 18301, 18302, 1918, 1921, 10116, 10122, 1936, 18322, 18323, 10132, 10137, 10142, 10145, 1955, 18345, 10155, 1968, 1970, 1972, 10167, 10175, 18370, 10178, 10179, 1990, 18374, 10185, 10187, 10193, 2005, 10197, 18393, 10204, 18397, 18398, 2015, 10210, 2019, 2027, 10222, 2030, 10225, 18427, 2047, 18432, 2054, 2056, 2057, 2070, 18464, 18481, 2101, 2102, 2106, 2108, 18493, 10302, 10303, 10310, 18511, 18512, 10321, 2130, 2135, 10336, 18548, 10380, 2189, 10382, 2193, 10385, 18579, 2197, 10403, 2216, 18605, 18609, 2234, 2239, 10435, 2247, 2249, 2250, 10445, 10448, 2257, 2275, 2276, 10475, 2289, 10482, 10481, 2294, 10489, 2298, 2297, 10495, 18697, 10506, 18698, 10517, 2333, 2349, 18741, 2358, 18757, 10571, 10572, 10575, 2387, 18780, 10602, 10604, 18803, 18808, 2431, 10633, 2441, 2449, 10652, 10667, 2482, 2484, 2486, 10682, 18876, 18880, 10691, 2502, 18886, 10703, 18896, 18900, 18905, 10720, 10723, 18916, 10728, 18923, 2542, 2543, 18926, 10742, 10747, 10751, 10753, 2564, 10756, 2566, 18956, 10775, 10777, 18969, 2590, 18979, 2600, 2606, 2607, 10811, 19009, 10818, 10819, 19019, 2635, 10831, 19032, 10847, 10856, 2677, 10874, 10875, 19067, 10878, 2686, 10879, 19076, 10885, 10889, 2697, 10891, 2707, 10920, 10931, 19138, 10947, 19152, 19163, 2781, 2791, 19175, 2799, 10994, 11000, 19198, 11006, 2818, 11011, 2820, 11018, 19222, 19229, 2847, 19231, 2856, 2865, 11059, 2868, 2872, 19267, 2893, 2896, 11089, 11091, 19283, 19288, 2905, 11103, 19311, 19313, 11126, 2936, 19321, 11129, 19324, 11136, 11138, 19335, 2968, 11164, 2986, 11186, 2998, 3001, 3002, 11206, 19399, 11211, 3019, 19409, 3028, 3032, 19417, 19422, 11233, 3044, 3046, 3050, 11256, 19450, 11261, 19459, 3080, 19468, 3085, 11279, 3088, 3090, 19480, 3101, 11300, 3109, 3120, 3122, 11321, 3135, 3136, 19523, 11333, 3147, 3148, 3155, 19541, 19544, 11355, 3165, 19562, 11371, 11370, 19566, 11382, 3190, 11385, 11386, 11387, 19583, 3205, 19592, 19598, 11407, 19600, 19602, 19614, 3230, 3236, 3246, 19638, 11473, 11474, 11479, 19677, 11486, 11495, 3304, 11498, 19691, 11502, 3318, 11519, 11521, 11523, 19716, 3332, 11527, 3340, 19731, 11540, 19732, 19740, 11554, 19755, 11569, 19765, 3382, 11579, 3392, 11585, 11586, 19780, 19782, 11592, 11594, 3410, 19797, 19801, 19803, 19806, 3425, 11618, 19811, 3426, 11629, 19823, 11634, 19827, 11646, 3457, 3459, 3463, 11660, 11661, 11666, 19868, 19869, 19896, 3514, 11710, 19906, 3523, 3525, 19913, 19916, 3542, 3550, 19936, 19938, 19948, 11756, 3567, 11760, 3575, 3579, 11773, 19973, 11799, 3613, 3616, 3618, 3642, 11836, 3645, 11841, 20035, 11851, 3660, 3661, 20046, 11888, 11891, 20085, 3708, 11901, 11908, 11913, 3732, 3735, 11937, 3752, 3757, 20144, 3761, 11954, 3764, 11959, 3774, 3775, 11968, 20160, 11971, 3779, 3780, 11972, 20171, 11991, 3816, 12012, 3820, 3822, 20206, 3826, 12032, 20225, 3843, 3853, 20243, 20246, 12054, 12075, 3886, 12082, 20275, 12089, 12090, 20283, 3901, 20293, 12112, 3924, 3926, 20319, 3942, 20328, 3945, 3948, 3952, 3963, 3966, 20350, 3969, 12166, 20363, 20369, 20373, 12185, 3997, 12194, 20387, 12196, 4006, 20391, 20392, 12204, 20398, 12237, 20452, 4075, 4076, 4084, 12277, 20474, 12288, 4097, 4104, 12306, 20498, 12312, 20506, 4122, 4127, 20511, 12322, 4137, 12340, 4152, 12352, 4161, 4164, 4170, 20558, 4182, 20569, 12396, 4208, 20598, 20603, 4226, 4227, 4228, 4229, 20613, 4231, 4230, 4241, 20631, 20640, 4256, 12456, 20653, 20654, 20660, 4311, 4313, 20699, 12509, 12511, 4325, 4331, 4335, 20720, 4336, 4338, 4341, 12538, 12539, 20739, 12548, 12550, 12553, 4362, 12555, 4366, 12558, 12561, 4377, 4388, 12588, 20800, 4417, 12610, 4419, 12614, 20815, 20826, 4442, 4444, 12636, 20839, 12648, 4457, 4458, 20850, 12660, 4474, 4476, 20862, 20864, 4482, 12677, 20883, 4503, 4513, 12706, 20901, 20908, 12716, 12727, 4538, 20940, 4559, 20948, 12765, 20959, 20974, 4597, 12791, 4604, 4615, 21002, 12819, 12832, 4645, 21035, 4661, 12854, 21048, 4673, 21062, 4682, 4695, 21081, 21088, 21092, 4710, 4716, 4721, 12915, 4732, 4737, 4738, 21123, 21131, 21134, 12946, 4762, 4794, 12987, 4806, 21191, 4812, 21203, 4821, 13014, 4823, 4826, 13019, 13022, 21221, 21224, 4841, 21225, 13054, 4866, 4880, 21264, 21272, 21274, 21275, 21276, 13092, 21286, 21288, 4905, 21294, 21301, 21307, 13116, 13119, 13121, 13126, 4935, 13134, 21333, 13159, 21354, 4974, 21361, 13176, 21376, 5000, 21388, 5007, 13206, 5017, 5023, 5029, 21417, 13241, 21433, 13244, 13248, 5060, 21449, 5069, 21454, 21457, 13268, 13281, 5093, 5099, 13296, 5116, 21505, 21510, 13330, 5142, 5150, 5153, 5154, 13349, 21544, 21552, 21555, 21559, 21567, 5185, 13380, 13385, 21580, 13391, 5201, 5218, 13412, 13423, 21621, 21624, 21629, 13448, 5260, 5267, 13464, 21660, 21667, 13476, 13478, 5300, 21687, 5304, 21689, 5316, 5320, 13523, 21721, 5344, 21729, 21731, 13544, 13554, 5365, 21755, 21759, 5375, 13568, 5380, 5386, 13582, 5391, 5396, 13589, 5404, 5408, 5411, 5416, 13611, 13630, 13632, 21827, 13638, 21831, 21834, 13643, 21838, 21839, 5459, 13653, 13654, 5463, 5465, 13669, 5479, 21864, 21866, 13678, 21882, 21891, 21893, 5512, 5513, 13706, 5522, 21914, 13723, 21926, 13742, 5553, 21944, 21948, 21949, 5571, 5577, 21961, 5592, 13792, 13797, 21992, 21999, 5617, 13809, 5633, 5641, 5642, 22027, 5645, 13838, 22034, 5653, 13849, 5661, 22045, 13856, 22054, 13864, 5693, 13895, 22088, 22090, 22098, 22101, 5722, 13917, 13920, 22113, 22123, 22128, 13936, 22142, 13955, 13958, 13960, 22165, 5782, 5784, 5799, 5809, 5811, 22196, 14013, 14014, 14016, 22209, 5829, 14023, 22220, 22221, 14040, 14047, 5859, 22250, 5872, 22259, 14079, 5907, 14101, 14104, 5916, 22302, 5938, 5941, 14144, 22347, 5965, 22351, 5969, 22357, 5974, 22361, 22367, 5987, 5988, 5992, 6010, 22396, 22402, 6022, 14215, 6030, 14229, 6039, 22433, 14243, 14249, 14262, 14268, 22468, 22469, 22476, 22479, 14294, 6110, 6118, 22507, 6127, 22512, 14331, 22526, 14337, 22530, 22537, 22540, 6158, 14352, 14354, 14355, 22559, 22561, 22563, 22564, 22565, 14376, 14382, 14390, 14396, 22597, 6219, 22604, 14414, 14415, 14421, 22613, 22616, 6240, 14447, 22642, 6260, 14455, 6264, 14457, 6267, 6278, 22676, 6302, 22690, 14499, 6312, 14516, 6329, 6335, 6336, 14531, 6340, 22727, 22736, 6352, 6359, 22759, 14572, 6380, 6394, 6402, 22791, 14604, 14609, 6418, 6423, 6428, 22819, 14640, 22839, 6467, 14660, 22861, 6479, 22867, 6485, 6492, 22894, 22902, 22905, 22912, 14722, 14727, 14754, 22947, 14757, 6567, 14762, 22955, 14769, 14770, 14772, 14773, 6582, 6583, 14778, 22976, 22979, 6597, 14792, 6600, 14798, 23015, 6635, 14829, 6645, 23031, 14841, 14843, 6652, 14844, 23042, 14853, 23047, 6664, 14856, 6668, 23056, 23058, 14867, 14872, 23069, 14878, 6692, 23078, 6698, 23082, 23091, 6711, 14913, 23108, 14918, 6751, 23136, 6757, 14951, 23148, 23156, 14980, 23174, 14986, 6798, 23200, 15017, 15032, 6842, 23228, 6845, 15044, 15049, 15051, 23251, 15060, 6867, 15070, 6880, 15087, 15093, 23290, 23294, 6914, 23302, 6939, 23323, 15136, 23332, 6952, 23339, 15156, 15158, 23359, 23373, 15185, 6995, 15188, 23380, 7000, 7013, 7016, 7017, 23402, 23413, 7031, 7035, 23420, 23422, 15234, 23427, 7053, 15245, 23449, 23451, 23453, 23462, 7085, 23473, 15282, 7094, 15299, 7107, 23498, 23514, 23521, 7147, 7158, 15351, 15355, 7163, 15361, 7180, 23567, 15376, 15384, 7202, 23587, 7207, 7209, 7211, 23599, 15418, 15430, 7238, 15432, 15451, 23647, 23654, 15468, 15476, 23671, 7291, 15486, 23681, 7300, 15492, 15499, 15507, 7323, 15518, 15522, 7334, 7336, 23725, 7349, 23739, 7361, 23749, 15562, 23755, 23764, 7384, 23777, 23778, 15587, 23784, 7412, 15608, 15613, 23816, 7435, 15631, 23829, 23834, 23839, 7457, 23843, 7462, 7470, 15665, 23861, 15678, 7490, 23878, 7497, 23883, 15705, 7519, 23904, 7524, 7525, 7528, 23914, 7536, 23925, 7547, 15740, 23936, 23944, 23953, 7571, 23958, 15770, 23963, 7585, 7586, 15805, 15814, 7624, 15816, 15835, 24033, 24043, 7662, 7663, 24059, 7679, 15873, 15881, 24074, 24091, 24095, 24098, 24107, 7724, 7732, 15925, 24118, 15924, 7740, 15935, 7744, 7754, 24139, 15951, 7770, 15967, 15982, 24185, 24187, 16002, 16005, 16007, 7827, 24211, 7832, 16025, 7845, 16037, 7850, 16046, 16050, 16052, 16054, 24251, 16061, 16074, 16079, 7887, 16080, 16082, 7894, 16086, 7899, 24289, 16110, 16123, 7932, 24324, 24326, 16148, 24343, 7965, 24350, 24356, 7978, 24363, 24367, 7987, 24373, 16190, 24396, 16231, 16232, 24432, 8049, 24436, 16256, 16262, 8077, 8078, 8091, 8092, 8093, 8096, 16289, 16288, 8100, 16315, 8127, 8134, 8146, 16340, 16348, 16353, 8175}, 14: {16388, 8196, 8, 16392, 12, 16401, 8213, 16414, 36, 8236, 8247, 16447, 8266, 8281, 16486, 8298, 107, 106, 8307, 8308, 122, 8315, 129, 16515, 16521, 16538, 16539, 156, 8352, 161, 16544, 16547, 168, 170, 178, 16565, 8376, 16572, 194, 200, 211, 212, 8406, 216, 8408, 8411, 229, 250, 8443, 260, 267, 279, 16671, 287, 16680, 16690, 16691, 308, 8502, 319, 16703, 8515, 16711, 331, 341, 16725, 16730, 348, 8541, 8545, 16737, 16742, 8561, 8564, 379, 16765, 16766, 8575, 8580, 8583, 16776, 8584, 402, 404, 16789, 8598, 8599, 16796, 8606, 414, 16801, 425, 429, 452, 453, 8647, 8654, 16850, 16860, 8669, 478, 8672, 484, 485, 494, 16878, 16879, 497, 16888, 8698, 506, 16893, 510, 8703, 8715, 526, 8721, 8726, 16923, 8740, 549, 16941, 16960, 577, 8779, 588, 594, 16986, 603, 8796, 17010, 8822, 17017, 17022, 8831, 17026, 8839, 653, 662, 17048, 17050, 17065, 682, 17071, 17073, 697, 17091, 17093, 17096, 17098, 716, 17102, 725, 8918, 8930, 17130, 752, 760, 17151, 8960, 8963, 17156, 772, 8976, 8980, 17185, 17196, 9008, 17201, 9011, 820, 9018, 17220, 9031, 17228, 17231, 17233, 17240, 859, 866, 17253, 9079, 9082, 17276, 895, 17282, 17285, 17290, 9107, 915, 17301, 17302, 17304, 923, 928, 9122, 9142, 9144, 9146, 17340, 9150, 960, 961, 9156, 17349, 965, 9160, 9164, 17360, 977, 17365, 17369, 17371, 17384, 1003, 17390, 9204, 17398, 17418, 9228, 9229, 1037, 9241, 17434, 1050, 9252, 9261, 1081, 17466, 1083, 17474, 9290, 17512, 17517, 17518, 17520, 1137, 9333, 1154, 9351, 17545, 9354, 1165, 1170, 9364, 1177, 17565, 1186, 1200, 17590, 17603, 17605, 17607, 1226, 17614, 9433, 17626, 17643, 9453, 1263, 17649, 1266, 1269, 9462, 17655, 9474, 9478, 9479, 17675, 9490, 1300, 17695, 1314, 9509, 9512, 9513, 17706, 17705, 1342, 9539, 1350, 17744, 1364, 9562, 9566, 1381, 1383, 17771, 9595, 17791, 9616, 1425, 1426, 9620, 17814, 17818, 17821, 17822, 9631, 17823, 17824, 9639, 1450, 9652, 17853, 1475, 9668, 9674, 17866, 17879, 1501, 9694, 1518, 1529, 1530, 17915, 17916, 9739, 17932, 17934, 9743, 9744, 9749, 9759, 17953, 1570, 17958, 9770, 17967, 1608, 1609, 1613, 9814, 1625, 9833, 1656, 9850, 1660, 9859, 1670, 1672, 18057, 1673, 9870, 18063, 9877, 9880, 9889, 18086, 18091, 9899, 18094, 1711, 1722, 9917, 1725, 18111, 1733, 9932, 9938, 1746, 9946, 18142, 1758, 9952, 1763, 9965, 9968, 18161, 1779, 18167, 9975, 1795, 1802, 1804, 18199, 1822, 10018, 1835, 1838, 10040, 18236, 10057, 10071, 1888, 18276, 10089, 18285, 1903, 10095, 10106, 1916, 18308, 18313, 10131, 18324, 18326, 18328, 10141, 10147, 18346, 1964, 1965, 1973, 10165, 1983, 18389, 2022, 18414, 2034, 10228, 2053, 18442, 2062, 2067, 10264, 18465, 2088, 18475, 18477, 10287, 18480, 2099, 10293, 10297, 10298, 10301, 18497, 10306, 10312, 10319, 18516, 18530, 18539, 2156, 10354, 18566, 10375, 10379, 2196, 18594, 2215, 2217, 2221, 2222, 18617, 2243, 18630, 10439, 18645, 10463, 18656, 10467, 18673, 10486, 2296, 2307, 10503, 2313, 10508, 2332, 18718, 2335, 10526, 10529, 2339, 10534, 2346, 2347, 18731, 10543, 18743, 2360, 18746, 10557, 18754, 2381, 2394, 10589, 2401, 18789, 2410, 18801, 2422, 2425, 18819, 2435, 10630, 2440, 18829, 2446, 10640, 2452, 18836, 10659, 2468, 2472, 18859, 2480, 2481, 10676, 10683, 10684, 10689, 18890, 10699, 2508, 18898, 18899, 2516, 2517, 18903, 2522, 2529, 10721, 18913, 10734, 10741, 2550, 18935, 18945, 2565, 18952, 10762, 2572, 10767, 2583, 2584, 10789, 2609, 2615, 19000, 2616, 10810, 19003, 19006, 10815, 2628, 19012, 19030, 10841, 2650, 10842, 2658, 10858, 10866, 19062, 10873, 19065, 19073, 10884, 2693, 2704, 10901, 10905, 19098, 2715, 19109, 19112, 19131, 10945, 19141, 2759, 10951, 2762, 10956, 10957, 2767, 10968, 10972, 10973, 19174, 2805, 11003, 19196, 19204, 2825, 2834, 2836, 11029, 19225, 11034, 19230, 11038, 19234, 2853, 11048, 2862, 19253, 11063, 19258, 2877, 11069, 11072, 11086, 11090, 11099, 11121, 11123, 2933, 2938, 2940, 2941, 19329, 19333, 11143, 11157, 19351, 2975, 19364, 11176, 2992, 2993, 2994, 19385, 11194, 11198, 19391, 11207, 19403, 11213, 19427, 19429, 19432, 3053, 3065, 11259, 3067, 19455, 19457, 19461, 3084, 19478, 19484, 11295, 11296, 3108, 11303, 19497, 19500, 3127, 19511, 3131, 19516, 3140, 3141, 11336, 19531, 19533, 19535, 3152, 19543, 3160, 11353, 19546, 11357, 11358, 11364, 3201, 11395, 3208, 19599, 19601, 11417, 19617, 11430, 3238, 19624, 19636, 3253, 19639, 11447, 11449, 11455, 11456, 3265, 19654, 3275, 11467, 19681, 11490, 11509, 11510, 11512, 19713, 19721, 19734, 11544, 19736, 19741, 11549, 11552, 19745, 3360, 11568, 19762, 19767, 19772, 19777, 3395, 19779, 11599, 3412, 3422, 11625, 3440, 11633, 11636, 3444, 3455, 11654, 3467, 3469, 19861, 19865, 19883, 19891, 19897, 19898, 3517, 3518, 11712, 19910, 3531, 3539, 3548, 11741, 19940, 11751, 3561, 19951, 11763, 11771, 11772, 3584, 11778, 19976, 11784, 19984, 3601, 19990, 11805, 3615, 11817, 3626, 3627, 20021, 3638, 3641, 20039, 11870, 11876, 20071, 11893, 20090, 20096, 20098, 3717, 11920, 20117, 20133, 20138, 11947, 20139, 3766, 3767, 3769, 20168, 20170, 11997, 12014, 20207, 12037, 3846, 20231, 12040, 12041, 12047, 20252, 3869, 3868, 20257, 20267, 20276, 20286, 20288, 3905, 12110, 12114, 12118, 3933, 12128, 3943, 3947, 3950, 12146, 12148, 3961, 12167, 3976, 12168, 12170, 20370, 12180, 3992, 3996, 3998, 12202, 20395, 12207, 12213, 12215, 12220, 12221, 4034, 4040, 4041, 20428, 20429, 20430, 4064, 20450, 12266, 20477, 20480, 4114, 12308, 20505, 12315, 12319, 4133, 20520, 12328, 20525, 12338, 4168, 12364, 20562, 12373, 4185, 4186, 12384, 4192, 4194, 12388, 4200, 20585, 4202, 20588, 4206, 20594, 20597, 12405, 20599, 4219, 20606, 12418, 4235, 4237, 4240, 4244, 12439, 20641, 20646, 12457, 4275, 20665, 4282, 12476, 4287, 4288, 12480, 12483, 12498, 4319, 20707, 4327, 20716, 4333, 4339, 20725, 20732, 20733, 20737, 20740, 20744, 4363, 20751, 20753, 20764, 20772, 12581, 12590, 12594, 4408, 12605, 20799, 20804, 12616, 4426, 20819, 12640, 20835, 12651, 20848, 20860, 12672, 4484, 12679, 20892, 12702, 12703, 20907, 4524, 12722, 4531, 20916, 12737, 12742, 20957, 12771, 20967, 12776, 4586, 12784, 4598, 20987, 20988, 12796, 12810, 21006, 12817, 21022, 4643, 12837, 12838, 4649, 21034, 4654, 4655, 4656, 4657, 21041, 12849, 4660, 21056, 12871, 21065, 21068, 21077, 12886, 21084, 12899, 12904, 21109, 4725, 12922, 4735, 21126, 12936, 4745, 12939, 4773, 21161, 4778, 21165, 4784, 12985, 12986, 4796, 12995, 12998, 4811, 21199, 13008, 4817, 21202, 21208, 4829, 4833, 4839, 13036, 21229, 13058, 13066, 4885, 21273, 4894, 21289, 4907, 21296, 21298, 4920, 4921, 13117, 4930, 13123, 13132, 21326, 13151, 13153, 21349, 13158, 4970, 4972, 21356, 13168, 13180, 21375, 4997, 21386, 13195, 13197, 21396, 21400, 21401, 21403, 5057, 21445, 21450, 13262, 13267, 5081, 21466, 21465, 5086, 13283, 13287, 21482, 21486, 21489, 13303, 13306, 21502, 5141, 13333, 13336, 21531, 13345, 21539, 5156, 5164, 13357, 13360, 5172, 21557, 5178, 21563, 21565, 21566, 13381, 21585, 13398, 5208, 5223, 13419, 21618, 5240, 13435, 21653, 5269, 5273, 21657, 21659, 5281, 21672, 13486, 21684, 5310, 21705, 13516, 5330, 5332, 21730, 21748, 5368, 13565, 5374, 13576, 13579, 21781, 21783, 21785, 21787, 21789, 21791, 21794, 13605, 5418, 13613, 13614, 21808, 13617, 21810, 13622, 5437, 21822, 13634, 21833, 5460, 21844, 21846, 5468, 5470, 21857, 21863, 13682, 13685, 21878, 21880, 21885, 21886, 13705, 5518, 21903, 13713, 13716, 13724, 21920, 13729, 5539, 13734, 13735, 21928, 21929, 13750, 5561, 13754, 5563, 21951, 21979, 5603, 13796, 5606, 21991, 5614, 5615, 13810, 22003, 13817, 13827, 22030, 13839, 5651, 22047, 5664, 22053, 22057, 5680, 13883, 5694, 22094, 22096, 5716, 13908, 5718, 22102, 5727, 5728, 22136, 13954, 5767, 13961, 22154, 13964, 22158, 13979, 13991, 5800, 13998, 22193, 14005, 14017, 14018, 5827, 5830, 22217, 5847, 22232, 22233, 14039, 22241, 14054, 14055, 14071, 5887, 22278, 22279, 5900, 14094, 22317, 22327, 14139, 5947, 14148, 5956, 5972, 14165, 22366, 5984, 5986, 5991, 5999, 14192, 22385, 6002, 6012, 14212, 14217, 14220, 14221, 22416, 6033, 6034, 6048, 14242, 14247, 22441, 22443, 22444, 22445, 22448, 22449, 22463, 14272, 22474, 14290, 14293, 14303, 14304, 22499, 6122, 14316, 6124, 22514, 6137, 14333, 6144, 6145, 14357, 14362, 6173, 6175, 14367, 14369, 14372, 22568, 14378, 22571, 6191, 6192, 14394, 14397, 22592, 22594, 6213, 14408, 6220, 14413, 6222, 6224, 22609, 14427, 6239, 14432, 22628, 14446, 22643, 6265, 14459, 14472, 14474, 6282, 6284, 22667, 6299, 6309, 22694, 6310, 6314, 22699, 22700, 6331, 22719, 22723, 22734, 14551, 6364, 6366, 6374, 14567, 6393, 14597, 6414, 22809, 22812, 22815, 6436, 6442, 14645, 22838, 22859, 6475, 14676, 14692, 14695, 6509, 6510, 6512, 6515, 6517, 22907, 22919, 6541, 22928, 14736, 14747, 14748, 6561, 6575, 22965, 22974, 14786, 14787, 14789, 22992, 23007, 14815, 14817, 23011, 23019, 14830, 14832, 14838, 14846, 6659, 14859, 6673, 23063, 14883, 23079, 14891, 14894, 14899, 14901, 14907, 6722, 6726, 6738, 23134, 14947, 6768, 23153, 14962, 23152, 23163, 23171, 23176, 23178, 23205, 6824, 6826, 23211, 23224, 15043, 6854, 15052, 6862, 23260, 15078, 15085, 23279, 23286, 6903, 15096, 6909, 23304, 23312, 15123, 15124, 23318, 15127, 23320, 6941, 15135, 23340, 15151, 23347, 15160, 6971, 23372, 6989, 6999, 15192, 7006, 15203, 23405, 23410, 15220, 7039, 7049, 7054, 7068, 15270, 23475, 23478, 23483, 7105, 23491, 23494, 15306, 15308, 7116, 7120, 23509, 7125, 15320, 15322, 15333, 23526, 7144, 7155, 15350, 15359, 7168, 7176, 7178, 7182, 15394, 7206, 15403, 7212, 15404, 7223, 15422, 15423, 7248, 7260, 15460, 15464, 23660, 7277, 23662, 7282, 23674, 7307, 7317, 23702, 15511, 7320, 23704, 15517, 23719, 23721, 7337, 23724, 23726, 15537, 7352, 7353, 15547, 23741, 15551, 15558, 23757, 7375, 15567, 7379, 23776, 15609, 23808, 7436, 15628, 23824, 15644, 23842, 7459, 15652, 23846, 7471, 23859, 15667, 7482, 23886, 7508, 23901, 15717, 15718, 15719, 23919, 23920, 15730, 23927, 15735, 15737, 7543, 23934, 15744, 23940, 15755, 23948, 23952, 15764, 15771, 7580, 7581, 23964, 23969, 15778, 23979, 15790, 7600, 15797, 7612, 15818, 7638, 15834, 24031, 15845, 24038, 7654, 24040, 24048, 24050, 24054, 24069, 24070, 15882, 24076, 15887, 24080, 24082, 24085, 24090, 24093, 15909, 7721, 24106, 24127, 24129, 7753, 24140, 15950, 15958, 15964, 24156, 15971, 7783, 15975, 24172, 15981, 24174, 24177, 7808, 24206, 7823, 7824, 24209, 24215, 16027, 7842, 7843, 16041, 7855, 16047, 16049, 24249, 16060, 7874, 7875, 16067, 24264, 24265, 16078, 7893, 16087, 24282, 24293, 24295, 7913, 7915, 7921, 16116, 7927, 24311, 7930, 24315, 7935, 7939, 24325, 16136, 16138, 7948, 7953, 7956, 16150, 7962, 7974, 24364, 7982, 16175, 16174, 24374, 16186, 16188, 16196, 16197, 24390, 16207, 8022, 16215, 16216, 24415, 16247, 8058, 24445, 16254, 24449, 16269, 16270, 24465, 24467, 24480, 24485, 8111, 8112, 16318, 8131, 8133, 16332, 16333, 8141, 8160, 16382}}\n",
      "Done...\n"
     ]
    }
   ],
   "source": [
    "logger = SummaryWriter('../logs')\n",
    "#load data\n",
    "train_dataset, test_dataset,user_groups=get_dataset(options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (MLP): Sequential(\n",
      "    (0): Linear(in_features=30, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.01, inplace=False)\n",
      "    (3): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.01, inplace=False)\n",
      "    (6): Linear(in_features=100, out_features=15, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      " | Training Round : 1 |\n",
      "\n",
      "idxs_users [11  0  5 12  9  3  1  4 10 14  7 13  6  8  2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\perera\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_id 11\n",
      "|---- Test Accuracy_client: 0.87%\n",
      "|---- F1_score: 0.7325097906328765\n",
      "|---- Precision: 0.7724852801354437\n",
      "|---- Recall: 0.7427906670591892\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.8648    0.9759    0.9170       498\n",
      "           4     0.4729    0.9529    0.6321       467\n",
      "           5     0.9785    0.2172    0.3555       419\n",
      "           6     1.0000    0.6198    0.7653       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9332    0.9654       449\n",
      "          10     0.8911    0.9326    0.9114       430\n",
      "          11     0.9381    1.0000    0.9681       455\n",
      "          12     0.5313    0.5640    0.5472       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8715      7004\n",
      "   macro avg     0.7725    0.7428    0.7325      7004\n",
      "weighted avg     0.8954    0.8715    0.8600      7004\n",
      "\n",
      "Testing Loss : 0.00434905115295859\n",
      "client_id 0\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7558700780519575\n",
      "|---- Precision: 0.7721749271275246\n",
      "|---- Recall: 0.7603047591216467\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4837    0.7645    0.5925       467\n",
      "           5     0.7912    0.6420    0.7088       419\n",
      "           6     0.7137    0.7670    0.7394       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9354    0.9666       449\n",
      "          10     0.8302    0.9326    0.8784       430\n",
      "          11     0.9401    1.0000    0.9691       455\n",
      "          12     0.9944    0.4409    0.6109       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8869      7004\n",
      "   macro avg     0.7722    0.7603    0.7559      7004\n",
      "weighted avg     0.8934    0.8869    0.8808      7004\n",
      "\n",
      "Testing Loss : 0.004292715344841841\n",
      "client_id 5\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7599956047251059\n",
      "|---- Precision: 0.785984946154532\n",
      "|---- Recall: 0.7631017819702943\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4770    0.9315    0.6309       467\n",
      "           5     0.7935    0.6420    0.7098       419\n",
      "           6     0.9482    0.6440    0.7670       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.8379    1.0000    0.9118       455\n",
      "          12     0.9626    0.4433    0.6071       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8899      7004\n",
      "   macro avg     0.7860    0.7631    0.7600      7004\n",
      "weighted avg     0.9066    0.8899    0.8847      7004\n",
      "\n",
      "Testing Loss : 0.0037872733879113184\n",
      "client_id 12\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.759577662231742\n",
      "|---- Precision: 0.7711248833668456\n",
      "|---- Recall: 0.7627668288017492\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.5435    0.5482    0.5458       467\n",
      "           5     0.8282    0.6444    0.7248       419\n",
      "           6     0.9862    0.6264    0.7661       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9930    0.9443    0.9680       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9476    0.9934    0.9700       455\n",
      "          12     0.4542    0.8300    0.5871       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8862      7004\n",
      "   macro avg     0.7711    0.7628    0.7596      7004\n",
      "weighted avg     0.8959    0.8862    0.8843      7004\n",
      "\n",
      "Testing Loss : 0.0039975429190453595\n",
      "client_id 9\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7558784164680383\n",
      "|---- Precision: 0.7701401856603994\n",
      "|---- Recall: 0.7610942872317493\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.5022    0.7388    0.5979       467\n",
      "           5     0.7935    0.6420    0.7098       419\n",
      "           6     0.6902    0.8176    0.7485       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9726    0.9488    0.9605       449\n",
      "          10     0.8302    0.9326    0.8784       430\n",
      "          11     0.9506    0.9736    0.9620       455\n",
      "          12     0.9835    0.4409    0.6088       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8876      7004\n",
      "   macro avg     0.7701    0.7611    0.7559      7004\n",
      "weighted avg     0.8915    0.8876    0.8808      7004\n",
      "\n",
      "Testing Loss : 0.004034412300362034\n",
      "client_id 3\n",
      "|---- Test Accuracy_client: 0.85%\n",
      "|---- F1_score: 0.701848483565593\n",
      "|---- Precision: 0.6983459860619378\n",
      "|---- Recall: 0.7248718575083283\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.6269    0.5182    0.5674       467\n",
      "           5     0.0000    0.0000    0.0000       419\n",
      "           6     0.8588    0.6681    0.7515       455\n",
      "           7     0.7842    0.9978    0.8782       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9354    0.9666       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9401    1.0000    0.9691       455\n",
      "          12     0.3615    0.8966    0.5152       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8521      7004\n",
      "   macro avg     0.6983    0.7249    0.7018      7004\n",
      "weighted avg     0.8300    0.8521    0.8323      7004\n",
      "\n",
      "Testing Loss : 0.003987898069860968\n",
      "client_id 1\n",
      "|---- Test Accuracy_client: 0.86%\n",
      "|---- F1_score: 0.728045500399683\n",
      "|---- Precision: 0.7832218874024098\n",
      "|---- Recall: 0.7342481522204015\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.3799    0.8908    0.5327       467\n",
      "           5     0.9785    0.2172    0.3555       419\n",
      "           6     0.8196    0.6791    0.7428       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.8800    0.9310    0.9048       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.9835    0.4409    0.6088       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8639      7004\n",
      "   macro avg     0.7832    0.7342    0.7280      7004\n",
      "weighted avg     0.9028    0.8639    0.8552      7004\n",
      "\n",
      "Testing Loss : 0.004001703673387548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_id 4\n",
      "|---- Test Accuracy_client: 0.87%\n",
      "|---- F1_score: 0.7325641628681636\n",
      "|---- Precision: 0.7605320154078138\n",
      "|---- Recall: 0.7431482854844587\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.8294    0.9759    0.8967       498\n",
      "           4     0.4831    0.8244    0.6092       467\n",
      "           5     0.9796    0.2291    0.3714       419\n",
      "           6     0.7731    0.7341    0.7531       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.5548    0.5739    0.5642       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8715      7004\n",
      "   macro avg     0.7605    0.7431    0.7326      7004\n",
      "weighted avg     0.8832    0.8715    0.8596      7004\n",
      "\n",
      "Testing Loss : 0.003824799072006978\n",
      "client_id 10\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7561799451050856\n",
      "|---- Precision: 0.7651679853365089\n",
      "|---- Recall: 0.761127562311268\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.6115    0.5696    0.5898       467\n",
      "           5     0.7895    0.6444    0.7096       419\n",
      "           6     1.0000    0.6198    0.7653       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9930    0.9443    0.9680       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.8464    0.9934    0.9141       455\n",
      "          12     0.4666    0.7906    0.5868       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8849      7004\n",
      "   macro avg     0.7652    0.7611    0.7562      7004\n",
      "weighted avg     0.8901    0.8849    0.8810      7004\n",
      "\n",
      "Testing Loss : 0.004107616464157978\n",
      "client_id 14\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7509822695432466\n",
      "|---- Precision: 0.7766802099753505\n",
      "|---- Recall: 0.7559922020122621\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    0.9983    0.9991       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.6174    0.5910    0.6039       467\n",
      "           5     0.4531    0.8998    0.6027       419\n",
      "           6     0.9861    0.6220    0.7628       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9332    0.9654       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.8395    1.0000    0.9127       455\n",
      "          12     0.9835    0.4409    0.6088       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8811      7004\n",
      "   macro avg     0.7767    0.7560    0.7510      7004\n",
      "weighted avg     0.8994    0.8811    0.8764      7004\n",
      "\n",
      "Testing Loss : 0.003987266832876467\n",
      "client_id 7\n",
      "|---- Test Accuracy_client: 0.86%\n",
      "|---- F1_score: 0.7073732873988539\n",
      "|---- Precision: 0.6935724618184873\n",
      "|---- Recall: 0.7312319634505281\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4718    0.8608    0.6096       467\n",
      "           5     0.5211    0.6492    0.5781       419\n",
      "           6     0.8311    0.6813    0.7488       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     0.8858    0.9485    0.9161       466\n",
      "           9     0.9726    0.9488    0.9605       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9506    0.9736    0.9620       455\n",
      "          12     0.0000    0.0000    0.0000       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8618      7004\n",
      "   macro avg     0.6936    0.7312    0.7074      7004\n",
      "weighted avg     0.8245    0.8618    0.8382      7004\n",
      "\n",
      "Testing Loss : 0.003918034460545565\n",
      "client_id 13\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7586036121588715\n",
      "|---- Precision: 0.781406293643394\n",
      "|---- Recall: 0.7614754310325923\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.8294    0.9759    0.8967       498\n",
      "           4     0.4734    0.8779    0.6152       467\n",
      "           5     0.7907    0.6492    0.7130       419\n",
      "           6     0.8559    0.6659    0.7491       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.9836    0.4433    0.6112       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8882      7004\n",
      "   macro avg     0.7814    0.7615    0.7586      7004\n",
      "weighted avg     0.9015    0.8882    0.8829      7004\n",
      "\n",
      "Testing Loss : 0.003875444147486914\n",
      "client_id 6\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7545985639342444\n",
      "|---- Precision: 0.7565374024705152\n",
      "|---- Recall: 0.7604883408021744\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     0.9109    1.0000    0.9534       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.6396    0.5396    0.5854       467\n",
      "           5     0.7002    0.6802    0.6901       419\n",
      "           6     0.8916    0.6505    0.7522       455\n",
      "           7     0.8691    0.9978    0.9290       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.4967    0.7512    0.5980       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8844      7004\n",
      "   macro avg     0.7565    0.7605    0.7546      7004\n",
      "weighted avg     0.8806    0.8844    0.8788      7004\n",
      "\n",
      "Testing Loss : 0.0040377993042235594\n",
      "client_id 8\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7456150715869404\n",
      "|---- Precision: 0.7672046942073237\n",
      "|---- Recall: 0.7513804637163252\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.6528    0.4711    0.5473       467\n",
      "           5     0.4356    0.9045    0.5881       419\n",
      "           6     0.7518    0.6857    0.7172       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9576    0.9555    0.9565       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9561    0.9582    0.9572       455\n",
      "          12     0.9835    0.4409    0.6088       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8764      7004\n",
      "   macro avg     0.7672    0.7514    0.7456      7004\n",
      "weighted avg     0.8903    0.8764    0.8711      7004\n",
      "\n",
      "Testing Loss : 0.003818527304980668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 1/10 [00:05<00:46,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_id 2\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7503910843458509\n",
      "|---- Precision: 0.775145801508638\n",
      "|---- Recall: 0.7551605817876772\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.8294    0.9759    0.8967       498\n",
      "           4     0.6230    0.5696    0.5951       467\n",
      "           5     0.4500    0.9021    0.6005       419\n",
      "           6     0.9695    0.6286    0.7627       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9815    0.9465    0.9637       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9490    0.9824    0.9654       455\n",
      "          12     0.9730    0.4433    0.6091       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8802      7004\n",
      "   macro avg     0.7751    0.7552    0.7504      7004\n",
      "weighted avg     0.8975    0.8802    0.8755      7004\n",
      "\n",
      "Testing Loss : 0.003949639475189026\n",
      " \n",
      "Avg Training Stats after 1 global rounds:\n",
      "|---- Test Accuracy: 0.8802113078241005\n",
      "|---- F1_score: 0.7503910843458509\n",
      "|---- Precision: 0.775145801508638\n",
      "|---- Recall: 0.7551605817876772\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.8294    0.9759    0.8967       498\n",
      "           4     0.6230    0.5696    0.5951       467\n",
      "           5     0.4500    0.9021    0.6005       419\n",
      "           6     0.9695    0.6286    0.7627       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9815    0.9465    0.9637       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9490    0.9824    0.9654       455\n",
      "          12     0.9730    0.4433    0.6091       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8802      7004\n",
      "   macro avg     0.7751    0.7552    0.7504      7004\n",
      "weighted avg     0.8975    0.8802    0.8755      7004\n",
      "\n",
      "Testing Loss : 0.003949639475189026\n",
      "\n",
      " | Training Round : 2 |\n",
      "\n",
      "idxs_users [ 9  2 10  1 12  3  7 11  8  5 13  0  4 14  6]\n",
      "client_id 9\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7609559313478369\n",
      "|---- Precision: 0.7902906714528193\n",
      "|---- Recall: 0.7633792135632983\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4729    0.9529    0.6321       467\n",
      "           5     0.6783    0.6492    0.6634       419\n",
      "           6     1.0000    0.6198    0.7653       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9332    0.9654       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9381    1.0000    0.9681       455\n",
      "          12     0.9944    0.4409    0.6109       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8902      7004\n",
      "   macro avg     0.7903    0.7634    0.7610      7004\n",
      "weighted avg     0.9112    0.8902    0.8858      7004\n",
      "\n",
      "Testing Loss : 0.004131300804890833\n",
      "client_id 2\n",
      "|---- Test Accuracy_client: 0.85%\n",
      "|---- F1_score: 0.701468290814281\n",
      "|---- Precision: 0.6964290766560544\n",
      "|---- Recall: 0.7256345762841009\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.7172    0.4561    0.5576       467\n",
      "           5     0.3744    0.8998    0.5288       419\n",
      "           6     0.7585    0.7385    0.7483       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     0.8858    0.9485    0.9161       466\n",
      "           9     1.0000    0.9354    0.9666       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9401    1.0000    0.9691       455\n",
      "          12     0.0000    0.0000    0.0000       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8544      7004\n",
      "   macro avg     0.6964    0.7256    0.7015      7004\n",
      "weighted avg     0.8285    0.8544    0.8326      7004\n",
      "\n",
      "Testing Loss : 0.0038472731698246973\n",
      "client_id 10\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7491377494083813\n",
      "|---- Precision: 0.7864989313428014\n",
      "|---- Recall: 0.7545489262307932\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4577    0.9615    0.6202       467\n",
      "           5     0.7571    0.6396    0.6934       419\n",
      "           6     1.0000    0.6198    0.7653       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9726    0.9488    0.9605       449\n",
      "          10     0.8302    0.9326    0.8784       430\n",
      "          11     0.9506    0.9736    0.9620       455\n",
      "          12     1.0000    0.3202    0.4851       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8825      7004\n",
      "   macro avg     0.7865    0.7545    0.7491      7004\n",
      "weighted avg     0.9074    0.8825    0.8752      7004\n",
      "\n",
      "Testing Loss : 0.0039406295382826756\n",
      "client_id 1\n",
      "|---- Test Accuracy_client: 0.86%\n",
      "|---- F1_score: 0.7050046392108748\n",
      "|---- Precision: 0.7012367482163462\n",
      "|---- Recall: 0.7278783642881896\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.8294    0.9759    0.8967       498\n",
      "           4     0.4720    0.9572    0.6322       467\n",
      "           5     0.0000    0.0000    0.0000       419\n",
      "           6     1.0000    0.6198    0.7653       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9726    0.9488    0.9605       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9506    0.9736    0.9620       455\n",
      "          12     0.4421    0.5640    0.4957       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8581      7004\n",
      "   macro avg     0.7012    0.7279    0.7050      7004\n",
      "weighted avg     0.8312    0.8581    0.8352      7004\n",
      "\n",
      "Testing Loss : 0.004423312904809983\n",
      "client_id 12\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7602112235140346\n",
      "|---- Precision: 0.7902084098067266\n",
      "|---- Recall: 0.7632181466206801\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4729    0.9529    0.6321       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     1.0000    0.6220    0.7669       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.8302    0.9326    0.8784       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.9944    0.4409    0.6109       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8901      7004\n",
      "   macro avg     0.7902    0.7632    0.7602      7004\n",
      "weighted avg     0.9109    0.8901    0.8851      7004\n",
      "\n",
      "Testing Loss : 0.0038478293207882003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_id 3\n",
      "|---- Test Accuracy_client: 0.87%\n",
      "|---- F1_score: 0.7318681977193681\n",
      "|---- Precision: 0.7554769263747083\n",
      "|---- Recall: 0.7421699751694859\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     0.9653    1.0000    0.9824       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.4783    0.8009    0.5989       467\n",
      "           5     0.9439    0.2411    0.3840       419\n",
      "           6     0.7590    0.7407    0.7497       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.8691    0.9465    0.9062       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9492    0.9846    0.9666       455\n",
      "          12     0.5531    0.5640    0.5585       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8705      7004\n",
      "   macro avg     0.7555    0.7422    0.7319      7004\n",
      "weighted avg     0.8783    0.8705    0.8589      7004\n",
      "\n",
      "Testing Loss : 0.0036862983891175582\n",
      "client_id 7\n",
      "|---- Test Accuracy_client: 0.87%\n",
      "|---- F1_score: 0.7312153308824172\n",
      "|---- Precision: 0.7975068698710482\n",
      "|---- Recall: 0.7369410211423186\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.3894    0.9957    0.5599       467\n",
      "           5     0.9785    0.2172    0.3555       419\n",
      "           6     1.0000    0.6220    0.7669       455\n",
      "           7     0.9124    0.9978    0.9532       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.7894    0.9326    0.8550       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.9944    0.4335    0.6038       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8668      7004\n",
      "   macro avg     0.7975    0.7369    0.7312      7004\n",
      "weighted avg     0.9173    0.8668    0.8588      7004\n",
      "\n",
      "Testing Loss : 0.0039455576867358745\n",
      "client_id 11\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7525074718153972\n",
      "|---- Precision: 0.7839366967983799\n",
      "|---- Recall: 0.7557232058065363\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4715    0.9550    0.6313       467\n",
      "           5     0.7918    0.6444    0.7105       419\n",
      "           6     1.0000    0.6220    0.7669       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.8599    0.9844    0.9180       449\n",
      "          10     0.8302    0.9326    0.8784       430\n",
      "          11     0.9821    0.8418    0.9065       455\n",
      "          12     0.9944    0.4335    0.6038       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8828      7004\n",
      "   macro avg     0.7839    0.7557    0.7525      7004\n",
      "weighted avg     0.9049    0.8828    0.8776      7004\n",
      "\n",
      "Testing Loss : 0.004053416824407624\n",
      "client_id 8\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7579268618394185\n",
      "|---- Precision: 0.7669527402323303\n",
      "|---- Recall: 0.7610320972850303\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9526    1.0000    0.9757      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.6320    0.5589    0.5932       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     0.9632    0.6330    0.7639       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9815    0.9465    0.9637       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9490    0.9824    0.9654       455\n",
      "          12     0.4653    0.7931    0.5865       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8848      7004\n",
      "   macro avg     0.7670    0.7610    0.7579      7004\n",
      "weighted avg     0.8872    0.8848    0.8803      7004\n",
      "\n",
      "Testing Loss : 0.0036324794188265217\n",
      "client_id 5\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7432022556630938\n",
      "|---- Precision: 0.7614500465566728\n",
      "|---- Recall: 0.754228795919225\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.8782    0.2934    0.4398       467\n",
      "           5     0.7884    0.6492    0.7120       419\n",
      "           6     0.7221    0.7824    0.7511       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9976    0.9354    0.9655       449\n",
      "          10     0.7909    0.9326    0.8559       430\n",
      "          11     0.9400    0.9978    0.9680       455\n",
      "          12     0.4316    0.8005    0.5608       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8776      7004\n",
      "   macro avg     0.7615    0.7542    0.7432      7004\n",
      "weighted avg     0.8879    0.8776    0.8687      7004\n",
      "\n",
      "Testing Loss : 0.0038938257137982593\n",
      "client_id 13\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7465195739561163\n",
      "|---- Precision: 0.7681783480790227\n",
      "|---- Recall: 0.7533907178004008\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.6657    0.4732    0.5532       467\n",
      "           5     0.4438    0.9045    0.5954       419\n",
      "           6     0.8298    0.6967    0.7575       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.8803    0.9332    0.9059       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9381    1.0000    0.9681       455\n",
      "          12     0.9944    0.4384    0.6085       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8784      7004\n",
      "   macro avg     0.7682    0.7534    0.7465      7004\n",
      "weighted avg     0.8913    0.8784    0.8720      7004\n",
      "\n",
      "Testing Loss : 0.0038086403624555134\n",
      "client_id 0\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7542682391403875\n",
      "|---- Precision: 0.7767439276120416\n",
      "|---- Recall: 0.7589174340372715\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.5337    0.7452    0.6220       467\n",
      "           5     0.5254    0.7900    0.6311       419\n",
      "           6     1.0000    0.6220    0.7669       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.8379    1.0000    0.9118       455\n",
      "          12     0.9835    0.4409    0.6088       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8848      7004\n",
      "   macro avg     0.7767    0.7589    0.7543      7004\n",
      "weighted avg     0.8989    0.8848    0.8795      7004\n",
      "\n",
      "Testing Loss : 0.004249220147387453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_id 4\n",
      "|---- Test Accuracy_client: 0.85%\n",
      "|---- F1_score: 0.7015216114595064\n",
      "|---- Precision: 0.7010735270705365\n",
      "|---- Recall: 0.7238523080445016\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.8294    0.9759    0.8967       498\n",
      "           4     0.6398    0.5096    0.5673       467\n",
      "           5     0.0000    0.0000    0.0000       419\n",
      "           6     0.9046    0.6462    0.7538       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.3543    0.9163    0.5110       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8509      7004\n",
      "   macro avg     0.7011    0.7239    0.7015      7004\n",
      "weighted avg     0.8319    0.8509    0.8316      7004\n",
      "\n",
      "Testing Loss : 0.004178287000126758\n",
      "client_id 14\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7604712061670629\n",
      "|---- Precision: 0.7903668150318086\n",
      "|---- Recall: 0.7634630913626802\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4720    0.9572    0.6322       467\n",
      "           5     0.7970    0.6372    0.7082       419\n",
      "           6     1.0000    0.6220    0.7669       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9976    0.9421    0.9691       449\n",
      "          10     0.8302    0.9326    0.8784       430\n",
      "          11     0.9458    0.9978    0.9711       455\n",
      "          12     0.9835    0.4409    0.6088       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8903      7004\n",
      "   macro avg     0.7904    0.7635    0.7605      7004\n",
      "weighted avg     0.9111    0.8903    0.8853      7004\n",
      "\n",
      "Testing Loss : 0.0040905892128669475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [00:10<00:41,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_id 6\n",
      "|---- Test Accuracy_client: 0.86%\n",
      "|---- F1_score: 0.7252649591153768\n",
      "|---- Precision: 0.7668292871130308\n",
      "|---- Recall: 0.7394018817289889\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.7928    0.3769    0.5109       467\n",
      "           5     0.9802    0.2363    0.3808       419\n",
      "           6     0.7178    0.7714    0.7436       455\n",
      "           7     0.8691    0.9978    0.9290       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9354    0.9666       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.8410    1.0000    0.9137       455\n",
      "          12     0.3979    0.9163    0.5548       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8646      7004\n",
      "   macro avg     0.7668    0.7394    0.7253      7004\n",
      "weighted avg     0.8917    0.8646    0.8528      7004\n",
      "\n",
      "Testing Loss : 0.004136666892759123\n",
      " \n",
      "Avg Training Stats after 2 global rounds:\n",
      "|---- Test Accuracy: 0.8646487721302113\n",
      "|---- F1_score: 0.7252649591153768\n",
      "|---- Precision: 0.7668292871130308\n",
      "|---- Recall: 0.7394018817289889\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.7928    0.3769    0.5109       467\n",
      "           5     0.9802    0.2363    0.3808       419\n",
      "           6     0.7178    0.7714    0.7436       455\n",
      "           7     0.8691    0.9978    0.9290       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9354    0.9666       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.8410    1.0000    0.9137       455\n",
      "          12     0.3979    0.9163    0.5548       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8646      7004\n",
      "   macro avg     0.7668    0.7394    0.7253      7004\n",
      "weighted avg     0.8917    0.8646    0.8528      7004\n",
      "\n",
      "Testing Loss : 0.004136666892759123\n",
      "\n",
      " | Training Round : 3 |\n",
      "\n",
      "idxs_users [ 1  3 10  7  8 13  4  6 11  0  5 12 14  9  2]\n",
      "client_id 1\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7426673342971749\n",
      "|---- Precision: 0.7742610837086307\n",
      "|---- Recall: 0.750786669786222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.9812    0.3362    0.5008       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     0.4525    0.9736    0.6179       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9332    0.9654       449\n",
      "          10     0.8302    0.9326    0.8784       430\n",
      "          11     0.9381    1.0000    0.9681       455\n",
      "          12     0.7925    0.5172    0.6259       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8764      7004\n",
      "   macro avg     0.7743    0.7508    0.7427      7004\n",
      "weighted avg     0.8977    0.8764    0.8676      7004\n",
      "\n",
      "Testing Loss : 0.003764608373353816\n",
      "client_id 3\n",
      "|---- Test Accuracy_client: 0.84%\n",
      "|---- F1_score: 0.6930723457300925\n",
      "|---- Precision: 0.7175546293594698\n",
      "|---- Recall: 0.7142345065500596\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     0.9214    1.0000    0.9591       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.9643    0.3469    0.5102       467\n",
      "           5     0.0000    0.0000    0.0000       419\n",
      "           6     0.9077    0.6484    0.7564       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9266    0.9555    0.9408       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9546    0.9253    0.9397       455\n",
      "          12     0.3182    0.9828    0.4807       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8408      7004\n",
      "   macro avg     0.7176    0.7142    0.6931      7004\n",
      "weighted avg     0.8470    0.8408    0.8226      7004\n",
      "\n",
      "Testing Loss : 0.004091001893614426\n",
      "client_id 10\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7515285597683816\n",
      "|---- Precision: 0.7599818984645438\n",
      "|---- Recall: 0.7563003985823078\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.6590    0.4882    0.5609       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     0.8324    0.6220    0.7119       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9726    0.9488    0.9605       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9506    0.9736    0.9620       455\n",
      "          12     0.4245    0.8103    0.5572       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8799      7004\n",
      "   macro avg     0.7600    0.7563    0.7515      7004\n",
      "weighted avg     0.8854    0.8799    0.8766      7004\n",
      "\n",
      "Testing Loss : 0.0038405034937355974\n",
      "client_id 7\n",
      "|---- Test Accuracy_client: 0.86%\n",
      "|---- F1_score: 0.7235743468537866\n",
      "|---- Precision: 0.7753718682476641\n",
      "|---- Recall: 0.7351314997578756\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.6193    0.5503    0.5828       467\n",
      "           5     0.3868    0.9093    0.5427       419\n",
      "           6     0.9965    0.6220    0.7659       455\n",
      "           7     0.7842    0.9978    0.8782       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9354    0.9666       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9401    1.0000    0.9691       455\n",
      "          12     1.0000    0.1552    0.2687       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8626      7004\n",
      "   macro avg     0.7754    0.7351    0.7236      7004\n",
      "weighted avg     0.8985    0.8626    0.8524      7004\n",
      "\n",
      "Testing Loss : 0.0039028582947146784\n",
      "client_id 8\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7459113143393419\n",
      "|---- Precision: 0.7705152276624421\n",
      "|---- Recall: 0.7508168101467532\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.8294    0.9759    0.8967       498\n",
      "           4     0.6287    0.5439    0.5832       467\n",
      "           5     0.4428    0.9045    0.5945       419\n",
      "           6     0.9568    0.6330    0.7619       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9269    0.9599    0.9431       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9590    0.9253    0.9418       455\n",
      "          12     0.9624    0.4409    0.6047       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8759      7004\n",
      "   macro avg     0.7705    0.7508    0.7459      7004\n",
      "weighted avg     0.8931    0.8759    0.8712      7004\n",
      "\n",
      "Testing Loss : 0.004024733034460388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_id 13\n",
      "|---- Test Accuracy_client: 0.87%\n",
      "|---- F1_score: 0.7399038479488017\n",
      "|---- Precision: 0.7762222576474915\n",
      "|---- Recall: 0.7463581448501917\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.6097    0.5653    0.5867       467\n",
      "           5     0.4195    0.9021    0.5727       419\n",
      "           6     1.0000    0.6220    0.7669       455\n",
      "           7     0.8179    0.9978    0.8989       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     1.0000    0.3202    0.4851       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8725      7004\n",
      "   macro avg     0.7762    0.7464    0.7399      7004\n",
      "weighted avg     0.8989    0.8725    0.8665      7004\n",
      "\n",
      "Testing Loss : 0.0041490677546822535\n",
      "client_id 4\n",
      "|---- Test Accuracy_client: 0.86%\n",
      "|---- F1_score: 0.718465975623365\n",
      "|---- Precision: 0.7786631252471583\n",
      "|---- Recall: 0.732649793705253\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.8294    0.9759    0.8967       498\n",
      "           4     1.0000    0.2719    0.4276       467\n",
      "           5     0.9537    0.2458    0.3909       419\n",
      "           6     0.7456    0.7407    0.7431       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9332    0.9654       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9381    1.0000    0.9681       455\n",
      "          12     0.3613    0.9433    0.5225       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8577      7004\n",
      "   macro avg     0.7787    0.7326    0.7185      7004\n",
      "weighted avg     0.9032    0.8577    0.8458      7004\n",
      "\n",
      "Testing Loss : 0.0039036283109960455\n",
      "client_id 6\n",
      "|---- Test Accuracy_client: 0.85%\n",
      "|---- F1_score: 0.6986792520219182\n",
      "|---- Precision: 0.6900865312701101\n",
      "|---- Recall: 0.7252656367213967\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.0000    0.0000    0.0000       467\n",
      "           5     0.4014    0.9523    0.5648       419\n",
      "           6     0.8592    0.6703    0.7531       455\n",
      "           7     0.8179    0.9978    0.8989       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9838    0.9465    0.9648       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9492    0.9846    0.9666       455\n",
      "          12     0.4799    0.4704    0.4751       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8497      7004\n",
      "   macro avg     0.6901    0.7253    0.6987      7004\n",
      "weighted avg     0.8177    0.8497    0.8255      7004\n",
      "\n",
      "Testing Loss : 0.004394617124749449\n",
      "client_id 11\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7474644854067978\n",
      "|---- Precision: 0.7582227934752759\n",
      "|---- Recall: 0.7511036296800967\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.5489    0.5289    0.5387       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     0.9794    0.6264    0.7641       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.8569    0.9866    0.9172       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9845    0.8374    0.9050       455\n",
      "          12     0.4431    0.7857    0.5666       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8751      7004\n",
      "   macro avg     0.7582    0.7511    0.7475      7004\n",
      "weighted avg     0.8835    0.8751    0.8726      7004\n",
      "\n",
      "Testing Loss : 0.0046414189415769505\n",
      "client_id 0\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7604318131213998\n",
      "|---- Precision: 0.7902234090359286\n",
      "|---- Recall: 0.7633685826731206\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4724    0.9529    0.6317       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     1.0000    0.6198    0.7653       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     0.8858    0.9485    0.9161       466\n",
      "           9     1.0000    0.9354    0.9666       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9401    1.0000    0.9691       455\n",
      "          12     0.9944    0.4409    0.6109       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8902      7004\n",
      "   macro avg     0.7902    0.7634    0.7604      7004\n",
      "weighted avg     0.9104    0.8902    0.8850      7004\n",
      "\n",
      "Testing Loss : 0.004064957576158692\n",
      "client_id 5\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7615300848610914\n",
      "|---- Precision: 0.7942048638727232\n",
      "|---- Recall: 0.7623584543368858\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.4458    0.9507    0.6070       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     1.0000    0.6220    0.7669       455\n",
      "           7     0.8691    0.9978    0.9290       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9704    0.9488    0.9595       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9505    0.9714    0.9609       455\n",
      "          12     0.9835    0.4409    0.6088       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8892      7004\n",
      "   macro avg     0.7942    0.7624    0.7615      7004\n",
      "weighted avg     0.9147    0.8892    0.8862      7004\n",
      "\n",
      "Testing Loss : 0.003777986390916136\n",
      "client_id 12\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.76002132365436\n",
      "|---- Precision: 0.7859537311390657\n",
      "|---- Recall: 0.7628571374951836\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.8294    0.9759    0.8967       498\n",
      "           4     0.4766    0.9165    0.6271       467\n",
      "           5     0.7884    0.6492    0.7120       419\n",
      "           6     0.9105    0.6484    0.7574       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9332    0.9654       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9381    1.0000    0.9681       455\n",
      "          12     0.9944    0.4409    0.6109       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8896      7004\n",
      "   macro avg     0.7860    0.7629    0.7600      7004\n",
      "weighted avg     0.9058    0.8896    0.8843      7004\n",
      "\n",
      "Testing Loss : 0.0040697049395288145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_id 14\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7442652702360706\n",
      "|---- Precision: 0.7951897147331771\n",
      "|---- Recall: 0.7464739796190039\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4088    0.9936    0.5793       467\n",
      "           5     0.9803    0.3556    0.5219       419\n",
      "           6     0.8324    0.6220    0.7119       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9976    0.9399    0.9679       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9439    0.9978    0.9701       455\n",
      "          12     0.9944    0.4335    0.6038       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8754      7004\n",
      "   macro avg     0.7952    0.7465    0.7443      7004\n",
      "weighted avg     0.9143    0.8754    0.8702      7004\n",
      "\n",
      "Testing Loss : 0.003998358281188857\n",
      "client_id 9\n",
      "|---- Test Accuracy_client: 0.87%\n",
      "|---- F1_score: 0.7420549054060446\n",
      "|---- Precision: 0.7866464137279415\n",
      "|---- Recall: 0.748248597192063\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.8294    0.9759    0.8967       498\n",
      "           4     0.9756    0.3426    0.5071       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     0.4288    0.9934    0.5991       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9354    0.9666       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9401    1.0000    0.9691       455\n",
      "          12     0.9839    0.4507    0.6182       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8744      7004\n",
      "   macro avg     0.7866    0.7482    0.7421      7004\n",
      "weighted avg     0.9074    0.8744    0.8666      7004\n",
      "\n",
      "Testing Loss : 0.003993298426517277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 3/10 [00:15<00:35,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_id 2\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7618275159385244\n",
      "|---- Precision: 0.7930717076677266\n",
      "|---- Recall: 0.7627467713972531\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9526    1.0000    0.9757      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4728    0.9486    0.6311       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     0.9828    0.6264    0.7651       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9838    0.9465    0.9648       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9492    0.9846    0.9666       455\n",
      "          12     0.9944    0.4335    0.6038       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8896      7004\n",
      "   macro avg     0.7931    0.7627    0.7618      7004\n",
      "weighted avg     0.9087    0.8896    0.8841      7004\n",
      "\n",
      "Testing Loss : 0.0036976320790323715\n",
      " \n",
      "Avg Training Stats after 3 global rounds:\n",
      "|---- Test Accuracy: 0.8896344945745288\n",
      "|---- F1_score: 0.7618275159385244\n",
      "|---- Precision: 0.7930717076677266\n",
      "|---- Recall: 0.7627467713972531\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9526    1.0000    0.9757      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4728    0.9486    0.6311       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     0.9828    0.6264    0.7651       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9838    0.9465    0.9648       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9492    0.9846    0.9666       455\n",
      "          12     0.9944    0.4335    0.6038       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8896      7004\n",
      "   macro avg     0.7931    0.7627    0.7618      7004\n",
      "weighted avg     0.9087    0.8896    0.8841      7004\n",
      "\n",
      "Testing Loss : 0.0036976320790323715\n",
      "\n",
      " | Training Round : 4 |\n",
      "\n",
      "idxs_users [14  8 12  2  1 11  9 10  6  3  5  0 13  4  7]\n",
      "client_id 14\n",
      "|---- Test Accuracy_client: 0.87%\n",
      "|---- F1_score: 0.7403476925833334\n",
      "|---- Precision: 0.7776538603550286\n",
      "|---- Recall: 0.747245395062017\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.9935    0.3276    0.4928       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     0.4275    0.9978    0.5985       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.7469    0.4507    0.5622       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8734      7004\n",
      "   macro avg     0.7777    0.7472    0.7403      7004\n",
      "weighted avg     0.9009    0.8734    0.8656      7004\n",
      "\n",
      "Testing Loss : 0.003909899293365801\n",
      "client_id 8\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7601633671246778\n",
      "|---- Precision: 0.7809689404528807\n",
      "|---- Recall: 0.762957123766104\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.4733    0.9507    0.6320       467\n",
      "           5     0.7884    0.6492    0.7120       419\n",
      "           6     1.0000    0.6220    0.7669       455\n",
      "           7     0.8691    0.9978    0.9290       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9771    0.9488    0.9627       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9509    0.9780    0.9642       455\n",
      "          12     0.7521    0.4409    0.5559       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8898      7004\n",
      "   macro avg     0.7810    0.7630    0.7602      7004\n",
      "weighted avg     0.9035    0.8898    0.8853      7004\n",
      "\n",
      "Testing Loss : 0.003951947188280365\n",
      "client_id 12\n",
      "|---- Test Accuracy_client: 0.87%\n",
      "|---- F1_score: 0.7433058115906868\n",
      "|---- Precision: 0.7914469719025544\n",
      "|---- Recall: 0.7478301214322387\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.9874    0.3362    0.5016       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     0.4068    0.9978    0.5780       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9838    0.9465    0.9648       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9492    0.9846    0.9666       455\n",
      "          12     0.9839    0.4507    0.6182       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8739      7004\n",
      "   macro avg     0.7914    0.7478    0.7433      7004\n",
      "weighted avg     0.9127    0.8739    0.8681      7004\n",
      "\n",
      "Testing Loss : 0.004221048768222021\n",
      "client_id 2\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7575546139778998\n",
      "|---- Precision: 0.7801095107930531\n",
      "|---- Recall: 0.7610226159617075\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.4730    0.8822    0.6158       467\n",
      "           5     0.7982    0.6229    0.6997       419\n",
      "           6     0.8374    0.6791    0.7500       455\n",
      "           7     0.8691    0.9978    0.9290       459\n",
      "           8     0.8858    0.9485    0.9161       466\n",
      "           9     1.0000    0.9354    0.9666       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9401    1.0000    0.9691       455\n",
      "          12     0.9944    0.4409    0.6109       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8879      7004\n",
      "   macro avg     0.7801    0.7610    0.7576      7004\n",
      "weighted avg     0.9007    0.8879    0.8823      7004\n",
      "\n",
      "Testing Loss : 0.0038153372140125774\n",
      "client_id 1\n",
      "|---- Test Accuracy_client: 0.87%\n",
      "|---- F1_score: 0.7411752759633675\n",
      "|---- Precision: 0.7870951682979859\n",
      "|---- Recall: 0.7459418642765349\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9526    1.0000    0.9757      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.9812    0.3362    0.5008       467\n",
      "           5     0.7861    0.6492    0.7111       419\n",
      "           6     0.4294    0.9956    0.6000       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9510    0.9510    0.9510       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9516    0.9516    0.9516       455\n",
      "          12     0.9839    0.4507    0.6182       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8721      7004\n",
      "   macro avg     0.7871    0.7459    0.7412      7004\n",
      "weighted avg     0.9038    0.8721    0.8636      7004\n",
      "\n",
      "Testing Loss : 0.003992840478825423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_id 11\n",
      "|---- Test Accuracy_client: 0.85%\n",
      "|---- F1_score: 0.7070978308594714\n",
      "|---- Precision: 0.7741218515680682\n",
      "|---- Recall: 0.7221921186090992\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.8743    0.3576    0.5076       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     0.3798    0.9758    0.5468       455\n",
      "           7     0.8179    0.9978    0.8989       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9346    0.9555    0.9449       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9551    0.9341    0.9444       455\n",
      "          12     1.0000    0.1084    0.1956       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8514      7004\n",
      "   macro avg     0.7741    0.7222    0.7071      7004\n",
      "weighted avg     0.8955    0.8514    0.8358      7004\n",
      "\n",
      "Testing Loss : 0.0041183954174457485\n",
      "client_id 9\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7609418613839545\n",
      "|---- Precision: 0.7917479494227808\n",
      "|---- Recall: 0.7633685826731206\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.8294    0.9759    0.8967       498\n",
      "           4     0.4729    0.9529    0.6321       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     1.0000    0.6220    0.7669       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9976    0.9354    0.9655       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9400    0.9978    0.9680       455\n",
      "          12     0.9944    0.4409    0.6109       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8902      7004\n",
      "   macro avg     0.7917    0.7634    0.7609      7004\n",
      "weighted avg     0.9115    0.8902    0.8853      7004\n",
      "\n",
      "Testing Loss : 0.004405016071566815\n",
      "client_id 10\n",
      "|---- Test Accuracy_client: 0.87%\n",
      "|---- F1_score: 0.7413832623964591\n",
      "|---- Precision: 0.7783805729918438\n",
      "|---- Recall: 0.7464803290295461\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.7376    0.4154    0.5315       467\n",
      "           5     0.4006    0.9714    0.5672       419\n",
      "           6     1.0000    0.6198    0.7653       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.8282    0.9555    0.8873       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9549    0.9297    0.9421       455\n",
      "          12     0.9839    0.4507    0.6182       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8711      7004\n",
      "   macro avg     0.7784    0.7465    0.7414      7004\n",
      "weighted avg     0.9017    0.8711    0.8671      7004\n",
      "\n",
      "Testing Loss : 0.004096598534082631\n",
      "client_id 6\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7467723229187785\n",
      "|---- Precision: 0.770560971997343\n",
      "|---- Recall: 0.7541994572554315\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    0.9983    0.9991       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.9318    0.3512    0.5101       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     0.8438    0.6769    0.7512       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.8745    0.9465    0.9091       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9495    0.9912    0.9699       455\n",
      "          12     0.3981    0.8473    0.5417       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8774      7004\n",
      "   macro avg     0.7706    0.7542    0.7468      7004\n",
      "weighted avg     0.8964    0.8774    0.8721      7004\n",
      "\n",
      "Testing Loss : 0.003884517211282997\n",
      "client_id 3\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7531867014690874\n",
      "|---- Precision: 0.7858234737809966\n",
      "|---- Recall: 0.7558142991520677\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    0.9983    0.9991       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4725    0.9550    0.6322       467\n",
      "           5     0.7918    0.6444    0.7105       419\n",
      "           6     1.0000    0.6220    0.7669       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.7686    0.9911    0.8658       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9895    0.8308    0.9032       455\n",
      "          12     0.9944    0.4409    0.6109       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8828      7004\n",
      "   macro avg     0.7858    0.7558    0.7532      7004\n",
      "weighted avg     0.9064    0.8828    0.8781      7004\n",
      "\n",
      "Testing Loss : 0.0038463710956522785\n",
      "client_id 5\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7551307171444236\n",
      "|---- Precision: 0.7629338481178372\n",
      "|---- Recall: 0.7604347463486971\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.6181    0.5546    0.5847       467\n",
      "           5     0.7884    0.6492    0.7120       419\n",
      "           6     0.9896    0.6264    0.7672       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     0.8858    0.9485    0.9161       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.8911    0.9326    0.9114       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.4619    0.7906    0.5831       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8842      7004\n",
      "   macro avg     0.7629    0.7604    0.7551      7004\n",
      "weighted avg     0.8882    0.8842    0.8801      7004\n",
      "\n",
      "Testing Loss : 0.0037556014918344047\n",
      "client_id 0\n",
      "|---- Test Accuracy_client: 0.87%\n",
      "|---- F1_score: 0.7426900567086421\n",
      "|---- Precision: 0.7734492497220913\n",
      "|---- Recall: 0.7486420078234455\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.6722    0.4304    0.5248       467\n",
      "           5     0.9447    0.4487    0.6084       419\n",
      "           6     0.9930    0.6264    0.7682       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.8302    0.9326    0.8784       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.3960    0.9384    0.5570       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8725      7004\n",
      "   macro avg     0.7734    0.7486    0.7427      7004\n",
      "weighted avg     0.8983    0.8725    0.8687      7004\n",
      "\n",
      "Testing Loss : 0.004107859979746922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_id 13\n",
      "|---- Test Accuracy_client: 0.85%\n",
      "|---- F1_score: 0.70078633161833\n",
      "|---- Precision: 0.6885350111432762\n",
      "|---- Recall: 0.7244612742626854\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4419    0.5289    0.4815       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     0.4876    0.9055    0.6338       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.8379    1.0000    0.9118       455\n",
      "          12     0.0000    0.0000    0.0000       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8547      7004\n",
      "   macro avg     0.6885    0.7245    0.7008      7004\n",
      "weighted avg     0.8184    0.8547    0.8310      7004\n",
      "\n",
      "Testing Loss : 0.004319425895416536\n",
      "client_id 4\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7586534998960145\n",
      "|---- Precision: 0.7801493520327852\n",
      "|---- Recall: 0.7620665347955468\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.8648    0.9759    0.9170       498\n",
      "           4     0.4825    0.8865    0.6249       467\n",
      "           5     0.7432    0.6563    0.6971       419\n",
      "           6     0.8292    0.6615    0.7359       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.9944    0.4409    0.6109       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8888      7004\n",
      "   macro avg     0.7801    0.7621    0.7587      7004\n",
      "weighted avg     0.9006    0.8888    0.8832      7004\n",
      "\n",
      "Testing Loss : 0.004179232673898237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [00:20<00:30,  5.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_id 7\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7444977282458141\n",
      "|---- Precision: 0.7882280623894841\n",
      "|---- Recall: 0.7488834452345535\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.9290    0.3640    0.5231       467\n",
      "           5     0.7907    0.6492    0.7130       419\n",
      "           6     0.4111    0.9912    0.5812       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9458    0.9332    0.9395       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9381    1.0000    0.9681       455\n",
      "          12     0.9944    0.4409    0.6109       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8751      7004\n",
      "   macro avg     0.7882    0.7489    0.7445      7004\n",
      "weighted avg     0.9097    0.8751    0.8695      7004\n",
      "\n",
      "Testing Loss : 0.003969830229744392\n",
      " \n",
      "Avg Training Stats after 4 global rounds:\n",
      "|---- Test Accuracy: 0.8750713877784123\n",
      "|---- F1_score: 0.7444977282458141\n",
      "|---- Precision: 0.7882280623894841\n",
      "|---- Recall: 0.7488834452345535\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.9290    0.3640    0.5231       467\n",
      "           5     0.7907    0.6492    0.7130       419\n",
      "           6     0.4111    0.9912    0.5812       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9458    0.9332    0.9395       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9381    1.0000    0.9681       455\n",
      "          12     0.9944    0.4409    0.6109       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8751      7004\n",
      "   macro avg     0.7882    0.7489    0.7445      7004\n",
      "weighted avg     0.9097    0.8751    0.8695      7004\n",
      "\n",
      "Testing Loss : 0.003969830229744392\n",
      "\n",
      " | Training Round : 5 |\n",
      "\n",
      "idxs_users [ 8 12 10  3  5  4  9  6 11 13 14  2  1  0  7]\n",
      "client_id 8\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7565622168426074\n",
      "|---- Precision: 0.7879322893418748\n",
      "|---- Recall: 0.759445075524974\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4721    0.9593    0.6328       467\n",
      "           5     0.7976    0.6396    0.7099       419\n",
      "           6     0.9930    0.6264    0.7682       455\n",
      "           7     0.8179    0.9978    0.8989       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9231    0.9621    0.9422       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9610    0.9209    0.9405       455\n",
      "          12     0.9943    0.4286    0.5990       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8865      7004\n",
      "   macro avg     0.7879    0.7594    0.7566      7004\n",
      "weighted avg     0.9083    0.8865    0.8814      7004\n",
      "\n",
      "Testing Loss : 0.0041510378378100355\n",
      "client_id 12\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7555700020947737\n",
      "|---- Precision: 0.7708386622724087\n",
      "|---- Recall: 0.7603309629888869\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4943    0.7473    0.5951       467\n",
      "           5     0.7935    0.6420    0.7098       419\n",
      "           6     0.6877    0.7890    0.7349       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     0.8858    0.9485    0.9161       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.9944    0.4409    0.6109       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8869      7004\n",
      "   macro avg     0.7708    0.7603    0.7556      7004\n",
      "weighted avg     0.8915    0.8869    0.8802      7004\n",
      "\n",
      "Testing Loss : 0.003941631192199967\n",
      "client_id 10\n",
      "|---- Test Accuracy_client: 0.87%\n",
      "|---- F1_score: 0.740388465399494\n",
      "|---- Precision: 0.7850211912261607\n",
      "|---- Recall: 0.7464540099229983\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     0.9214    1.0000    0.9591       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     1.0000    0.2677    0.4223       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     1.0000    0.6198    0.7653       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.3571    0.8768    0.5075       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8695      7004\n",
      "   macro avg     0.7850    0.7465    0.7404      7004\n",
      "weighted avg     0.9084    0.8695    0.8647      7004\n",
      "\n",
      "Testing Loss : 0.003963664995170257\n",
      "client_id 3\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.748410988117698\n",
      "|---- Precision: 0.7622066018505661\n",
      "|---- Recall: 0.7578575605618281\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.8737    0.3555    0.5053       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     0.6552    0.7978    0.7195       455\n",
      "           7     0.8179    0.9978    0.8989       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9726    0.9488    0.9605       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9506    0.9736    0.9620       455\n",
      "          12     0.4693    0.7906    0.5890       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8814      7004\n",
      "   macro avg     0.7622    0.7579    0.7484      7004\n",
      "weighted avg     0.8878    0.8814    0.8734      7004\n",
      "\n",
      "Testing Loss : 0.003585801568950847\n",
      "client_id 5\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7426046249258108\n",
      "|---- Precision: 0.7659526920657255\n",
      "|---- Recall: 0.7515304499801212\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.8294    0.9759    0.8967       498\n",
      "           4     0.9874    0.3362    0.5016       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     0.4889    0.9187    0.6382       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9682    0.9488    0.9584       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9504    0.9692    0.9597       455\n",
      "          12     0.6231    0.5985    0.6106       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8765      7004\n",
      "   macro avg     0.7660    0.7515    0.7426      7004\n",
      "weighted avg     0.8898    0.8765    0.8672      7004\n",
      "\n",
      "Testing Loss : 0.003939469209949309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_id 4\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7552600110819216\n",
      "|---- Precision: 0.7704525036384164\n",
      "|---- Recall: 0.7598902254340866\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4890    0.7623    0.5958       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     0.7039    0.7626    0.7321       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.8302    0.9326    0.8784       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.9781    0.4409    0.6078       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8865      7004\n",
      "   macro avg     0.7705    0.7599    0.7553      7004\n",
      "weighted avg     0.8918    0.8865    0.8802      7004\n",
      "\n",
      "Testing Loss : 0.003958425835372805\n",
      "client_id 9\n",
      "|---- Test Accuracy_client: 0.87%\n",
      "|---- F1_score: 0.73829110197105\n",
      "|---- Precision: 0.771623312288195\n",
      "|---- Recall: 0.7473039593049123\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.6056    0.6445    0.6245       467\n",
      "           5     0.9593    0.2816    0.4354       419\n",
      "           6     1.0000    0.6220    0.7669       455\n",
      "           7     0.8691    0.9978    0.9290       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.8606    0.9488    0.9025       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9506    0.9736    0.9620       455\n",
      "          12     0.4254    0.8842    0.5744       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8728      7004\n",
      "   macro avg     0.7716    0.7473    0.7383      7004\n",
      "weighted avg     0.8961    0.8728    0.8653      7004\n",
      "\n",
      "Testing Loss : 0.0041081832905594095\n",
      "client_id 6\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7499581089055702\n",
      "|---- Precision: 0.7830699690830415\n",
      "|---- Recall: 0.7542564690342892\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4463    0.9165    0.6003       467\n",
      "           5     0.7010    0.6492    0.6741       419\n",
      "           6     0.9049    0.6484    0.7554       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9726    0.9488    0.9605       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9506    0.9736    0.9620       455\n",
      "          12     1.0000    0.3227    0.4879       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8821      7004\n",
      "   macro avg     0.7831    0.7543    0.7500      7004\n",
      "weighted avg     0.9040    0.8821    0.8759      7004\n",
      "\n",
      "Testing Loss : 0.0037380907370622185\n",
      "client_id 11\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7446283378587679\n",
      "|---- Precision: 0.7662714230440185\n",
      "|---- Recall: 0.7514212150956964\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    0.9983    0.9991       583\n",
      "           3     0.8648    0.9759    0.9170       498\n",
      "           4     0.6424    0.4540    0.5320       467\n",
      "           5     0.4459    0.9045    0.5973       419\n",
      "           6     0.8109    0.6879    0.7444       455\n",
      "           7     0.8691    0.9978    0.9290       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.9835    0.4409    0.6088       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8764      7004\n",
      "   macro avg     0.7663    0.7514    0.7446      7004\n",
      "weighted avg     0.8890    0.8764    0.8699      7004\n",
      "\n",
      "Testing Loss : 0.00394760220054152\n",
      "client_id 13\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7516548355691369\n",
      "|---- Precision: 0.7557194348315941\n",
      "|---- Recall: 0.7581426402766218\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.6427    0.4775    0.5479       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     0.8311    0.6813    0.7488       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.8379    1.0000    0.9118       455\n",
      "          12     0.4635    0.7808    0.5817       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8819      7004\n",
      "   macro avg     0.7557    0.7581    0.7517      7004\n",
      "weighted avg     0.8809    0.8819    0.8766      7004\n",
      "\n",
      "Testing Loss : 0.00395724688603972\n",
      "client_id 14\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7413028062349183\n",
      "|---- Precision: 0.7801703900142888\n",
      "|---- Recall: 0.752247584246929\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.9936    0.3340    0.5000       467\n",
      "           5     0.4420    0.9093    0.5948       419\n",
      "           6     0.6757    0.8242    0.7426       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.8333    0.9465    0.8863       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9495    0.9912    0.9699       455\n",
      "          12     0.9942    0.4236    0.5941       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8771      7004\n",
      "   macro avg     0.7802    0.7522    0.7413      7004\n",
      "weighted avg     0.9038    0.8771    0.8671      7004\n",
      "\n",
      "Testing Loss : 0.004210690281745887\n",
      "client_id 2\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7522153238132228\n",
      "|---- Precision: 0.7936056913473274\n",
      "|---- Recall: 0.7553816338096271\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     0.9109    1.0000    0.9534       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.4396    0.9657    0.6042       467\n",
      "           5     0.8012    0.6348    0.7084       419\n",
      "           6     1.0000    0.6220    0.7669       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9332    0.9654       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9381    1.0000    0.9681       455\n",
      "          12     1.0000    0.3202    0.4851       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8834      7004\n",
      "   macro avg     0.7936    0.7554    0.7522      7004\n",
      "weighted avg     0.9123    0.8834    0.8772      7004\n",
      "\n",
      "Testing Loss : 0.004152744570731061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_id 1\n",
      "|---- Test Accuracy_client: 0.87%\n",
      "|---- F1_score: 0.7402834780986589\n",
      "|---- Precision: 0.7755764261703927\n",
      "|---- Recall: 0.7472223567770075\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.6086    0.5760    0.5919       467\n",
      "           5     0.4233    0.9021    0.5762       419\n",
      "           6     1.0000    0.6198    0.7653       455\n",
      "           7     0.8691    0.9978    0.9290       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9354    0.9666       449\n",
      "          10     0.8302    0.9326    0.8784       430\n",
      "          11     0.9401    1.0000    0.9691       455\n",
      "          12     1.0000    0.3202    0.4851       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8734      7004\n",
      "   macro avg     0.7756    0.7472    0.7403      7004\n",
      "weighted avg     0.8990    0.8734    0.8673      7004\n",
      "\n",
      "Testing Loss : 0.003862579604733133\n",
      "client_id 0\n",
      "|---- Test Accuracy_client: 0.87%\n",
      "|---- F1_score: 0.7351183912761655\n",
      "|---- Precision: 0.7719386953942455\n",
      "|---- Recall: 0.744535143020821\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4725    0.9550    0.6322       467\n",
      "           5     0.9623    0.2434    0.3886       419\n",
      "           6     1.0000    0.6198    0.7653       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     0.8858    0.9485    0.9161       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.5518    0.5640    0.5579       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8731      7004\n",
      "   macro avg     0.7719    0.7445    0.7351      7004\n",
      "weighted avg     0.8947    0.8731    0.8623      7004\n",
      "\n",
      "Testing Loss : 0.0043843539446583825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [00:26<00:26,  5.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_id 7\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7554538435162171\n",
      "|---- Precision: 0.7637561835831018\n",
      "|---- Recall: 0.7606596200633355\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    0.9983    0.9991       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.6158    0.5525    0.5824       467\n",
      "           5     0.7971    0.6468    0.7141       419\n",
      "           6     0.9930    0.6264    0.7682       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.8803    0.9332    0.9059       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9381    1.0000    0.9681       455\n",
      "          12     0.4615    0.7980    0.5848       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8844      7004\n",
      "   macro avg     0.7638    0.7607    0.7555      7004\n",
      "weighted avg     0.8888    0.8844    0.8803      7004\n",
      "\n",
      "Testing Loss : 0.003952502202017096\n",
      " \n",
      "Avg Training Stats after 5 global rounds:\n",
      "|---- Test Accuracy: 0.884351798972016\n",
      "|---- F1_score: 0.7554538435162171\n",
      "|---- Precision: 0.7637561835831018\n",
      "|---- Recall: 0.7606596200633355\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    0.9983    0.9991       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.6158    0.5525    0.5824       467\n",
      "           5     0.7971    0.6468    0.7141       419\n",
      "           6     0.9930    0.6264    0.7682       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.8803    0.9332    0.9059       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9381    1.0000    0.9681       455\n",
      "          12     0.4615    0.7980    0.5848       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8844      7004\n",
      "   macro avg     0.7638    0.7607    0.7555      7004\n",
      "weighted avg     0.8888    0.8844    0.8803      7004\n",
      "\n",
      "Testing Loss : 0.003952502202017096\n",
      "\n",
      " | Training Round : 6 |\n",
      "\n",
      "idxs_users [ 9  6  5  4  2 14  8 11 12  1  0 10  3  7 13]\n",
      "client_id 9\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7603648149226074\n",
      "|---- Precision: 0.7901994095316193\n",
      "|---- Recall: 0.7632256765639702\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4738    0.9486    0.6320       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     0.9862    0.6264    0.7661       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.8379    1.0000    0.9118       455\n",
      "          12     0.9944    0.4409    0.6109       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8901      7004\n",
      "   macro avg     0.7902    0.7632    0.7604      7004\n",
      "weighted avg     0.9105    0.8901    0.8850      7004\n",
      "\n",
      "Testing Loss : 0.0039970039668998794\n",
      "client_id 6\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7534030909946274\n",
      "|---- Precision: 0.7631701802571469\n",
      "|---- Recall: 0.7585016606055249\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9328    0.9759    0.9539       498\n",
      "           4     0.6547    0.5075    0.5718       467\n",
      "           5     0.7884    0.6492    0.7120       419\n",
      "           6     0.9572    0.6396    0.7668       455\n",
      "           7     0.8963    0.9978    0.9443       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.8379    1.0000    0.9118       455\n",
      "          12     0.4389    0.7956    0.5657       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8822      7004\n",
      "   macro avg     0.7632    0.7585    0.7534      7004\n",
      "weighted avg     0.8885    0.8822    0.8785      7004\n",
      "\n",
      "Testing Loss : 0.0037471794322247952\n",
      "client_id 5\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7585381123488693\n",
      "|---- Precision: 0.7837250988992112\n",
      "|---- Recall: 0.7617993925770782\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4759    0.9293    0.6294       467\n",
      "           5     0.7918    0.6444    0.7105       419\n",
      "           6     0.9452    0.6440    0.7660       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     0.8858    0.9485    0.9161       466\n",
      "           9     0.9617    0.9510    0.9563       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9522    0.9626    0.9574       455\n",
      "          12     0.9728    0.4409    0.6068       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8886      7004\n",
      "   macro avg     0.7837    0.7618    0.7585      7004\n",
      "weighted avg     0.9042    0.8886    0.8832      7004\n",
      "\n",
      "Testing Loss : 0.003910622384366199\n",
      "client_id 4\n",
      "|---- Test Accuracy_client: 0.87%\n",
      "|---- F1_score: 0.7394469821663723\n",
      "|---- Precision: 0.7949072405397863\n",
      "|---- Recall: 0.7443744501491222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     0.9214    1.0000    0.9591       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.9932    0.3148    0.4780       467\n",
      "           5     0.3715    0.9905    0.5404       419\n",
      "           6     0.9828    0.6264    0.7651       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.9479    0.4483    0.6087       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8688      7004\n",
      "   macro avg     0.7949    0.7444    0.7394      7004\n",
      "weighted avg     0.9161    0.8688    0.8640      7004\n",
      "\n",
      "Testing Loss : 0.004032053425296426\n",
      "client_id 2\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7605454271875238\n",
      "|---- Precision: 0.7886392080776148\n",
      "|---- Recall: 0.7636874646771941\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4758    0.9465    0.6332       467\n",
      "           5     0.7907    0.6492    0.7130       419\n",
      "           6     0.9728    0.6286    0.7637       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     0.8858    0.9485    0.9161       466\n",
      "           9     0.9976    0.9376    0.9667       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9419    0.9978    0.9691       455\n",
      "          12     0.9944    0.4409    0.6109       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8905      7004\n",
      "   macro avg     0.7886    0.7637    0.7605      7004\n",
      "weighted avg     0.9088    0.8905    0.8851      7004\n",
      "\n",
      "Testing Loss : 0.0035636126880067534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_id 14\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7460830726744401\n",
      "|---- Precision: 0.7769086593010708\n",
      "|---- Recall: 0.7509678453426784\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.6109    0.5782    0.5941       467\n",
      "           5     0.9401    0.3747    0.5358       419\n",
      "           6     1.0000    0.6220    0.7669       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.8911    0.9326    0.9114       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.4024    0.9039    0.5569       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8756      7004\n",
      "   macro avg     0.7769    0.7510    0.7461      7004\n",
      "weighted avg     0.9016    0.8756    0.8725      7004\n",
      "\n",
      "Testing Loss : 0.004160631797608862\n",
      "client_id 8\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7580390327240674\n",
      "|---- Precision: 0.7860295669479354\n",
      "|---- Recall: 0.7617376532043505\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4714    0.9336    0.6264       467\n",
      "           5     0.7907    0.6492    0.7130       419\n",
      "           6     0.9452    0.6440    0.7660       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.8787    0.9354    0.9061       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9400    0.9978    0.9680       455\n",
      "          12     0.9940    0.4113    0.5819       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8888      7004\n",
      "   macro avg     0.7860    0.7617    0.7580      7004\n",
      "weighted avg     0.9066    0.8888    0.8830      7004\n",
      "\n",
      "Testing Loss : 0.0038669262665655276\n",
      "client_id 11\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7582674061090072\n",
      "|---- Precision: 0.7883035365040719\n",
      "|---- Recall: 0.7607362527692619\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     0.9605    1.0000    0.9798       583\n",
      "           3     0.8648    0.9759    0.9170       498\n",
      "           4     0.4729    0.9529    0.6321       467\n",
      "           5     0.7895    0.6444    0.7096       419\n",
      "           6     1.0000    0.6220    0.7669       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9408    0.9555    0.9481       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9554    0.9407    0.9480       455\n",
      "          12     0.9890    0.4409    0.6099       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8876      7004\n",
      "   macro avg     0.7883    0.7607    0.7583      7004\n",
      "weighted avg     0.9077    0.8876    0.8824      7004\n",
      "\n",
      "Testing Loss : 0.004107323156319759\n",
      "client_id 12\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7574943223008941\n",
      "|---- Precision: 0.767476453485982\n",
      "|---- Recall: 0.76155862691423\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.8648    0.9759    0.9170       498\n",
      "           4     0.6156    0.5760    0.5951       467\n",
      "           5     0.7878    0.6468    0.7104       419\n",
      "           6     1.0000    0.6220    0.7669       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9332    0.9654       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9381    1.0000    0.9681       455\n",
      "          12     0.4540    0.7906    0.5768       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8854      7004\n",
      "   macro avg     0.7675    0.7616    0.7575      7004\n",
      "weighted avg     0.8921    0.8854    0.8822      7004\n",
      "\n",
      "Testing Loss : 0.0039046270440921383\n",
      "client_id 1\n",
      "|---- Test Accuracy_client: 0.86%\n",
      "|---- F1_score: 0.7305188451331921\n",
      "|---- Precision: 0.793984164869135\n",
      "|---- Recall: 0.7374552306681639\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.9936    0.3319    0.4976       467\n",
      "           5     0.3560    0.9976    0.5248       419\n",
      "           6     0.9344    0.6264    0.7500       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     0.9189    0.9485    0.9335       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     1.0000    0.3202    0.4851       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8629      7004\n",
      "   macro avg     0.7940    0.7375    0.7305      7004\n",
      "weighted avg     0.9172    0.8629    0.8575      7004\n",
      "\n",
      "Testing Loss : 0.004359524007216479\n",
      "client_id 0\n",
      "|---- Test Accuracy_client: 0.82%\n",
      "|---- F1_score: 0.6624672877009875\n",
      "|---- Precision: 0.6649952482697877\n",
      "|---- Recall: 0.6945882101812526\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.0000    0.0000    0.0000       467\n",
      "           5     0.0000    0.0000    0.0000       419\n",
      "           6     0.9930    0.6264    0.7682       455\n",
      "           7     0.8691    0.9978    0.9290       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9976    0.9399    0.9679       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9439    0.9978    0.9701       455\n",
      "          12     0.2676    1.0000    0.4223       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8210      7004\n",
      "   macro avg     0.6650    0.6946    0.6625      7004\n",
      "weighted avg     0.7971    0.8210    0.7936      7004\n",
      "\n",
      "Testing Loss : 0.004691404004944015\n",
      "client_id 10\n",
      "|---- Test Accuracy_client: 0.82%\n",
      "|---- F1_score: 0.6586913355914267\n",
      "|---- Precision: 0.6569621295425206\n",
      "|---- Recall: 0.6939766143173719\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.0000    0.0000    0.0000       467\n",
      "           5     0.0000    0.0000    0.0000       419\n",
      "           6     0.9760    0.6264    0.7631       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.7909    0.9326    0.8559       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.2784    0.9975    0.4352       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8204      7004\n",
      "   macro avg     0.6570    0.6940    0.6587      7004\n",
      "weighted avg     0.7897    0.8204    0.7902      7004\n",
      "\n",
      "Testing Loss : 0.004441554637371574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_id 3\n",
      "|---- Test Accuracy_client: 0.87%\n",
      "|---- F1_score: 0.7333224076974852\n",
      "|---- Precision: 0.7826905390689607\n",
      "|---- Recall: 0.7402349474234298\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.8648    0.9759    0.9170       498\n",
      "           4     1.0000    0.3191    0.4838       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     0.4239    0.9978    0.5950       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     0.8863    0.9871    0.9340       466\n",
      "           9     1.0000    0.9354    0.9666       449\n",
      "          10     0.9799    0.7953    0.8780       430\n",
      "          11     0.8957    1.0000    0.9450       455\n",
      "          12     0.9891    0.4483    0.6169       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8671      7004\n",
      "   macro avg     0.7827    0.7402    0.7333      7004\n",
      "weighted avg     0.9035    0.8671    0.8583      7004\n",
      "\n",
      "Testing Loss : 0.003951122841746177\n",
      "client_id 7\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7511333607357231\n",
      "|---- Precision: 0.7778043182884847\n",
      "|---- Recall: 0.7552683337885019\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     0.9214    1.0000    0.9591       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.6086    0.5760    0.5919       467\n",
      "           5     0.4495    0.9021    0.6000       419\n",
      "           6     1.0000    0.6198    0.7653       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9354    0.9666       449\n",
      "          10     0.8911    0.9326    0.9114       430\n",
      "          11     0.9401    1.0000    0.9691       455\n",
      "          12     0.9835    0.4409    0.6088       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8804      7004\n",
      "   macro avg     0.7778    0.7553    0.7511      7004\n",
      "weighted avg     0.8986    0.8804    0.8755      7004\n",
      "\n",
      "Testing Loss : 0.004006684605651315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [00:31<00:21,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_id 13\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7423227177302166\n",
      "|---- Precision: 0.755328847623103\n",
      "|---- Recall: 0.7532773534218934\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.8198    0.3019    0.4413       467\n",
      "           5     0.7285    0.6659    0.6958       419\n",
      "           6     0.7205    0.7648    0.7420       455\n",
      "           7     0.8179    0.9978    0.8989       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.4471    0.7808    0.5686       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8768      7004\n",
      "   macro avg     0.7553    0.7533    0.7423      7004\n",
      "weighted avg     0.8812    0.8768    0.8674      7004\n",
      "\n",
      "Testing Loss : 0.003942297316017507\n",
      " \n",
      "Avg Training Stats after 6 global rounds:\n",
      "|---- Test Accuracy: 0.8767846944603084\n",
      "|---- F1_score: 0.7423227177302166\n",
      "|---- Precision: 0.755328847623103\n",
      "|---- Recall: 0.7532773534218934\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.8198    0.3019    0.4413       467\n",
      "           5     0.7285    0.6659    0.6958       419\n",
      "           6     0.7205    0.7648    0.7420       455\n",
      "           7     0.8179    0.9978    0.8989       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.4471    0.7808    0.5686       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8768      7004\n",
      "   macro avg     0.7553    0.7533    0.7423      7004\n",
      "weighted avg     0.8812    0.8768    0.8674      7004\n",
      "\n",
      "Testing Loss : 0.003942297316017507\n",
      "\n",
      " | Training Round : 7 |\n",
      "\n",
      "idxs_users [ 7 10  3 13  8  4  6  2 12  1  9 14  0  5 11]\n",
      "client_id 7\n",
      "|---- Test Accuracy_client: 0.85%\n",
      "|---- F1_score: 0.7055299880600184\n",
      "|---- Precision: 0.7334360785825518\n",
      "|---- Recall: 0.7225318757312668\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.0000    0.0000    0.0000       467\n",
      "           5     0.3104    0.9928    0.4730       419\n",
      "           6     0.9862    0.6264    0.7661       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9354    0.9666       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9401    1.0000    0.9691       455\n",
      "          12     0.9943    0.4286    0.5990       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8471      7004\n",
      "   macro avg     0.7334    0.7225    0.7055      7004\n",
      "weighted avg     0.8569    0.8471    0.8318      7004\n",
      "\n",
      "Testing Loss : 0.004330337289316545\n",
      "client_id 10\n",
      "|---- Test Accuracy_client: 0.87%\n",
      "|---- F1_score: 0.7438963787302775\n",
      "|---- Precision: 0.7580080976583901\n",
      "|---- Recall: 0.7501578061980706\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.7000    0.4347    0.5363       467\n",
      "           5     0.4583    0.9045    0.6083       419\n",
      "           6     1.0000    0.6198    0.7653       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9816    0.9488    0.9649       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9511    0.9824    0.9665       455\n",
      "          12     0.5086    0.5074    0.5080       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8746      7004\n",
      "   macro avg     0.7580    0.7502    0.7439      7004\n",
      "weighted avg     0.8847    0.8746    0.8700      7004\n",
      "\n",
      "Testing Loss : 0.0040813046827995305\n",
      "client_id 3\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.759253201100541\n",
      "|---- Precision: 0.7864412972769839\n",
      "|---- Recall: 0.7627452488098767\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.4775    0.9315    0.6313       467\n",
      "           5     0.7466    0.6611    0.7013       419\n",
      "           6     1.0000    0.6220    0.7669       455\n",
      "           7     0.8691    0.9978    0.9290       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.8800    0.9310    0.9048       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.9835    0.4409    0.6088       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8895      7004\n",
      "   macro avg     0.7864    0.7627    0.7593      7004\n",
      "weighted avg     0.9075    0.8895    0.8842      7004\n",
      "\n",
      "Testing Loss : 0.003500112664164057\n",
      "client_id 13\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7603678722031572\n",
      "|---- Precision: 0.7913695780249126\n",
      "|---- Recall: 0.7634181229248712\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4743    0.9700    0.6371       467\n",
      "           5     0.8061    0.6348    0.7103       419\n",
      "           6     1.0000    0.6198    0.7653       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.8302    0.9326    0.8784       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.9944    0.4409    0.6109       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8903      7004\n",
      "   macro avg     0.7914    0.7634    0.7604      7004\n",
      "weighted avg     0.9120    0.8903    0.8852      7004\n",
      "\n",
      "Testing Loss : 0.00384044978578284\n",
      "client_id 8\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7465131527441389\n",
      "|---- Precision: 0.7681454005225561\n",
      "|---- Recall: 0.7524083907298207\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.6531    0.4797    0.5531       467\n",
      "           5     0.4415    0.9093    0.5944       419\n",
      "           6     0.7321    0.6725    0.7010       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9815    0.9465    0.9637       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9490    0.9824    0.9654       455\n",
      "          12     0.9944    0.4409    0.6109       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8774      7004\n",
      "   macro avg     0.7681    0.7524    0.7465      7004\n",
      "weighted avg     0.8911    0.8774    0.8720      7004\n",
      "\n",
      "Testing Loss : 0.0038523527306982175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_id 4\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7534651198705653\n",
      "|---- Precision: 0.7580784033007816\n",
      "|---- Recall: 0.7593992750302974\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.6349    0.5139    0.5680       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     0.8699    0.6615    0.7516       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9332    0.9654       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.8395    1.0000    0.9127       455\n",
      "          12     0.4662    0.7808    0.5838       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8832      7004\n",
      "   macro avg     0.7581    0.7594    0.7535      7004\n",
      "weighted avg     0.8832    0.8832    0.8784      7004\n",
      "\n",
      "Testing Loss : 0.0037151484316462775\n",
      "client_id 6\n",
      "|---- Test Accuracy_client: 0.87%\n",
      "|---- F1_score: 0.7328990806172196\n",
      "|---- Precision: 0.7590090660325424\n",
      "|---- Recall: 0.7437250567033001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.8294    0.9759    0.8967       498\n",
      "           4     0.8966    0.2784    0.4248       467\n",
      "           5     0.4292    0.9189    0.5851       419\n",
      "           6     0.8604    0.6637    0.7494       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9976    0.9399    0.9679       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9439    0.9978    0.9701       455\n",
      "          12     0.5763    0.5025    0.5368       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8681      7004\n",
      "   macro avg     0.7590    0.7437    0.7329      7004\n",
      "weighted avg     0.8851    0.8681    0.8587      7004\n",
      "\n",
      "Testing Loss : 0.004318563034625002\n",
      "client_id 2\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7559073669075341\n",
      "|---- Precision: 0.7731692584856522\n",
      "|---- Recall: 0.7598451967086212\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4755    0.7901    0.5937       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     0.7411    0.7297    0.7353       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     0.8858    0.9485    0.9161       466\n",
      "           9     1.0000    0.9354    0.9666       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9401    1.0000    0.9691       455\n",
      "          12     0.9944    0.4409    0.6109       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8865      7004\n",
      "   macro avg     0.7732    0.7598    0.7559      7004\n",
      "weighted avg     0.8937    0.8865    0.8805      7004\n",
      "\n",
      "Testing Loss : 0.0036373019366307577\n",
      "client_id 12\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7591597624964262\n",
      "|---- Precision: 0.7808581121549494\n",
      "|---- Recall: 0.7628165807202032\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    0.9983    0.9991       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.4774    0.8801    0.6190       467\n",
      "           5     0.7907    0.6492    0.7130       419\n",
      "           6     0.8540    0.6813    0.7579       455\n",
      "           7     0.8691    0.9978    0.9290       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.8760    0.9443    0.9089       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9476    0.9934    0.9700       455\n",
      "          12     0.9944    0.4409    0.6109       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8895      7004\n",
      "   macro avg     0.7809    0.7628    0.7592      7004\n",
      "weighted avg     0.9018    0.8895    0.8840      7004\n",
      "\n",
      "Testing Loss : 0.004016361867361141\n",
      "client_id 1\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7601607541444508\n",
      "|---- Precision: 0.7894982381916624\n",
      "|---- Recall: 0.7632345004354629\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4728    0.9507    0.6316       467\n",
      "           5     0.7884    0.6492    0.7120       419\n",
      "           6     1.0000    0.6220    0.7669       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.8800    0.9310    0.9048       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.9944    0.4409    0.6109       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8901      7004\n",
      "   macro avg     0.7895    0.7632    0.7602      7004\n",
      "weighted avg     0.9099    0.8901    0.8848      7004\n",
      "\n",
      "Testing Loss : 0.003939390811709627\n",
      "client_id 9\n",
      "|---- Test Accuracy_client: 0.87%\n",
      "|---- F1_score: 0.7428875106324312\n",
      "|---- Precision: 0.7719450607071875\n",
      "|---- Recall: 0.7502346011439084\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.7390    0.3940    0.5140       467\n",
      "           5     0.4184    0.9427    0.5796       419\n",
      "           6     0.8387    0.6857    0.7545       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9354    0.9666       449\n",
      "          10     0.8302    0.9326    0.8784       430\n",
      "          11     0.9401    1.0000    0.9691       455\n",
      "          12     0.9835    0.4409    0.6088       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8749      7004\n",
      "   macro avg     0.7719    0.7502    0.7429      7004\n",
      "weighted avg     0.8956    0.8749    0.8687      7004\n",
      "\n",
      "Testing Loss : 0.00418026827867204\n",
      "client_id 14\n",
      "|---- Test Accuracy_client: 0.87%\n",
      "|---- F1_score: 0.7432199746224427\n",
      "|---- Precision: 0.775974359916517\n",
      "|---- Recall: 0.7500056567314862\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     0.9214    1.0000    0.9591       668\n",
      "           2     1.0000    0.9983    0.9991       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.8065    0.3747    0.5117       467\n",
      "           5     0.4180    0.9427    0.5792       419\n",
      "           6     0.7906    0.7055    0.7456       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9332    0.9654       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9381    1.0000    0.9681       455\n",
      "          12     0.9944    0.4409    0.6109       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8746      7004\n",
      "   macro avg     0.7760    0.7500    0.7432      7004\n",
      "weighted avg     0.8967    0.8746    0.8675      7004\n",
      "\n",
      "Testing Loss : 0.004258464218177399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_id 0\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7582120759466858\n",
      "|---- Precision: 0.784217867698826\n",
      "|---- Recall: 0.7609965854327366\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4725    0.9572    0.6327       467\n",
      "           5     0.7929    0.6396    0.7081       419\n",
      "           6     0.8324    0.6220    0.7119       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9449    0.9555    0.9502       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9556    0.9451    0.9503       455\n",
      "          12     0.9944    0.4409    0.6109       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8879      7004\n",
      "   macro avg     0.7842    0.7610    0.7582      7004\n",
      "weighted avg     0.9047    0.8879    0.8829      7004\n",
      "\n",
      "Testing Loss : 0.00421451786062284\n",
      "client_id 5\n",
      "|---- Test Accuracy_client: 0.86%\n",
      "|---- F1_score: 0.7331537933558568\n",
      "|---- Precision: 0.7973721375558088\n",
      "|---- Recall: 0.7397960595845023\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     0.9109    1.0000    0.9534       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     1.0000    0.2505    0.4007       467\n",
      "           5     0.3593    1.0000    0.5287       419\n",
      "           6     1.0000    0.6198    0.7653       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.9835    0.4409    0.6088       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8642      7004\n",
      "   macro avg     0.7974    0.7398    0.7332      7004\n",
      "weighted avg     0.9191    0.8642    0.8582      7004\n",
      "\n",
      "Testing Loss : 0.004063250042092819\n",
      "client_id 11\n",
      "|---- Test Accuracy_client: 0.85%\n",
      "|---- F1_score: 0.7034165314009073\n",
      "|---- Precision: 0.7161873805765122\n",
      "|---- Recall: 0.7253429092070919\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.0000    0.0000    0.0000       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     0.3715    0.9978    0.5414       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.8376    0.9421    0.8868       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9458    0.9978    0.9711       455\n",
      "          12     0.9835    0.4409    0.6088       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8515      7004\n",
      "   macro avg     0.7162    0.7253    0.7034      7004\n",
      "weighted avg     0.8381    0.8515    0.8286      7004\n",
      "\n",
      "Testing Loss : 0.004317591042746644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [00:37<00:16,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Avg Training Stats after 7 global rounds:\n",
      "|---- Test Accuracy: 0.8515134209023415\n",
      "|---- F1_score: 0.7034165314009073\n",
      "|---- Precision: 0.7161873805765122\n",
      "|---- Recall: 0.7253429092070919\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.0000    0.0000    0.0000       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     0.3715    0.9978    0.5414       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.8376    0.9421    0.8868       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9458    0.9978    0.9711       455\n",
      "          12     0.9835    0.4409    0.6088       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8515      7004\n",
      "   macro avg     0.7162    0.7253    0.7034      7004\n",
      "weighted avg     0.8381    0.8515    0.8286      7004\n",
      "\n",
      "Testing Loss : 0.004317591042746644\n",
      "\n",
      " | Training Round : 8 |\n",
      "\n",
      "idxs_users [ 8  5  2 14 10  0 13  9  3 11  4  7 12  1  6]\n",
      "client_id 8\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7428421128346646\n",
      "|---- Precision: 0.7624027425589527\n",
      "|---- Recall: 0.7542372091515842\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.8648    0.9759    0.9170       498\n",
      "           4     0.9310    0.2891    0.4412       467\n",
      "           5     0.7354    0.6635    0.6976       419\n",
      "           6     0.7154    0.7846    0.7484       455\n",
      "           7     0.8691    0.9978    0.9290       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.4428    0.7906    0.5676       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8776      7004\n",
      "   macro avg     0.7624    0.7542    0.7428      7004\n",
      "weighted avg     0.8880    0.8776    0.8678      7004\n",
      "\n",
      "Testing Loss : 0.003870005137752476\n",
      "client_id 5\n",
      "|---- Test Accuracy_client: 0.85%\n",
      "|---- F1_score: 0.7060807953572578\n",
      "|---- Precision: 0.7302374426058987\n",
      "|---- Recall: 0.7230937041241169\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.3634    1.0000    0.5331       467\n",
      "           5     0.0000    0.0000    0.0000       419\n",
      "           6     1.0000    0.6198    0.7653       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.8302    0.9326    0.8784       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.9944    0.4409    0.6109       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8544      7004\n",
      "   macro avg     0.7302    0.7231    0.7061      7004\n",
      "weighted avg     0.8564    0.8544    0.8358      7004\n",
      "\n",
      "Testing Loss : 0.00436553482108801\n",
      "client_id 2\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7435963455645187\n",
      "|---- Precision: 0.7841465853054659\n",
      "|---- Recall: 0.7477525068665832\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     0.9109    1.0000    0.9534       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4486    0.9529    0.6100       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     1.0000    0.6198    0.7653       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.8525    0.9911    0.9166       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9895    0.8308    0.9032       455\n",
      "          12     1.0000    0.3202    0.4851       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8758      7004\n",
      "   macro avg     0.7841    0.7478    0.7436      7004\n",
      "weighted avg     0.9030    0.8758    0.8687      7004\n",
      "\n",
      "Testing Loss : 0.003887271544077502\n",
      "client_id 14\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7603584773850024\n",
      "|---- Precision: 0.7894082931114561\n",
      "|---- Recall: 0.7628024699391927\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4732    0.9465    0.6310       467\n",
      "           5     0.6667    0.6492    0.6578       419\n",
      "           6     1.0000    0.6198    0.7653       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.9944    0.4409    0.6109       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8896      7004\n",
      "   macro avg     0.7894    0.7628    0.7604      7004\n",
      "weighted avg     0.9104    0.8896    0.8853      7004\n",
      "\n",
      "Testing Loss : 0.003854190371983787\n",
      "client_id 10\n",
      "|---- Test Accuracy_client: 0.87%\n",
      "|---- F1_score: 0.7298211180740551\n",
      "|---- Precision: 0.7612574866919758\n",
      "|---- Recall: 0.7420780071603132\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.8294    0.9759    0.8967       498\n",
      "           4     1.0000    0.2484    0.3979       467\n",
      "           5     0.4396    0.9117    0.5932       419\n",
      "           6     0.8453    0.6725    0.7491       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9793    0.9488    0.9638       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9510    0.9802    0.9654       455\n",
      "          12     0.5225    0.5148    0.5186       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8664      7004\n",
      "   macro avg     0.7613    0.7421    0.7298      7004\n",
      "weighted avg     0.8878    0.8664    0.8558      7004\n",
      "\n",
      "Testing Loss : 0.00418160365004069\n",
      "client_id 0\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7479690052401663\n",
      "|---- Precision: 0.753514516137698\n",
      "|---- Recall: 0.756904874220882\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.7552    0.3897    0.5141       467\n",
      "           5     0.7806    0.6539    0.7117       419\n",
      "           6     0.7288    0.7560    0.7422       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     0.8858    0.9485    0.9161       466\n",
      "           9     0.9639    0.9510    0.9574       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9523    0.9648    0.9585       455\n",
      "          12     0.4656    0.7833    0.5840       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8805      7004\n",
      "   macro avg     0.7535    0.7569    0.7480      7004\n",
      "weighted avg     0.8788    0.8805    0.8728      7004\n",
      "\n",
      "Testing Loss : 0.004125504770971992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_id 13\n",
      "|---- Test Accuracy_client: 0.87%\n",
      "|---- F1_score: 0.7415478519394064\n",
      "|---- Precision: 0.7860305471921794\n",
      "|---- Recall: 0.7477918306540516\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     0.9109    1.0000    0.9534       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.9693    0.3383    0.5016       467\n",
      "           5     0.7884    0.6492    0.7120       419\n",
      "           6     0.4294    0.9956    0.6000       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9332    0.9654       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9381    1.0000    0.9681       455\n",
      "          12     0.9837    0.4458    0.6136       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8739      7004\n",
      "   macro avg     0.7860    0.7478    0.7415      7004\n",
      "weighted avg     0.9058    0.8739    0.8656      7004\n",
      "\n",
      "Testing Loss : 0.003621486231162378\n",
      "client_id 9\n",
      "|---- Test Accuracy_client: 0.85%\n",
      "|---- F1_score: 0.7032722746346667\n",
      "|---- Precision: 0.715924478825699\n",
      "|---- Recall: 0.7252081986676505\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.0000    0.0000    0.0000       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     0.3718    0.9978    0.5418       455\n",
      "           7     0.8691    0.9978    0.9290       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.8805    0.9354    0.9071       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9401    1.0000    0.9691       455\n",
      "          12     0.9836    0.4433    0.6112       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8514      7004\n",
      "   macro avg     0.7159    0.7252    0.7033      7004\n",
      "weighted avg     0.8377    0.8514    0.8285      7004\n",
      "\n",
      "Testing Loss : 0.004320842103164607\n",
      "client_id 3\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7548945137424485\n",
      "|---- Precision: 0.7599915576556054\n",
      "|---- Recall: 0.7605773116630004\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.6382    0.5289    0.5785       467\n",
      "           5     0.7861    0.6492    0.7111       419\n",
      "           6     0.8743    0.6571    0.7503       455\n",
      "           7     0.7842    0.9978    0.8782       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9354    0.9666       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9401    1.0000    0.9691       455\n",
      "          12     0.4732    0.7833    0.5900       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8844      7004\n",
      "   macro avg     0.7600    0.7606    0.7549      7004\n",
      "weighted avg     0.8852    0.8844    0.8798      7004\n",
      "\n",
      "Testing Loss : 0.0039902776434511235\n",
      "client_id 11\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7504040793401606\n",
      "|---- Precision: 0.7773839325437729\n",
      "|---- Recall: 0.7548925070733526\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    0.9983    0.9991       583\n",
      "           3     0.8294    0.9759    0.8967       498\n",
      "           4     0.6106    0.5675    0.5882       467\n",
      "           5     0.4464    0.9045    0.5978       419\n",
      "           6     1.0000    0.6220    0.7669       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9953    0.9399    0.9668       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9437    0.9956    0.9690       455\n",
      "          12     0.9835    0.4409    0.6088       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8799      7004\n",
      "   macro avg     0.7774    0.7549    0.7504      7004\n",
      "weighted avg     0.8996    0.8799    0.8755      7004\n",
      "\n",
      "Testing Loss : 0.004024785408922506\n",
      "client_id 4\n",
      "|---- Test Accuracy_client: 0.86%\n",
      "|---- F1_score: 0.7098729406185027\n",
      "|---- Precision: 0.7754899680646469\n",
      "|---- Recall: 0.7308183980941493\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.4299    0.9786    0.5974       467\n",
      "           5     1.0000    0.0119    0.0236       419\n",
      "           6     1.0000    0.6220    0.7669       455\n",
      "           7     0.7842    0.9978    0.8782       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.5783    0.5640    0.5711       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8609      7004\n",
      "   macro avg     0.7755    0.7308    0.7099      7004\n",
      "weighted avg     0.8981    0.8609    0.8396      7004\n",
      "\n",
      "Testing Loss : 0.003839851159715162\n",
      "client_id 7\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7555903958703972\n",
      "|---- Precision: 0.771995693814344\n",
      "|---- Recall: 0.7597266612816823\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.4844    0.7666    0.5937       467\n",
      "           5     0.7935    0.6420    0.7098       419\n",
      "           6     0.6784    0.7648    0.7190       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.8641    0.9488    0.9045       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9509    0.9780    0.9642       455\n",
      "          12     0.9944    0.4409    0.6109       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8864      7004\n",
      "   macro avg     0.7720    0.7597    0.7556      7004\n",
      "weighted avg     0.8932    0.8864    0.8805      7004\n",
      "\n",
      "Testing Loss : 0.0037984551692611273\n",
      "client_id 12\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7614288794783606\n",
      "|---- Precision: 0.7956645045994638\n",
      "|---- Recall: 0.7622677533369291\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    0.9983    0.9991       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4439    0.9572    0.6065       467\n",
      "           5     0.7935    0.6420    0.7098       419\n",
      "           6     0.9965    0.6198    0.7642       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.9943    0.4310    0.6014       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8892      7004\n",
      "   macro avg     0.7957    0.7623    0.7614      7004\n",
      "weighted avg     0.9158    0.8892    0.8861      7004\n",
      "\n",
      "Testing Loss : 0.004233834258988627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_id 1\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7605509025466778\n",
      "|---- Precision: 0.7895435674455953\n",
      "|---- Recall: 0.7636779767809437\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4739    0.9507    0.6325       467\n",
      "           5     0.7907    0.6492    0.7130       419\n",
      "           6     0.9930    0.6242    0.7665       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.8805    0.9354    0.9071       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9401    1.0000    0.9691       455\n",
      "          12     0.9944    0.4409    0.6109       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8905      7004\n",
      "   macro avg     0.7895    0.7637    0.7606      7004\n",
      "weighted avg     0.9100    0.8905    0.8852      7004\n",
      "\n",
      "Testing Loss : 0.003671881037236094\n",
      "client_id 6\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.760741416765469\n",
      "|---- Precision: 0.7897773765197019\n",
      "|---- Recall: 0.7628327779712637\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.4617    0.9422    0.6197       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     1.0000    0.6220    0.7669       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.8379    1.0000    0.9118       455\n",
      "          12     0.9427    0.4458    0.6054       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8896      7004\n",
      "   macro avg     0.7898    0.7628    0.7607      7004\n",
      "weighted avg     0.9107    0.8896    0.8855      7004\n",
      "\n",
      "Testing Loss : 0.004000376783117762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [00:43<00:11,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Avg Training Stats after 8 global rounds:\n",
      "|---- Test Accuracy: 0.8896344945745288\n",
      "|---- F1_score: 0.760741416765469\n",
      "|---- Precision: 0.7897773765197019\n",
      "|---- Recall: 0.7628327779712637\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.4617    0.9422    0.6197       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     1.0000    0.6220    0.7669       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.8379    1.0000    0.9118       455\n",
      "          12     0.9427    0.4458    0.6054       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8896      7004\n",
      "   macro avg     0.7898    0.7628    0.7607      7004\n",
      "weighted avg     0.9107    0.8896    0.8855      7004\n",
      "\n",
      "Testing Loss : 0.004000376783117762\n",
      "\n",
      " | Training Round : 9 |\n",
      "\n",
      "idxs_users [ 9 13 12  4  3  8 14 10  0  5  6  7  1 11  2]\n",
      "client_id 9\n",
      "|---- Test Accuracy_client: 0.87%\n",
      "|---- F1_score: 0.7267589088673837\n",
      "|---- Precision: 0.7662891178559018\n",
      "|---- Recall: 0.7410505017970819\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4740    0.8779    0.6156       467\n",
      "           5     0.5778    0.6468    0.6104       419\n",
      "           6     0.7338    0.6725    0.7018       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9332    0.9654       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9381    1.0000    0.9681       455\n",
      "          12     1.0000    0.1305    0.2309       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8705      7004\n",
      "   macro avg     0.7663    0.7411    0.7268      7004\n",
      "weighted avg     0.8883    0.8705    0.8554      7004\n",
      "\n",
      "Testing Loss : 0.004314381940312139\n",
      "client_id 13\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7426968144776812\n",
      "|---- Precision: 0.779410703115256\n",
      "|---- Recall: 0.7530915932217003\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.9691    0.3362    0.4992       467\n",
      "           5     0.4486    0.9069    0.6003       419\n",
      "           6     0.6757    0.8242    0.7426       455\n",
      "           7     0.8179    0.9978    0.8989       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.9836    0.4433    0.6112       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8778      7004\n",
      "   macro avg     0.7794    0.7531    0.7427      7004\n",
      "weighted avg     0.9026    0.8778    0.8681      7004\n",
      "\n",
      "Testing Loss : 0.004092125916057854\n",
      "client_id 12\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7570870899230662\n",
      "|---- Precision: 0.7750391957236723\n",
      "|---- Recall: 0.7606837392200391\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4871    0.7687    0.5963       467\n",
      "           5     0.7935    0.6420    0.7098       419\n",
      "           6     0.6425    0.7780    0.7038       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9976    0.9354    0.9655       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9400    0.9978    0.9680       455\n",
      "          12     0.9944    0.4335    0.6038       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8874      7004\n",
      "   macro avg     0.7750    0.7607    0.7571      7004\n",
      "weighted avg     0.8957    0.8874    0.8818      7004\n",
      "\n",
      "Testing Loss : 0.00404780487389517\n",
      "client_id 4\n",
      "|---- Test Accuracy_client: 0.85%\n",
      "|---- F1_score: 0.7023049618690449\n",
      "|---- Precision: 0.7013840370258431\n",
      "|---- Recall: 0.7245623189472321\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.6387    0.5225    0.5748       467\n",
      "           5     0.0000    0.0000    0.0000       419\n",
      "           6     0.8772    0.6440    0.7427       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.8379    1.0000    0.9118       455\n",
      "          12     0.3526    0.9163    0.5092       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8517      7004\n",
      "   macro avg     0.7014    0.7246    0.7023      7004\n",
      "weighted avg     0.8331    0.8517    0.8328      7004\n",
      "\n",
      "Testing Loss : 0.004147965676892657\n",
      "client_id 3\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7460973931228712\n",
      "|---- Precision: 0.7768922510125351\n",
      "|---- Recall: 0.7508340016071816\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.6117    0.5803    0.5956       467\n",
      "           5     0.4271    0.9021    0.5798       419\n",
      "           6     1.0000    0.6220    0.7669       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.8503    0.9488    0.8968       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9500    0.9604    0.9552       455\n",
      "          12     1.0000    0.3941    0.5654       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8764      7004\n",
      "   macro avg     0.7769    0.7508    0.7461      7004\n",
      "weighted avg     0.9000    0.8764    0.8722      7004\n",
      "\n",
      "Testing Loss : 0.0038368698107934997\n",
      "client_id 8\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7599715134308783\n",
      "|---- Precision: 0.7807635172458056\n",
      "|---- Recall: 0.7626533016486109\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4713    0.9486    0.6297       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     1.0000    0.6198    0.7653       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9815    0.9465    0.9637       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9490    0.9824    0.9654       455\n",
      "          12     0.7490    0.4409    0.5550       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8895      7004\n",
      "   macro avg     0.7808    0.7627    0.7600      7004\n",
      "weighted avg     0.9030    0.8895    0.8850      7004\n",
      "\n",
      "Testing Loss : 0.0036674325707034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_id 14\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7568454475556118\n",
      "|---- Precision: 0.7870039361515053\n",
      "|---- Recall: 0.7596779041140552\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4720    0.9572    0.6322       467\n",
      "           5     0.7976    0.6396    0.7099       419\n",
      "           6     1.0000    0.6198    0.7653       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.8266    0.9555    0.8864       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9548    0.9275    0.9409       455\n",
      "          12     0.9835    0.4409    0.6088       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8866      7004\n",
      "   macro avg     0.7870    0.7597    0.7568      7004\n",
      "weighted avg     0.9076    0.8866    0.8817      7004\n",
      "\n",
      "Testing Loss : 0.004255624369586228\n",
      "client_id 10\n",
      "|---- Test Accuracy_client: 0.87%\n",
      "|---- F1_score: 0.7461296624258692\n",
      "|---- Precision: 0.7843757117032396\n",
      "|---- Recall: 0.749242927554723\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.7348    0.4154    0.5308       467\n",
      "           5     0.3801    0.9570    0.5441       419\n",
      "           6     0.9604    0.6396    0.7678       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.9835    0.4409    0.6088       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8739      7004\n",
      "   macro avg     0.7844    0.7492    0.7461      7004\n",
      "weighted avg     0.9075    0.8739    0.8718      7004\n",
      "\n",
      "Testing Loss : 0.0037794085958816202\n",
      "client_id 0\n",
      "|---- Test Accuracy_client: 0.86%\n",
      "|---- F1_score: 0.707527062466834\n",
      "|---- Precision: 0.7967629447476094\n",
      "|---- Recall: 0.7238766602292687\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.3648    1.0000    0.5346       467\n",
      "           5     1.0000    0.0095    0.0189       419\n",
      "           6     1.0000    0.6220    0.7669       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     0.8858    0.9485    0.9161       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.8922    1.0000    0.9430       455\n",
      "          12     0.9944    0.4409    0.6109       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8551      7004\n",
      "   macro avg     0.7968    0.7239    0.7075      7004\n",
      "weighted avg     0.9157    0.8551    0.8370      7004\n",
      "\n",
      "Testing Loss : 0.004304956612860659\n",
      "client_id 5\n",
      "|---- Test Accuracy_client: 0.85%\n",
      "|---- F1_score: 0.7040131540975271\n",
      "|---- Precision: 0.7057779265704842\n",
      "|---- Recall: 0.7250280290426427\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.6116    0.5632    0.5864       467\n",
      "           5     0.3665    0.9045    0.5217       419\n",
      "           6     1.0000    0.6220    0.7669       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.8379    1.0000    0.9118       455\n",
      "          12     0.0000    0.0000    0.0000       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8539      7004\n",
      "   macro avg     0.7058    0.7250    0.7040      7004\n",
      "weighted avg     0.8376    0.8539    0.8353      7004\n",
      "\n",
      "Testing Loss : 0.004149112801694086\n",
      "client_id 6\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7505690832674288\n",
      "|---- Precision: 0.7995394848885375\n",
      "|---- Recall: 0.7514891971971475\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     0.9109    1.0000    0.9534       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4230    0.9936    0.5934       467\n",
      "           5     0.9611    0.4129    0.5776       419\n",
      "           6     1.0000    0.6198    0.7653       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9332    0.9654       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9381    1.0000    0.9681       455\n",
      "          12     0.9894    0.4581    0.6263       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8798      7004\n",
      "   macro avg     0.7995    0.7515    0.7506      7004\n",
      "weighted avg     0.9171    0.8798    0.8751      7004\n",
      "\n",
      "Testing Loss : 0.004286515002305391\n",
      "client_id 7\n",
      "|---- Test Accuracy_client: 0.87%\n",
      "|---- F1_score: 0.7436875368072637\n",
      "|---- Precision: 0.7851054664282756\n",
      "|---- Recall: 0.7472114012230932\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.8199    0.3704    0.5103       467\n",
      "           5     0.8562    0.6110    0.7131       419\n",
      "           6     0.4074    0.9956    0.5782       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9953    0.9399    0.9668       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9437    0.9956    0.9690       455\n",
      "          12     0.9835    0.4409    0.6088       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8736      7004\n",
      "   macro avg     0.7851    0.7472    0.7437      7004\n",
      "weighted avg     0.9059    0.8736    0.8685      7004\n",
      "\n",
      "Testing Loss : 0.004203668979689615\n",
      "client_id 1\n",
      "|---- Test Accuracy_client: 0.87%\n",
      "|---- F1_score: 0.7401226827595291\n",
      "|---- Precision: 0.767565315036905\n",
      "|---- Recall: 0.7449997388197119\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.8294    0.9759    0.8967       498\n",
      "           4     0.7037    0.4069    0.5156       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     0.4317    0.9033    0.5842       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9726    0.9488    0.9605       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9506    0.9736    0.9620       455\n",
      "          12     0.9835    0.4409    0.6088       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8714      7004\n",
      "   macro avg     0.7676    0.7450    0.7401      7004\n",
      "weighted avg     0.8884    0.8714    0.8648      7004\n",
      "\n",
      "Testing Loss : 0.0037882444712049887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_id 11\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7499109714784485\n",
      "|---- Precision: 0.7765868766344305\n",
      "|---- Recall: 0.754844976229449\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.6124    0.5717    0.5914       467\n",
      "           5     0.4464    0.9045    0.5978       419\n",
      "           6     1.0000    0.6198    0.7653       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     0.8451    0.9485    0.8938       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.9944    0.4409    0.6109       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8799      7004\n",
      "   macro avg     0.7766    0.7548    0.7499      7004\n",
      "weighted avg     0.8993    0.8799    0.8753      7004\n",
      "\n",
      "Testing Loss : 0.0040151402005858755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 9/10 [00:49<00:05,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_id 2\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7560739029953472\n",
      "|---- Precision: 0.7747061250412567\n",
      "|---- Recall: 0.7610058184314414\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.8294    0.9759    0.8967       498\n",
      "           4     0.5417    0.7645    0.6341       467\n",
      "           5     0.5565    0.7757    0.6481       419\n",
      "           6     0.9105    0.6484    0.7574       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.9944    0.4409    0.6109       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8869      7004\n",
      "   macro avg     0.7747    0.7610    0.7561      7004\n",
      "weighted avg     0.8962    0.8869    0.8808      7004\n",
      "\n",
      "Testing Loss : 0.0037413809282533924\n",
      " \n",
      "Avg Training Stats after 9 global rounds:\n",
      "|---- Test Accuracy: 0.88692175899486\n",
      "|---- F1_score: 0.7560739029953472\n",
      "|---- Precision: 0.7747061250412567\n",
      "|---- Recall: 0.7610058184314414\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.8294    0.9759    0.8967       498\n",
      "           4     0.5417    0.7645    0.6341       467\n",
      "           5     0.5565    0.7757    0.6481       419\n",
      "           6     0.9105    0.6484    0.7574       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.9944    0.4409    0.6109       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8869      7004\n",
      "   macro avg     0.7747    0.7610    0.7561      7004\n",
      "weighted avg     0.8962    0.8869    0.8808      7004\n",
      "\n",
      "Testing Loss : 0.0037413809282533924\n",
      "\n",
      " | Training Round : 10 |\n",
      "\n",
      "idxs_users [ 3  2  6  5  7 10 14 13 12  4  9  8  0  1 11]\n",
      "client_id 3\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7497694398863939\n",
      "|---- Precision: 0.7692148723602135\n",
      "|---- Recall: 0.7589973032130216\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.9639    0.3426    0.5055       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     0.6045    0.8198    0.6959       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9907    0.9465    0.9681       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9495    0.9912    0.9699       455\n",
      "          12     0.4690    0.7833    0.5867       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8825      7004\n",
      "   macro avg     0.7692    0.7590    0.7498      7004\n",
      "weighted avg     0.8946    0.8825    0.8746      7004\n",
      "\n",
      "Testing Loss : 0.003619638791675868\n",
      "client_id 2\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.75980535388505\n",
      "|---- Precision: 0.7846178457327759\n",
      "|---- Recall: 0.7630170614438744\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.8648    0.9759    0.9170       498\n",
      "           4     0.4765    0.9122    0.6260       467\n",
      "           5     0.7907    0.6492    0.7130       419\n",
      "           6     0.8997    0.6505    0.7551       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9976    0.9399    0.9679       449\n",
      "          10     0.8911    0.9326    0.9114       430\n",
      "          11     0.9439    0.9978    0.9701       455\n",
      "          12     0.9944    0.4409    0.6109       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8898      7004\n",
      "   macro avg     0.7846    0.7630    0.7598      7004\n",
      "weighted avg     0.9049    0.8898    0.8843      7004\n",
      "\n",
      "Testing Loss : 0.003678429556924367\n",
      "client_id 6\n",
      "|---- Test Accuracy_client: 0.87%\n",
      "|---- F1_score: 0.7448291838485519\n",
      "|---- Precision: 0.7633020965432328\n",
      "|---- Recall: 0.7498336516883151\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.7044    0.4133    0.5209       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     1.0000    0.6198    0.7653       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.8208    0.9688    0.8887       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9675    0.9165    0.9413       455\n",
      "          12     0.3962    0.8276    0.5359       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8734      7004\n",
      "   macro avg     0.7633    0.7498    0.7448      7004\n",
      "weighted avg     0.8890    0.8734    0.8703      7004\n",
      "\n",
      "Testing Loss : 0.004172371620889955\n",
      "client_id 5\n",
      "|---- Test Accuracy_client: 0.87%\n",
      "|---- F1_score: 0.740963032173121\n",
      "|---- Precision: 0.7764757583489506\n",
      "|---- Recall: 0.747780723998027\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.8698    0.3576    0.5068       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     0.4343    0.9736    0.6007       455\n",
      "           7     0.8691    0.9978    0.9290       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9332    0.9654       449\n",
      "          10     0.8302    0.9326    0.8784       430\n",
      "          11     0.9381    1.0000    0.9681       455\n",
      "          12     0.9531    0.4507    0.6120       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8739      7004\n",
      "   macro avg     0.7765    0.7478    0.7410      7004\n",
      "weighted avg     0.8987    0.8739    0.8662      7004\n",
      "\n",
      "Testing Loss : 0.0037201516592060284\n",
      "client_id 7\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7600427921136708\n",
      "|---- Precision: 0.7871862103455545\n",
      "|---- Recall: 0.7630468334224421\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.4762    0.9422    0.6326       467\n",
      "           5     0.7861    0.6492    0.7111       419\n",
      "           6     0.9794    0.6264    0.7641       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     0.8451    0.9485    0.8938       466\n",
      "           9     0.9683    0.9532    0.9607       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9545    0.9692    0.9618       455\n",
      "          12     0.9839    0.4507    0.6182       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8898      7004\n",
      "   macro avg     0.7872    0.7630    0.7600      7004\n",
      "weighted avg     0.9077    0.8898    0.8847      7004\n",
      "\n",
      "Testing Loss : 0.003948835145641142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_id 10\n",
      "|---- Test Accuracy_client: 0.88%\n",
      "|---- F1_score: 0.7423925546351409\n",
      "|---- Precision: 0.7685807738883008\n",
      "|---- Recall: 0.7509761280375826\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.7449    0.3940    0.5154       467\n",
      "           5     0.4362    0.9308    0.5941       419\n",
      "           6     0.7687    0.7231    0.7452       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.8302    0.9326    0.8784       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.9831    0.4310    0.5993       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8758      7004\n",
      "   macro avg     0.7686    0.7510    0.7424      7004\n",
      "weighted avg     0.8922    0.8758    0.8682      7004\n",
      "\n",
      "Testing Loss : 0.003888947716879545\n",
      "client_id 14\n",
      "|---- Test Accuracy_client: 0.85%\n",
      "|---- F1_score: 0.6976446143243081\n",
      "|---- Precision: 0.7629229751669286\n",
      "|---- Recall: 0.7208897092094786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.8294    0.9759    0.8967       498\n",
      "           4     0.6937    0.4218    0.5246       467\n",
      "           5     1.0000    0.0072    0.0142       419\n",
      "           6     0.8061    0.6945    0.7462       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9616    0.9488    0.9552       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9501    0.9626    0.9563       455\n",
      "          12     0.3511    0.9236    0.5088       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8478      7004\n",
      "   macro avg     0.7629    0.7209    0.6976      7004\n",
      "weighted avg     0.8872    0.8478    0.8277      7004\n",
      "\n",
      "Testing Loss : 0.004195164637251846\n",
      "client_id 13\n",
      "|---- Test Accuracy_client: 0.84%\n",
      "|---- F1_score: 0.6904278280329532\n",
      "|---- Precision: 0.7098425603253433\n",
      "|---- Recall: 0.7178127090410413\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.9847    0.2762    0.4314       467\n",
      "           5     0.0000    0.0000    0.0000       419\n",
      "           6     0.7317    0.7670    0.7489       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9332    0.9654       449\n",
      "          10     0.8302    0.9326    0.8784       430\n",
      "          11     0.9381    1.0000    0.9681       455\n",
      "          12     0.3336    0.9360    0.4919       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8445      7004\n",
      "   macro avg     0.7098    0.7178    0.6904      7004\n",
      "weighted avg     0.8422    0.8445    0.8212      7004\n",
      "\n",
      "Testing Loss : 0.00418584726789146\n",
      "client_id 12\n",
      "|---- Test Accuracy_client: 0.86%\n",
      "|---- F1_score: 0.7274180685771054\n",
      "|---- Precision: 0.7808937665883473\n",
      "|---- Recall: 0.7321230024817675\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.3793    0.9893    0.5484       467\n",
      "           5     0.6364    0.2506    0.3596       419\n",
      "           6     0.9896    0.6264    0.7672       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9976    0.9354    0.9655       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9400    0.9978    0.9680       455\n",
      "          12     1.0000    0.3276    0.4935       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8626      7004\n",
      "   macro avg     0.7809    0.7321    0.7274      7004\n",
      "weighted avg     0.9020    0.8626    0.8554      7004\n",
      "\n",
      "Testing Loss : 0.004029816215877011\n",
      "client_id 4\n",
      "|---- Test Accuracy_client: 0.86%\n",
      "|---- F1_score: 0.7255087074169412\n",
      "|---- Precision: 0.7729284603187089\n",
      "|---- Recall: 0.735692421881211\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.6568    0.4754    0.5516       467\n",
      "           5     0.9780    0.2124    0.3490       419\n",
      "           6     0.9930    0.6264    0.7682       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     0.8451    0.9485    0.8938       466\n",
      "           9     1.0000    0.9354    0.9666       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9401    1.0000    0.9691       455\n",
      "          12     0.3666    0.9310    0.5261       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8612      7004\n",
      "   macro avg     0.7729    0.7357    0.7255      7004\n",
      "weighted avg     0.8974    0.8612    0.8534      7004\n",
      "\n",
      "Testing Loss : 0.003960815242367735\n",
      "client_id 9\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7571068396083201\n",
      "|---- Precision: 0.7744907262415519\n",
      "|---- Recall: 0.7607392249383856\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4938    0.7645    0.6000       467\n",
      "           5     0.7918    0.6444    0.7105       419\n",
      "           6     0.6292    0.7758    0.6949       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9976    0.9354    0.9655       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9400    0.9978    0.9680       455\n",
      "          12     0.9944    0.4384    0.6085       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8874      7004\n",
      "   macro avg     0.7745    0.7607    0.7571      7004\n",
      "weighted avg     0.8952    0.8874    0.8818      7004\n",
      "\n",
      "Testing Loss : 0.004267648934106902\n",
      "client_id 8\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7593159325014318\n",
      "|---- Precision: 0.7839889216823164\n",
      "|---- Recall: 0.7626924565151957\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9187    0.9759    0.9464       498\n",
      "           4     0.4746    0.9208    0.6264       467\n",
      "           5     0.7901    0.6468    0.7113       419\n",
      "           6     0.9331    0.6440    0.7620       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.8787    0.9354    0.9061       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9400    0.9978    0.9680       455\n",
      "          12     0.9728    0.4409    0.6068       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8895      7004\n",
      "   macro avg     0.7840    0.7627    0.7593      7004\n",
      "weighted avg     0.9047    0.8895    0.8840      7004\n",
      "\n",
      "Testing Loss : 0.0036526826489025324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_id 0\n",
      "|---- Test Accuracy_client: 0.85%\n",
      "|---- F1_score: 0.7195221005271223\n",
      "|---- Precision: 0.7967859899889863\n",
      "|---- Recall: 0.7279006938474748\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     1.0000    0.2484    0.3979       467\n",
      "           5     0.9237    0.2601    0.4060       419\n",
      "           6     1.0000    0.6198    0.7653       455\n",
      "           7     0.8691    0.9978    0.9290       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9354    0.9666       449\n",
      "          10     0.9413    0.9326    0.9369       430\n",
      "          11     0.9401    1.0000    0.9691       455\n",
      "          12     0.3152    1.0000    0.4793       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8525      7004\n",
      "   macro avg     0.7968    0.7279    0.7195      7004\n",
      "weighted avg     0.9221    0.8525    0.8474      7004\n",
      "\n",
      "Testing Loss : 0.004321140393525413\n",
      "client_id 1\n",
      "|---- Test Accuracy_client: 0.89%\n",
      "|---- F1_score: 0.7604640613434884\n",
      "|---- Precision: 0.7840616826046317\n",
      "|---- Recall: 0.7636626491471254\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     1.0000    1.0000    1.0000       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     0.4792    0.9358    0.6338       467\n",
      "           5     0.7935    0.6420    0.7098       419\n",
      "           6     0.7936    0.6505    0.7150       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     1.0000    0.9310    0.9642       449\n",
      "          10     0.8911    0.9326    0.9114       430\n",
      "          11     0.9362    1.0000    0.9671       455\n",
      "          12     0.9944    0.4409    0.6109       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8905      7004\n",
      "   macro avg     0.7841    0.7637    0.7605      7004\n",
      "weighted avg     0.9050    0.8905    0.8853      7004\n",
      "\n",
      "Testing Loss : 0.0038543196901921575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:54<00:00,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_id 11\n",
      "|---- Test Accuracy_client: 0.84%\n",
      "|---- F1_score: 0.6913922770446332\n",
      "|---- Precision: 0.7247035856818393\n",
      "|---- Recall: 0.7126753484864342\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     0.9214    1.0000    0.9591       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     1.0000    0.3041    0.4663       467\n",
      "           5     0.0000    0.0000    0.0000       419\n",
      "           6     0.9728    0.6286    0.7637       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9470    0.9555    0.9512       449\n",
      "          10     0.8911    0.9326    0.9114       430\n",
      "          11     0.9557    0.9473    0.9514       455\n",
      "          12     0.3097    1.0000    0.4729       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8391      7004\n",
      "   macro avg     0.7247    0.7127    0.6914      7004\n",
      "weighted avg     0.8545    0.8391    0.8211      7004\n",
      "\n",
      "Testing Loss : 0.004433789441608284\n",
      " \n",
      "Avg Training Stats after 10 global rounds:\n",
      "|---- Test Accuracy: 0.839091947458595\n",
      "|---- F1_score: 0.6913922770446332\n",
      "|---- Precision: 0.7247035856818393\n",
      "|---- Recall: 0.7126753484864342\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1145\n",
      "           1     0.9214    1.0000    0.9591       668\n",
      "           2     1.0000    1.0000    1.0000       583\n",
      "           3     0.9624    0.9759    0.9691       498\n",
      "           4     1.0000    0.3041    0.4663       467\n",
      "           5     0.0000    0.0000    0.0000       419\n",
      "           6     0.9728    0.6286    0.7637       455\n",
      "           7     0.9105    0.9978    0.9522       459\n",
      "           8     1.0000    0.9485    0.9736       466\n",
      "           9     0.9470    0.9555    0.9512       449\n",
      "          10     0.8911    0.9326    0.9114       430\n",
      "          11     0.9557    0.9473    0.9514       455\n",
      "          12     0.3097    1.0000    0.4729       406\n",
      "          13     0.0000    0.0000    0.0000        57\n",
      "          14     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.8391      7004\n",
      "   macro avg     0.7247    0.7127    0.6914      7004\n",
      "weighted avg     0.8545    0.8391    0.8211      7004\n",
      "\n",
      "Testing Loss : 0.004433789441608284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# BUILD MODEL\n",
    "#global_model=MLPModel(options=options, train_dataset=train_dataset, test_dataset=test_dataset, logger=logger)\n",
    "MLP_model=MLP()\n",
    "print(MLP_model)\n",
    "# Training\n",
    "train_loss, train_accuracy = [], []\n",
    "val_acc_list, net_list = [], []\n",
    "cv_loss, cv_acc = [], []\n",
    "print_every = 2\n",
    "val_loss_pre, counter = 0, 0\n",
    "\n",
    "for rounds in tqdm(range(options.rounds)):\n",
    "    # in the server\n",
    "    local_weights, local_losses = [], []\n",
    "    print(f'\\n | Training Round : {rounds+1} |\\n')\n",
    "    \n",
    "    #global_model.train(auto_encoder_model)\n",
    "    m = max(int(options.frac * options.num_users), 1)\n",
    "    idxs_users = np.random.choice(range(options.num_users), m, replace=False)\n",
    "    print(\"idxs_users\",idxs_users)\n",
    "\n",
    "    \n",
    "    \n",
    "    for idx in idxs_users:\n",
    "        MLP_client = MLPModel(options=options, train_dataset=train_dataset, test_dataset=test_dataset,\n",
    "                                    idxs=user_groups[idx], logger=logger)\n",
    "        loss, train_acc, w = MLP_client.train(model=copy.deepcopy(MLP_model))\n",
    "        #print(w)\n",
    "        #print(\"w\", w)\n",
    "        #print(\"loss\", loss)\n",
    "        local_weights.append(copy.deepcopy(w))\n",
    "        #local_losses.append(copy.deepcopy(loss))\n",
    "        \n",
    "        test_acc, F1_score, Precision, Recall, class_report, test_loss = MLP_client.test_inference(MLP_model, test_dataset)\n",
    "        print(f'client_id {idx}')\n",
    "        print(\"|---- Test Accuracy_client: {:.2f}%\".format(test_acc))\n",
    "        print(\"|---- F1_score:\", F1_score)\n",
    "        print(\"|---- Precision:\", Precision)\n",
    "        print(\"|---- Recall:\", Recall)\n",
    "        print(class_report)\n",
    "        print(f'Testing Loss : {np.mean(np.array(test_loss))}')\n",
    "    #print(local_weights)\n",
    "    MLP_model.load_state_dict(average_weights(local_weights))\n",
    "    #loss_avg = sum(local_losses) / len(local_losses)\n",
    "    #train_loss.append(loss_avg)\n",
    "    \n",
    "    #mean_losses_superv = global_model.train(auto_encoder_model)\n",
    "    #mean_losses_superv_avg = sum(mean_losses_superv) / len(mean_losses_superv)\n",
    "    \n",
    "    \n",
    "   \n",
    "    # Calculate avg training accuracy over all users at every epoch\n",
    "    #list_acc, list_loss = [], []\n",
    "    #auto_encoder_model.eval()\n",
    "    #for c in range(options.num_users):\n",
    "        #local_model = AutoEncoder(options=options, dataset=train_dataset,\n",
    "                                       #idxs=user_groups[idx], logger=logger)\n",
    "        #acc, loss = auto_encoder.inference(model=auto_encoder_model)\n",
    "        #list_acc.append(acc)\n",
    "        #list_loss.append(loss)\n",
    "    #train_accuracy.append(sum(list_acc)/len(list_acc))\n",
    "\n",
    "    # print global training loss after every 'i' rounds\n",
    "    #if (round+1) % print_every == 0:\n",
    "        #print(f' \\nAvg Training Stats after {round+1} global rounds:')\n",
    "        #print(f'Training Loss : {np.mean(np.array(train_loss))}')\n",
    "        #print('Train Accuracy: {:.2f}% \\n'.format(100*train_accuracy[-1]))\n",
    "#print(\"yes\")\n",
    "\n",
    "    test_acc_g, F1_score_g, Precision_g, Recall_g, class_report_g, test_loss_g = MLP_client.test_inference(MLP_model, test_dataset)\n",
    "\n",
    "    \n",
    "    #if (rounds+1) % print_every == 0:\n",
    "    print(f' \\nAvg Training Stats after {rounds+1} global rounds:')\n",
    "    print(\"|---- Test Accuracy:\", test_acc_g)\n",
    "    print(\"|---- F1_score:\", F1_score_g)\n",
    "    print(\"|---- Precision:\", Precision_g)\n",
    "    print(\"|---- Recall:\", Recall_g)\n",
    "    print(class_report_g)\n",
    "    #print(f'Training Loss : {np.mean(np.array(mean_losses_superv_avg))}')\n",
    "    print(f'Testing Loss : {np.mean(np.array(test_loss))}')\n",
    "    \n",
    "   \n",
    "        #print(f' \\n Results after {options.rounds} global rounds of training:')\n",
    "#print(\"|---- Avg Train Accuracy: {:.2f}%\".format(100*train_accuracy[-1]))\n",
    "        #print(f'Training Loss : {np.mean(np.array(mean_losses_superv_avg))}')\n",
    "        #print(f'Testing Loss : {np.mean(np.array(test_loss))}')\n",
    "        \n",
    "        #print(\"yes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
